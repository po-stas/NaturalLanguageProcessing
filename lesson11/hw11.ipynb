{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ДЗ №11. Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень популярная статеечка http://jalammar.github.io/illustrated-transformer/\n",
    "Просто по какой-то причине читать методичку (даже на русском языке) очень трудно. Даже код не всегда спасает. Такое чувство что это автоматический перевод (вот ведь ирония) какой-то оригинальной работы..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/ru_to_en', with_info=True,\n",
    "                               as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = iter(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = i.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "к : успех , перемены возможны только с оружием в руках .\n"
     ]
    }
   ],
   "source": [
    "tf.print(example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c : success , the change is only coming through the barrel of the gun .\n"
     ]
    }
   ],
   "source": [
    "tf.print(example[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "к : успех , перемены возможны только с оружием в руках .\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_static_value(example[0]).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сабворд токенизатор\n",
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for ru, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_ru = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (ru.numpy() for ru, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8073, 139, 13, 54, 7, 2298]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_string = tokenizer_en.encode('This is just a test')\n",
    "tokenized_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is just a test'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.decode(tokenized_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token=8073 ===> \"T\"\n",
      "token=139 ===> \"his \"\n",
      "token=13 ===> \"is \"\n",
      "token=54 ===> \"just \"\n",
      "token=7 ===> \"a \"\n",
      "token=2298 ===> \"test\"\n"
     ]
    }
   ],
   "source": [
    "for token in tokenized_string:\n",
    "    print(f'{token=} ===> \"{tokenizer_en.decode([token])}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция кодирования (с добавлением метки начала и конца)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(ru, en):\n",
    "    ru = [tokenizer_ru.vocab_size] + tokenizer_ru.encode(\n",
    "      ru.numpy()) + [tokenizer_ru.vocab_size+1]\n",
    "\n",
    "    en = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      en.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "\n",
    "    return ru, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(ru, en):\n",
    "    result_ru, result_en = tf.py_function(encode, [ru, en], [tf.int64, tf.int64])\n",
    "    result_ru.set_shape([None])\n",
    "    result_en.set_shape([None])\n",
    "\n",
    "    return result_ru, result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length, tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 38), dtype=int64, numpy=\n",
       " array([[8179,   57,   86, ...,    0,    0,    0],\n",
       "        [8179,    3,   38, ...,    0,    0,    0],\n",
       "        [8179,   57,  135, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8179,    3,    7, ...,    0,    0,    0],\n",
       "        [8179,  138,  250, ...,    0,    0,    0],\n",
       "        [8179,   19,    7, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8245,   90,  101, ...,    0,    0,    0],\n",
       "        [8245,   70,   25, ...,    0,    0,    0],\n",
       "        [8245,   90,  153, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8245,    4,   18, ...,    0,    0,    0],\n",
       "        [8245,   19,   59, ...,    0,    0,    0],\n",
       "        [8245,   24,   18, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_batch, en_batch = next(iter(val_dataset))\n",
    "ru_batch, en_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(38,), dtype=int64, numpy=\n",
       "array([8179,   57,   86,   54,  578,   84, 4550, 1003, 5905, 6326, 1197,\n",
       "          2, 8180,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoder - готовим матрицу векторов (для всех позиций в предложении не более длины max_len)\n",
    "# Потом будем их (соответствующий позиции вектор) добавлять к эмбеддингу каждого слова, \n",
    "# как кодированную позицию слова в предложении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $\n",
    "\n",
    "$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 2, 4, 4, 6, 6, 8, 8]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.arange(10)[np.newaxis, :]//2) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 0.96466162, 0.96466162, 0.93057204,\n",
       "        0.93057204, 0.89768713, 0.89768713, 0.86596432, 0.86596432,\n",
       "        0.83536255, 0.83536255, 0.80584219, 0.80584219, 0.77736503,\n",
       "        0.77736503, 0.74989421, 0.74989421, 0.72339416, 0.72339416,\n",
       "        0.69783058, 0.69783058, 0.67317038, 0.67317038, 0.64938163,\n",
       "        0.64938163, 0.62643354, 0.62643354, 0.60429639, 0.60429639,\n",
       "        0.58294153, 0.58294153, 0.56234133, 0.56234133, 0.54246909,\n",
       "        0.54246909, 0.52329911, 0.52329911, 0.50480657, 0.50480657,\n",
       "        0.48696753, 0.48696753, 0.46975888, 0.46975888, 0.45315836,\n",
       "        0.45315836, 0.43714448, 0.43714448, 0.4216965 , 0.4216965 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / np.power(10000, (2 * (np.arange(50)[np.newaxis, :]//2)) / np.float32(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [1.        , 1.        , 0.96466162, 0.96466162, 0.93057204],\n",
       "       [2.        , 2.        , 1.92932324, 1.92932324, 1.86114408],\n",
       "       [3.        , 3.        , 2.89398486, 2.89398486, 2.79171612],\n",
       "       [4.        , 4.        , 3.85864648, 3.85864648, 3.72228816]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# То, от чего берется синус (для четных элементов) и косинус для нечетных\n",
    "rates = (1 / np.power(10000, (2 * (np.arange(5)[np.newaxis, :]//2))\\\n",
    "                      / np.float32(512))) * np.arange(5)[:, np.newaxis]\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 2, 4, 4, 6, 6, 8, 8]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (np.arange(10)[np.newaxis, :]//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 512), dtype=float32, numpy=\n",
       "array([[[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
       "          1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
       "        [ 8.4147096e-01,  5.4030228e-01,  8.2185620e-01, ...,\n",
       "          1.0000000e+00,  1.0366329e-04,  1.0000000e+00],\n",
       "        [ 9.0929741e-01, -4.1614684e-01,  9.3641472e-01, ...,\n",
       "          1.0000000e+00,  2.0732658e-04,  1.0000000e+00],\n",
       "        [ 1.4112000e-01, -9.8999250e-01,  2.4508542e-01, ...,\n",
       "          9.9999994e-01,  3.1098988e-04,  9.9999994e-01],\n",
       "        [-7.5680250e-01, -6.5364361e-01, -6.5716684e-01, ...,\n",
       "          9.9999988e-01,  4.1465316e-04,  9.9999994e-01]]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoding(5, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция маскирующая паддинговые нули... не супер понятно для чего там эти два доп. измерения\n",
    "# Написано для логитов внимания - вопрос - почему два....\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Маска закрывающая еще не предсказанную часть фразы. Чтобы самовнимание ее не использовало\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled dot product attention\n",
    "# Функция вычисления весов самовнимания. В нее подаются по три вектора Q, K, V производные из эмбеддингов слов.\n",
    "# Для каждого слова берется Q и скалярно перемножается с K векторами всех слов - полученные веса скейлятся \n",
    "# на корень из длины вектора, от них берется софтмакс и на них множатся V вектора. \n",
    "# Все соскейленные V вектора суммируются в итоговый вектор self attention (output)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  # Непонятно, почему так? Зачем отнимать огромное значение, почему не умножить на что нибудь маленькое\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = tf.constant([[0.0, 1.0, 2.0, 3.0]])\n",
    "k = tf.constant([[1.0, 0.0, 1.0, 0.0]])\n",
    "v = tf.constant([[5.0, 5.0, 5.0, 5.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(q, k, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[5., 5., 5., 5.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.constant([[1]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[5., 5., 5., 5.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       " array([[ 4.244919,  4.244919, 16.32622 ,  4.244919],\n",
       "        [ 4.      ,  4.      , 20.      ,  4.      ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[0.62245935, 0.37754068],\n",
       "        [0.5       , 0.5       ]], dtype=float32)>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = tf.constant([[0.0, 1.0, 2.0, 3.0], [1.0, 2.0, 3.0, 5.0]])\n",
    "k = tf.constant([[1.0, 0.0, 1.0, 0.0], [2.0, 1.0, 0.0, 0.0]])\n",
    "v = tf.constant([[5.0, 5.0, 5.0, 5.0], [3.0, 3.0, 35.0, 3.0]])\n",
    "scaled_dot_product_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Многоголовость внимания\n",
    "# Слоев внимания несколько. Для каждого слоя внимания мы прогоняем эмбеддинги через три dense слоя wq, wk, wv\n",
    "# Таким образом получая Q K V вектора, которые разбираем на части с помощью reshape (сколько голов внимания - столько частей)\n",
    "# Для всех вычисляется самовнимание и мы собираем матрицы векторов V из каждого \"внимания\" и конкатенейтим их (также через reshape). \n",
    "# И прогоняем через выходной FC слой. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Полносвязная сеточка (будет использоваться в конце енкодера)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Энкодер в трансформере состоит из нескольких повторяющихся слоёв. Вот один слой энкодера\n",
    "# В нем Multi-Head-Attention блок, выходная полносвязная сеть, и блоки нормализации и дропаут..\n",
    "# Сначала данные попадают в MHA, затем дропаут, затем нормалайз, затем в полносвязную сеть и снова дропаут и нормалайз\n",
    "# Кроме того есть два скип-соединения, один обходит MHA с дропаутом, \n",
    "# второй полносвязную сеть с дропаутом, чтобы при большой глубине не затухали градиенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собственно сам энкодер\n",
    "# Берет исходные фразы, делает для них эмбеддинги, скейлит их на корень из размерности эмбеддинга, добавляет позишн\n",
    "# И прогоняет их через какое-то количество энкодер-слоев (рассмотренных выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "    \n",
    "    \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Декодер. Так-же как и энкодер состоит из повторяющихся слоёв. \n",
    "# Каждый слой берет входные данные (эмбеддинги фразы или то, что пришло из предыдущего слоя декодера)\n",
    "# Вычисляет много-слойное само-внимание (с ограничением по маске). Маска в данном случае не дает вниманию\n",
    "# Вычислять скор с токенами которые находятся правее предсказываемого в данный момент. (поскольку декодер \n",
    "# генерирует результат токен за токеном - маска сначала закрывает все токены, потом постепенно сдвигаясь вправо\n",
    "# открывает ту часть которая уже была обработана)\n",
    "# Далее следует дропаут и слой нормализации со скип-соединением снизу.\n",
    "# После этого вычисляется скор еще одного самовнимания, на этот раз в нем используются Q и K вектора из Энкодера.\n",
    "# А V вектор используется от первого внимания (после дропаута и нормализации)\n",
    "# Дальше снова дропаут и нормалайз со скипом. \n",
    "# Далее полносвязная сеть и еще один проход дропаут и нормалайз. \n",
    "# И результат (V вектора, и веса первого и второго внимания) выдаются наружу для передачи в следующий слой декодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собственно сам Декодер\n",
    "# Все уже было расписано выше в основном.. Берем входные данные, эмбеддим их, скейлим корнем из глубины,\n",
    "# Добавляем позишн энкодинг, дальше дропаут и прогоняем через пачку слоев декодера (зачем-то сохраняем веса внимания)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Весь Трансформер.. Энкодер, декодер и финальный полносвязный слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        \n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры... \n",
    "# В оригинальном трансформере эмбеддинг 512, слоев энкодере и декодере по 6, размерность полносвязных слоев 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 6\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_ru.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимайзер с variable learning rate. Формула из пэйпера [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лосс функция. И метрика. Поскольку секвенции имеют паддинг - нужно исключать его из вычислений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем по-тренировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train_full\"\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Процесс тренировки - в энкодер подаются исходные фразы, \n",
    "# В декодер подаются таргетный фразы. Таргет подается в двух видах, снизу в исходном, сверху в сдвинутом на -1, \n",
    "# Поскольку предсказание энкодера (и процесс тренировки тоже) происходит в пошаговом режиме (предсказывается\n",
    "# по одному токену за шаг, а следующее предсказание строится на результатах предыдущего)\n",
    "# Сдвиг таргета делается чтобы таргетом для текущего слова было следующее - как при тренировке генерации текста...\n",
    "# Используется forced supervised learning, то-есть независимо от того, что предсказал на предыдущем шаге декодер\n",
    "# Следующий таргет все равно выдается по плану. (не отступает от эталонной таргетной фразы)\n",
    "# Чтобы самовнимание не учитывало часть как-бы \"еще не предсказанной\" фразу - используется look-ahead-mask\n",
    "# Это по-сути очевидно только на этапе тренировки, поскольку на инференсе - эта часть фразы и так неизвестна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 9.0171 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.7330 Accuracy 0.0362\n",
      "Epoch 1 Batch 100 Loss 8.4700 Accuracy 0.0449\n",
      "Epoch 1 Batch 150 Loss 8.2470 Accuracy 0.0491\n",
      "Epoch 1 Batch 200 Loss 7.9909 Accuracy 0.0603\n",
      "Epoch 1 Batch 250 Loss 7.7247 Accuracy 0.0730\n",
      "Epoch 1 Batch 300 Loss 7.4934 Accuracy 0.0822\n",
      "Epoch 1 Batch 350 Loss 7.3075 Accuracy 0.0895\n",
      "Epoch 1 Batch 400 Loss 7.1563 Accuracy 0.0972\n",
      "Epoch 1 Batch 450 Loss 7.0201 Accuracy 0.1063\n",
      "Epoch 1 Batch 500 Loss 6.8926 Accuracy 0.1156\n",
      "Epoch 1 Batch 550 Loss 6.7675 Accuracy 0.1253\n",
      "Epoch 1 Batch 600 Loss 6.6533 Accuracy 0.1341\n",
      "Epoch 1 Batch 650 Loss 6.5475 Accuracy 0.1426\n",
      "Epoch 1 Batch 700 Loss 6.4493 Accuracy 0.1504\n",
      "Epoch 1 Batch 750 Loss 6.3592 Accuracy 0.1575\n",
      "Epoch 1 Batch 800 Loss 6.2752 Accuracy 0.1643\n",
      "Epoch 1 Batch 850 Loss 6.1963 Accuracy 0.1707\n",
      "Epoch 1 Batch 900 Loss 6.1258 Accuracy 0.1763\n",
      "Epoch 1 Batch 950 Loss 6.0611 Accuracy 0.1814\n",
      "Epoch 1 Batch 1000 Loss 5.9995 Accuracy 0.1863\n",
      "Epoch 1 Batch 1050 Loss 5.9430 Accuracy 0.1908\n",
      "Epoch 1 Batch 1100 Loss 5.8901 Accuracy 0.1950\n",
      "Epoch 1 Batch 1150 Loss 5.8406 Accuracy 0.1988\n",
      "Epoch 1 Batch 1200 Loss 5.7943 Accuracy 0.2024\n",
      "Epoch 1 Batch 1250 Loss 5.7497 Accuracy 0.2058\n",
      "Epoch 1 Batch 1300 Loss 5.7076 Accuracy 0.2092\n",
      "Epoch 1 Batch 1350 Loss 5.6676 Accuracy 0.2123\n",
      "Epoch 1 Batch 1400 Loss 5.6302 Accuracy 0.2152\n",
      "Epoch 1 Batch 1450 Loss 5.5943 Accuracy 0.2180\n",
      "Epoch 1 Batch 1500 Loss 5.5606 Accuracy 0.2206\n",
      "Epoch 1 Batch 1550 Loss 5.5293 Accuracy 0.2231\n",
      "Epoch 1 Batch 1600 Loss 5.4978 Accuracy 0.2256\n",
      "Epoch 1 Batch 1650 Loss 5.4684 Accuracy 0.2278\n",
      "Epoch 1 Batch 1700 Loss 5.4406 Accuracy 0.2299\n",
      "Epoch 1 Batch 1750 Loss 5.4131 Accuracy 0.2321\n",
      "Epoch 1 Batch 1800 Loss 5.3880 Accuracy 0.2339\n",
      "Epoch 1 Batch 1850 Loss 5.3633 Accuracy 0.2358\n",
      "Epoch 1 Batch 1900 Loss 5.3390 Accuracy 0.2378\n",
      "Epoch 1 Batch 1950 Loss 5.3147 Accuracy 0.2396\n",
      "Epoch 1 Batch 2000 Loss 5.2923 Accuracy 0.2413\n",
      "Epoch 1 Batch 2050 Loss 5.2704 Accuracy 0.2430\n",
      "Epoch 1 Batch 2100 Loss 5.2494 Accuracy 0.2447\n",
      "Epoch 1 Batch 2150 Loss 5.2283 Accuracy 0.2463\n",
      "Epoch 1 Batch 2200 Loss 5.2092 Accuracy 0.2478\n",
      "Epoch 1 Batch 2250 Loss 5.1902 Accuracy 0.2493\n",
      "Epoch 1 Batch 2300 Loss 5.1721 Accuracy 0.2507\n",
      "Epoch 1 Batch 2350 Loss 5.1542 Accuracy 0.2521\n",
      "Epoch 1 Batch 2400 Loss 5.1375 Accuracy 0.2533\n",
      "Epoch 1 Batch 2450 Loss 5.1207 Accuracy 0.2546\n",
      "Epoch 1 Batch 2500 Loss 5.1039 Accuracy 0.2559\n",
      "Epoch 1 Batch 2550 Loss 5.0878 Accuracy 0.2572\n",
      "Epoch 1 Batch 2600 Loss 5.0716 Accuracy 0.2584\n",
      "Epoch 1 Loss 5.0645 Accuracy 0.2589\n",
      "Time taken for 1 epoch: 487.9386854171753 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 4.3565 Accuracy 0.3100\n",
      "Epoch 2 Batch 50 Loss 4.2280 Accuracy 0.3235\n",
      "Epoch 2 Batch 100 Loss 4.2359 Accuracy 0.3233\n",
      "Epoch 2 Batch 150 Loss 4.2367 Accuracy 0.3236\n",
      "Epoch 2 Batch 200 Loss 4.2431 Accuracy 0.3223\n",
      "Epoch 2 Batch 250 Loss 4.2392 Accuracy 0.3228\n",
      "Epoch 2 Batch 300 Loss 4.2405 Accuracy 0.3224\n",
      "Epoch 2 Batch 350 Loss 4.2354 Accuracy 0.3226\n",
      "Epoch 2 Batch 400 Loss 4.2333 Accuracy 0.3227\n",
      "Epoch 2 Batch 450 Loss 4.2315 Accuracy 0.3226\n",
      "Epoch 2 Batch 500 Loss 4.2264 Accuracy 0.3231\n",
      "Epoch 2 Batch 550 Loss 4.2241 Accuracy 0.3231\n",
      "Epoch 2 Batch 600 Loss 4.2225 Accuracy 0.3233\n",
      "Epoch 2 Batch 650 Loss 4.2196 Accuracy 0.3235\n",
      "Epoch 2 Batch 700 Loss 4.2156 Accuracy 0.3235\n",
      "Epoch 2 Batch 750 Loss 4.2115 Accuracy 0.3237\n",
      "Epoch 2 Batch 800 Loss 4.2095 Accuracy 0.3239\n",
      "Epoch 2 Batch 850 Loss 4.2066 Accuracy 0.3239\n",
      "Epoch 2 Batch 900 Loss 4.2033 Accuracy 0.3242\n",
      "Epoch 2 Batch 950 Loss 4.2009 Accuracy 0.3242\n",
      "Epoch 2 Batch 1000 Loss 4.1997 Accuracy 0.3241\n",
      "Epoch 2 Batch 1050 Loss 4.1982 Accuracy 0.3240\n",
      "Epoch 2 Batch 1100 Loss 4.1963 Accuracy 0.3240\n",
      "Epoch 2 Batch 1150 Loss 4.1943 Accuracy 0.3241\n",
      "Epoch 2 Batch 1200 Loss 4.1937 Accuracy 0.3240\n",
      "Epoch 2 Batch 1250 Loss 4.1929 Accuracy 0.3239\n",
      "Epoch 2 Batch 1300 Loss 4.1922 Accuracy 0.3238\n",
      "Epoch 2 Batch 1350 Loss 4.1913 Accuracy 0.3238\n",
      "Epoch 2 Batch 1400 Loss 4.1898 Accuracy 0.3238\n",
      "Epoch 2 Batch 1450 Loss 4.1887 Accuracy 0.3237\n",
      "Epoch 2 Batch 1500 Loss 4.1878 Accuracy 0.3237\n",
      "Epoch 2 Batch 1550 Loss 4.1866 Accuracy 0.3237\n",
      "Epoch 2 Batch 1600 Loss 4.1864 Accuracy 0.3236\n",
      "Epoch 2 Batch 1650 Loss 4.1841 Accuracy 0.3238\n",
      "Epoch 2 Batch 1700 Loss 4.1824 Accuracy 0.3239\n",
      "Epoch 2 Batch 1750 Loss 4.1814 Accuracy 0.3238\n",
      "Epoch 2 Batch 1800 Loss 4.1793 Accuracy 0.3239\n",
      "Epoch 2 Batch 1850 Loss 4.1779 Accuracy 0.3239\n",
      "Epoch 2 Batch 1900 Loss 4.1761 Accuracy 0.3240\n",
      "Epoch 2 Batch 1950 Loss 4.1747 Accuracy 0.3240\n",
      "Epoch 2 Batch 2000 Loss 4.1727 Accuracy 0.3241\n",
      "Epoch 2 Batch 2050 Loss 4.1703 Accuracy 0.3243\n",
      "Epoch 2 Batch 2100 Loss 4.1683 Accuracy 0.3244\n",
      "Epoch 2 Batch 2150 Loss 4.1664 Accuracy 0.3245\n",
      "Epoch 2 Batch 2200 Loss 4.1644 Accuracy 0.3245\n",
      "Epoch 2 Batch 2250 Loss 4.1625 Accuracy 0.3246\n",
      "Epoch 2 Batch 2300 Loss 4.1594 Accuracy 0.3248\n",
      "Epoch 2 Batch 2350 Loss 4.1570 Accuracy 0.3248\n",
      "Epoch 2 Batch 2400 Loss 4.1555 Accuracy 0.3249\n",
      "Epoch 2 Batch 2450 Loss 4.1536 Accuracy 0.3249\n",
      "Epoch 2 Batch 2500 Loss 4.1524 Accuracy 0.3250\n",
      "Epoch 2 Batch 2550 Loss 4.1502 Accuracy 0.3251\n",
      "Epoch 2 Batch 2600 Loss 4.1481 Accuracy 0.3252\n",
      "Epoch 2 Loss 4.1466 Accuracy 0.3253\n",
      "Time taken for 1 epoch: 461.87296509742737 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.9011 Accuracy 0.3333\n",
      "Epoch 3 Batch 50 Loss 4.0078 Accuracy 0.3307\n",
      "Epoch 3 Batch 100 Loss 4.0138 Accuracy 0.3310\n",
      "Epoch 3 Batch 150 Loss 4.0125 Accuracy 0.3321\n",
      "Epoch 3 Batch 200 Loss 4.0125 Accuracy 0.3321\n",
      "Epoch 3 Batch 250 Loss 4.0115 Accuracy 0.3321\n",
      "Epoch 3 Batch 300 Loss 4.0057 Accuracy 0.3321\n",
      "Epoch 3 Batch 350 Loss 4.0030 Accuracy 0.3324\n",
      "Epoch 3 Batch 400 Loss 3.9977 Accuracy 0.3328\n",
      "Epoch 3 Batch 450 Loss 3.9983 Accuracy 0.3327\n",
      "Epoch 3 Batch 500 Loss 3.9977 Accuracy 0.3327\n",
      "Epoch 3 Batch 550 Loss 3.9988 Accuracy 0.3325\n",
      "Epoch 3 Batch 600 Loss 3.9958 Accuracy 0.3330\n",
      "Epoch 3 Batch 650 Loss 3.9944 Accuracy 0.3332\n",
      "Epoch 3 Batch 700 Loss 3.9918 Accuracy 0.3334\n",
      "Epoch 3 Batch 750 Loss 3.9883 Accuracy 0.3336\n",
      "Epoch 3 Batch 800 Loss 3.9872 Accuracy 0.3337\n",
      "Epoch 3 Batch 850 Loss 3.9844 Accuracy 0.3341\n",
      "Epoch 3 Batch 900 Loss 3.9813 Accuracy 0.3343\n",
      "Epoch 3 Batch 950 Loss 3.9787 Accuracy 0.3345\n",
      "Epoch 3 Batch 1000 Loss 3.9763 Accuracy 0.3347\n",
      "Epoch 3 Batch 1050 Loss 3.9752 Accuracy 0.3348\n",
      "Epoch 3 Batch 1100 Loss 3.9736 Accuracy 0.3349\n",
      "Epoch 3 Batch 1150 Loss 3.9716 Accuracy 0.3352\n",
      "Epoch 3 Batch 1200 Loss 3.9702 Accuracy 0.3351\n",
      "Epoch 3 Batch 1250 Loss 3.9670 Accuracy 0.3355\n",
      "Epoch 3 Batch 1300 Loss 3.9651 Accuracy 0.3356\n",
      "Epoch 3 Batch 1350 Loss 3.9639 Accuracy 0.3357\n",
      "Epoch 3 Batch 1400 Loss 3.9620 Accuracy 0.3359\n",
      "Epoch 3 Batch 1450 Loss 3.9595 Accuracy 0.3361\n",
      "Epoch 3 Batch 1500 Loss 3.9584 Accuracy 0.3362\n",
      "Epoch 3 Batch 1550 Loss 3.9568 Accuracy 0.3363\n",
      "Epoch 3 Batch 1600 Loss 3.9558 Accuracy 0.3364\n",
      "Epoch 3 Batch 1650 Loss 3.9545 Accuracy 0.3365\n",
      "Epoch 3 Batch 1700 Loss 3.9522 Accuracy 0.3367\n",
      "Epoch 3 Batch 1750 Loss 3.9507 Accuracy 0.3367\n",
      "Epoch 3 Batch 1800 Loss 3.9487 Accuracy 0.3369\n",
      "Epoch 3 Batch 1850 Loss 3.9468 Accuracy 0.3370\n",
      "Epoch 3 Batch 1900 Loss 3.9452 Accuracy 0.3371\n",
      "Epoch 3 Batch 1950 Loss 3.9425 Accuracy 0.3374\n",
      "Epoch 3 Batch 2000 Loss 3.9408 Accuracy 0.3375\n",
      "Epoch 3 Batch 2050 Loss 3.9388 Accuracy 0.3376\n",
      "Epoch 3 Batch 2100 Loss 3.9369 Accuracy 0.3378\n",
      "Epoch 3 Batch 2150 Loss 3.9348 Accuracy 0.3380\n",
      "Epoch 3 Batch 2200 Loss 3.9334 Accuracy 0.3381\n",
      "Epoch 3 Batch 2250 Loss 3.9315 Accuracy 0.3383\n",
      "Epoch 3 Batch 2300 Loss 3.9295 Accuracy 0.3385\n",
      "Epoch 3 Batch 2350 Loss 3.9277 Accuracy 0.3386\n",
      "Epoch 3 Batch 2400 Loss 3.9264 Accuracy 0.3387\n",
      "Epoch 3 Batch 2450 Loss 3.9242 Accuracy 0.3390\n",
      "Epoch 3 Batch 2500 Loss 3.9234 Accuracy 0.3390\n",
      "Epoch 3 Batch 2550 Loss 3.9214 Accuracy 0.3391\n",
      "Epoch 3 Batch 2600 Loss 3.9198 Accuracy 0.3393\n",
      "Epoch 3 Loss 3.9194 Accuracy 0.3393\n",
      "Time taken for 1 epoch: 457.5914041996002 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 3.7414 Accuracy 0.3472\n",
      "Epoch 4 Batch 50 Loss 3.8010 Accuracy 0.3444\n",
      "Epoch 4 Batch 100 Loss 3.7964 Accuracy 0.3467\n",
      "Epoch 4 Batch 150 Loss 3.7991 Accuracy 0.3471\n",
      "Epoch 4 Batch 200 Loss 3.7993 Accuracy 0.3476\n",
      "Epoch 4 Batch 250 Loss 3.7986 Accuracy 0.3484\n",
      "Epoch 4 Batch 300 Loss 3.7982 Accuracy 0.3489\n",
      "Epoch 4 Batch 350 Loss 3.7990 Accuracy 0.3488\n",
      "Epoch 4 Batch 400 Loss 3.8004 Accuracy 0.3487\n",
      "Epoch 4 Batch 450 Loss 3.8007 Accuracy 0.3486\n",
      "Epoch 4 Batch 500 Loss 3.7998 Accuracy 0.3490\n",
      "Epoch 4 Batch 550 Loss 3.7978 Accuracy 0.3490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 600 Loss 3.7957 Accuracy 0.3492\n",
      "Epoch 4 Batch 650 Loss 3.7934 Accuracy 0.3494\n",
      "Epoch 4 Batch 700 Loss 3.7920 Accuracy 0.3494\n",
      "Epoch 4 Batch 750 Loss 3.7904 Accuracy 0.3495\n",
      "Epoch 4 Batch 800 Loss 3.7891 Accuracy 0.3495\n",
      "Epoch 4 Batch 850 Loss 3.7897 Accuracy 0.3494\n",
      "Epoch 4 Batch 900 Loss 3.7889 Accuracy 0.3494\n",
      "Epoch 4 Batch 950 Loss 3.7873 Accuracy 0.3495\n",
      "Epoch 4 Batch 1000 Loss 3.7855 Accuracy 0.3497\n",
      "Epoch 4 Batch 1050 Loss 3.7837 Accuracy 0.3499\n",
      "Epoch 4 Batch 1100 Loss 3.7822 Accuracy 0.3500\n",
      "Epoch 4 Batch 1150 Loss 3.7809 Accuracy 0.3501\n",
      "Epoch 4 Batch 1200 Loss 3.7800 Accuracy 0.3502\n",
      "Epoch 4 Batch 1250 Loss 3.7784 Accuracy 0.3504\n",
      "Epoch 4 Batch 1300 Loss 3.7774 Accuracy 0.3504\n",
      "Epoch 4 Batch 1350 Loss 3.7762 Accuracy 0.3505\n",
      "Epoch 4 Batch 1400 Loss 3.7744 Accuracy 0.3507\n",
      "Epoch 4 Batch 1450 Loss 3.7742 Accuracy 0.3508\n",
      "Epoch 4 Batch 1500 Loss 3.7731 Accuracy 0.3508\n",
      "Epoch 4 Batch 1550 Loss 3.7710 Accuracy 0.3511\n",
      "Epoch 4 Batch 1600 Loss 3.7695 Accuracy 0.3512\n",
      "Epoch 4 Batch 1650 Loss 3.7685 Accuracy 0.3512\n",
      "Epoch 4 Batch 1700 Loss 3.7680 Accuracy 0.3512\n",
      "Epoch 4 Batch 1750 Loss 3.7662 Accuracy 0.3514\n",
      "Epoch 4 Batch 1800 Loss 3.7653 Accuracy 0.3515\n",
      "Epoch 4 Batch 1850 Loss 3.7640 Accuracy 0.3516\n",
      "Epoch 4 Batch 1900 Loss 3.7632 Accuracy 0.3517\n",
      "Epoch 4 Batch 1950 Loss 3.7622 Accuracy 0.3517\n",
      "Epoch 4 Batch 2000 Loss 3.7611 Accuracy 0.3517\n",
      "Epoch 4 Batch 2050 Loss 3.7597 Accuracy 0.3519\n",
      "Epoch 4 Batch 2100 Loss 3.7580 Accuracy 0.3521\n",
      "Epoch 4 Batch 2150 Loss 3.7568 Accuracy 0.3522\n",
      "Epoch 4 Batch 2200 Loss 3.7562 Accuracy 0.3522\n",
      "Epoch 4 Batch 2250 Loss 3.7545 Accuracy 0.3524\n",
      "Epoch 4 Batch 2300 Loss 3.7536 Accuracy 0.3525\n",
      "Epoch 4 Batch 2350 Loss 3.7529 Accuracy 0.3525\n",
      "Epoch 4 Batch 2400 Loss 3.7520 Accuracy 0.3525\n",
      "Epoch 4 Batch 2450 Loss 3.7500 Accuracy 0.3527\n",
      "Epoch 4 Batch 2500 Loss 3.7487 Accuracy 0.3528\n",
      "Epoch 4 Batch 2550 Loss 3.7473 Accuracy 0.3529\n",
      "Epoch 4 Batch 2600 Loss 3.7463 Accuracy 0.3529\n",
      "Epoch 4 Loss 3.7461 Accuracy 0.3529\n",
      "Time taken for 1 epoch: 475.0916142463684 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 3.5473 Accuracy 0.3846\n",
      "Epoch 5 Batch 50 Loss 3.6545 Accuracy 0.3610\n",
      "Epoch 5 Batch 100 Loss 3.6506 Accuracy 0.3611\n",
      "Epoch 5 Batch 150 Loss 3.6615 Accuracy 0.3601\n",
      "Epoch 5 Batch 200 Loss 3.6610 Accuracy 0.3600\n",
      "Epoch 5 Batch 250 Loss 3.6578 Accuracy 0.3600\n",
      "Epoch 5 Batch 300 Loss 3.6563 Accuracy 0.3599\n",
      "Epoch 5 Batch 350 Loss 3.6518 Accuracy 0.3605\n",
      "Epoch 5 Batch 400 Loss 3.6514 Accuracy 0.3606\n",
      "Epoch 5 Batch 450 Loss 3.6488 Accuracy 0.3610\n",
      "Epoch 5 Batch 500 Loss 3.6472 Accuracy 0.3612\n",
      "Epoch 5 Batch 550 Loss 3.6490 Accuracy 0.3606\n",
      "Epoch 5 Batch 600 Loss 3.6482 Accuracy 0.3604\n",
      "Epoch 5 Batch 650 Loss 3.6482 Accuracy 0.3605\n",
      "Epoch 5 Batch 700 Loss 3.6482 Accuracy 0.3604\n",
      "Epoch 5 Batch 750 Loss 3.6474 Accuracy 0.3606\n",
      "Epoch 5 Batch 800 Loss 3.6485 Accuracy 0.3605\n",
      "Epoch 5 Batch 850 Loss 3.6468 Accuracy 0.3607\n",
      "Epoch 5 Batch 900 Loss 3.6455 Accuracy 0.3608\n",
      "Epoch 5 Batch 950 Loss 3.6449 Accuracy 0.3610\n",
      "Epoch 5 Batch 1000 Loss 3.6427 Accuracy 0.3612\n",
      "Epoch 5 Batch 1050 Loss 3.6413 Accuracy 0.3613\n",
      "Epoch 5 Batch 1100 Loss 3.6393 Accuracy 0.3614\n",
      "Epoch 5 Batch 1150 Loss 3.6396 Accuracy 0.3614\n",
      "Epoch 5 Batch 1200 Loss 3.6391 Accuracy 0.3614\n",
      "Epoch 5 Batch 1250 Loss 3.6398 Accuracy 0.3614\n",
      "Epoch 5 Batch 1300 Loss 3.6394 Accuracy 0.3614\n",
      "Epoch 5 Batch 1350 Loss 3.6382 Accuracy 0.3616\n",
      "Epoch 5 Batch 1400 Loss 3.6381 Accuracy 0.3617\n",
      "Epoch 5 Batch 1450 Loss 3.6368 Accuracy 0.3619\n",
      "Epoch 5 Batch 1500 Loss 3.6349 Accuracy 0.3622\n",
      "Epoch 5 Batch 1550 Loss 3.6342 Accuracy 0.3623\n",
      "Epoch 5 Batch 1600 Loss 3.6335 Accuracy 0.3625\n",
      "Epoch 5 Batch 1650 Loss 3.6328 Accuracy 0.3625\n",
      "Epoch 5 Batch 1700 Loss 3.6328 Accuracy 0.3625\n",
      "Epoch 5 Batch 1750 Loss 3.6316 Accuracy 0.3626\n",
      "Epoch 5 Batch 1800 Loss 3.6312 Accuracy 0.3626\n",
      "Epoch 5 Batch 1850 Loss 3.6313 Accuracy 0.3626\n",
      "Epoch 5 Batch 1900 Loss 3.6308 Accuracy 0.3626\n",
      "Epoch 5 Batch 1950 Loss 3.6306 Accuracy 0.3626\n",
      "Epoch 5 Batch 2000 Loss 3.6304 Accuracy 0.3626\n",
      "Epoch 5 Batch 2050 Loss 3.6294 Accuracy 0.3627\n",
      "Epoch 5 Batch 2100 Loss 3.6272 Accuracy 0.3629\n",
      "Epoch 5 Batch 2150 Loss 3.6260 Accuracy 0.3631\n",
      "Epoch 5 Batch 2200 Loss 3.6259 Accuracy 0.3630\n",
      "Epoch 5 Batch 2250 Loss 3.6248 Accuracy 0.3631\n",
      "Epoch 5 Batch 2300 Loss 3.6236 Accuracy 0.3632\n",
      "Epoch 5 Batch 2350 Loss 3.6230 Accuracy 0.3633\n",
      "Epoch 5 Batch 2400 Loss 3.6225 Accuracy 0.3633\n",
      "Epoch 5 Batch 2450 Loss 3.6216 Accuracy 0.3634\n",
      "Epoch 5 Batch 2500 Loss 3.6211 Accuracy 0.3634\n",
      "Epoch 5 Batch 2550 Loss 3.6203 Accuracy 0.3635\n",
      "Epoch 5 Batch 2600 Loss 3.6198 Accuracy 0.3635\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train_full/ckpt-1\n",
      "Epoch 5 Loss 3.6196 Accuracy 0.3635\n",
      "Time taken for 1 epoch: 494.7652328014374 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 3.6690 Accuracy 0.3657\n",
      "Epoch 6 Batch 50 Loss 3.5532 Accuracy 0.3679\n",
      "Epoch 6 Batch 100 Loss 3.5595 Accuracy 0.3673\n",
      "Epoch 6 Batch 150 Loss 3.5542 Accuracy 0.3688\n",
      "Epoch 6 Batch 200 Loss 3.5449 Accuracy 0.3697\n",
      "Epoch 6 Batch 250 Loss 3.5424 Accuracy 0.3703\n",
      "Epoch 6 Batch 300 Loss 3.5428 Accuracy 0.3703\n",
      "Epoch 6 Batch 350 Loss 3.5439 Accuracy 0.3699\n",
      "Epoch 6 Batch 400 Loss 3.5438 Accuracy 0.3697\n",
      "Epoch 6 Batch 450 Loss 3.5444 Accuracy 0.3700\n",
      "Epoch 6 Batch 500 Loss 3.5447 Accuracy 0.3699\n",
      "Epoch 6 Batch 550 Loss 3.5448 Accuracy 0.3699\n",
      "Epoch 6 Batch 600 Loss 3.5426 Accuracy 0.3702\n",
      "Epoch 6 Batch 650 Loss 3.5417 Accuracy 0.3703\n",
      "Epoch 6 Batch 700 Loss 3.5406 Accuracy 0.3705\n",
      "Epoch 6 Batch 750 Loss 3.5390 Accuracy 0.3706\n",
      "Epoch 6 Batch 800 Loss 3.5388 Accuracy 0.3706\n",
      "Epoch 6 Batch 850 Loss 3.5391 Accuracy 0.3706\n",
      "Epoch 6 Batch 900 Loss 3.5379 Accuracy 0.3706\n",
      "Epoch 6 Batch 950 Loss 3.5386 Accuracy 0.3707\n",
      "Epoch 6 Batch 1000 Loss 3.5374 Accuracy 0.3709\n",
      "Epoch 6 Batch 1050 Loss 3.5365 Accuracy 0.3709\n",
      "Epoch 6 Batch 1100 Loss 3.5351 Accuracy 0.3711\n",
      "Epoch 6 Batch 1150 Loss 3.5332 Accuracy 0.3713\n",
      "Epoch 6 Batch 1200 Loss 3.5332 Accuracy 0.3714\n",
      "Epoch 6 Batch 1250 Loss 3.5321 Accuracy 0.3715\n",
      "Epoch 6 Batch 1300 Loss 3.5313 Accuracy 0.3715\n",
      "Epoch 6 Batch 1350 Loss 3.5307 Accuracy 0.3716\n",
      "Epoch 6 Batch 1400 Loss 3.5294 Accuracy 0.3718\n",
      "Epoch 6 Batch 1450 Loss 3.5288 Accuracy 0.3718\n",
      "Epoch 6 Batch 1500 Loss 3.5280 Accuracy 0.3720\n",
      "Epoch 6 Batch 1550 Loss 3.5280 Accuracy 0.3720\n",
      "Epoch 6 Batch 1600 Loss 3.5276 Accuracy 0.3720\n",
      "Epoch 6 Batch 1650 Loss 3.5269 Accuracy 0.3721\n",
      "Epoch 6 Batch 1700 Loss 3.5264 Accuracy 0.3722\n",
      "Epoch 6 Batch 1750 Loss 3.5256 Accuracy 0.3723\n",
      "Epoch 6 Batch 1800 Loss 3.5240 Accuracy 0.3724\n",
      "Epoch 6 Batch 1850 Loss 3.5242 Accuracy 0.3724\n",
      "Epoch 6 Batch 1900 Loss 3.5238 Accuracy 0.3724\n",
      "Epoch 6 Batch 1950 Loss 3.5229 Accuracy 0.3725\n",
      "Epoch 6 Batch 2000 Loss 3.5222 Accuracy 0.3726\n",
      "Epoch 6 Batch 2050 Loss 3.5220 Accuracy 0.3726\n",
      "Epoch 6 Batch 2100 Loss 3.5214 Accuracy 0.3726\n",
      "Epoch 6 Batch 2150 Loss 3.5205 Accuracy 0.3727\n",
      "Epoch 6 Batch 2200 Loss 3.5201 Accuracy 0.3727\n",
      "Epoch 6 Batch 2250 Loss 3.5197 Accuracy 0.3727\n",
      "Epoch 6 Batch 2300 Loss 3.5189 Accuracy 0.3728\n",
      "Epoch 6 Batch 2350 Loss 3.5183 Accuracy 0.3729\n",
      "Epoch 6 Batch 2400 Loss 3.5176 Accuracy 0.3729\n",
      "Epoch 6 Batch 2450 Loss 3.5175 Accuracy 0.3729\n",
      "Epoch 6 Batch 2500 Loss 3.5167 Accuracy 0.3729\n",
      "Epoch 6 Batch 2550 Loss 3.5165 Accuracy 0.3729\n",
      "Epoch 6 Batch 2600 Loss 3.5158 Accuracy 0.3730\n",
      "Epoch 6 Loss 3.5160 Accuracy 0.3730\n",
      "Time taken for 1 epoch: 480.35352540016174 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 3.4628 Accuracy 0.3709\n",
      "Epoch 7 Batch 50 Loss 3.4764 Accuracy 0.3739\n",
      "Epoch 7 Batch 100 Loss 3.4585 Accuracy 0.3754\n",
      "Epoch 7 Batch 150 Loss 3.4594 Accuracy 0.3761\n",
      "Epoch 7 Batch 200 Loss 3.4616 Accuracy 0.3769\n",
      "Epoch 7 Batch 250 Loss 3.4575 Accuracy 0.3770\n",
      "Epoch 7 Batch 300 Loss 3.4570 Accuracy 0.3772\n",
      "Epoch 7 Batch 350 Loss 3.4547 Accuracy 0.3775\n",
      "Epoch 7 Batch 400 Loss 3.4575 Accuracy 0.3772\n",
      "Epoch 7 Batch 450 Loss 3.4536 Accuracy 0.3780\n",
      "Epoch 7 Batch 500 Loss 3.4543 Accuracy 0.3780\n",
      "Epoch 7 Batch 550 Loss 3.4537 Accuracy 0.3781\n",
      "Epoch 7 Batch 600 Loss 3.4543 Accuracy 0.3780\n",
      "Epoch 7 Batch 650 Loss 3.4563 Accuracy 0.3778\n",
      "Epoch 7 Batch 700 Loss 3.4550 Accuracy 0.3780\n",
      "Epoch 7 Batch 750 Loss 3.4538 Accuracy 0.3781\n",
      "Epoch 7 Batch 800 Loss 3.4537 Accuracy 0.3781\n",
      "Epoch 7 Batch 850 Loss 3.4510 Accuracy 0.3783\n",
      "Epoch 7 Batch 900 Loss 3.4506 Accuracy 0.3784\n",
      "Epoch 7 Batch 950 Loss 3.4500 Accuracy 0.3786\n",
      "Epoch 7 Batch 1000 Loss 3.4501 Accuracy 0.3785\n",
      "Epoch 7 Batch 1050 Loss 3.4500 Accuracy 0.3785\n",
      "Epoch 7 Batch 1100 Loss 3.4490 Accuracy 0.3786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 1150 Loss 3.4488 Accuracy 0.3788\n",
      "Epoch 7 Batch 1200 Loss 3.4486 Accuracy 0.3787\n",
      "Epoch 7 Batch 1250 Loss 3.4476 Accuracy 0.3788\n",
      "Epoch 7 Batch 1300 Loss 3.4469 Accuracy 0.3790\n",
      "Epoch 7 Batch 1350 Loss 3.4455 Accuracy 0.3790\n",
      "Epoch 7 Batch 1400 Loss 3.4453 Accuracy 0.3790\n",
      "Epoch 7 Batch 1450 Loss 3.4452 Accuracy 0.3791\n",
      "Epoch 7 Batch 1500 Loss 3.4441 Accuracy 0.3793\n",
      "Epoch 7 Batch 1550 Loss 3.4434 Accuracy 0.3794\n",
      "Epoch 7 Batch 1600 Loss 3.4439 Accuracy 0.3793\n",
      "Epoch 7 Batch 1650 Loss 3.4428 Accuracy 0.3794\n",
      "Epoch 7 Batch 1700 Loss 3.4427 Accuracy 0.3795\n",
      "Epoch 7 Batch 1750 Loss 3.4420 Accuracy 0.3795\n",
      "Epoch 7 Batch 1800 Loss 3.4414 Accuracy 0.3796\n",
      "Epoch 7 Batch 1850 Loss 3.4409 Accuracy 0.3797\n",
      "Epoch 7 Batch 1900 Loss 3.4405 Accuracy 0.3798\n",
      "Epoch 7 Batch 1950 Loss 3.4399 Accuracy 0.3798\n",
      "Epoch 7 Batch 2000 Loss 3.4399 Accuracy 0.3798\n",
      "Epoch 7 Batch 2050 Loss 3.4391 Accuracy 0.3799\n",
      "Epoch 7 Batch 2100 Loss 3.4388 Accuracy 0.3800\n",
      "Epoch 7 Batch 2150 Loss 3.4393 Accuracy 0.3799\n",
      "Epoch 7 Batch 2200 Loss 3.4387 Accuracy 0.3800\n",
      "Epoch 7 Batch 2250 Loss 3.4379 Accuracy 0.3800\n",
      "Epoch 7 Batch 2300 Loss 3.4375 Accuracy 0.3801\n",
      "Epoch 7 Batch 2350 Loss 3.4366 Accuracy 0.3802\n",
      "Epoch 7 Batch 2400 Loss 3.4365 Accuracy 0.3802\n",
      "Epoch 7 Batch 2450 Loss 3.4357 Accuracy 0.3803\n",
      "Epoch 7 Batch 2500 Loss 3.4350 Accuracy 0.3803\n",
      "Epoch 7 Batch 2550 Loss 3.4344 Accuracy 0.3804\n",
      "Epoch 7 Batch 2600 Loss 3.4343 Accuracy 0.3804\n",
      "Epoch 7 Loss 3.4342 Accuracy 0.3804\n",
      "Time taken for 1 epoch: 475.69864892959595 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 3.4766 Accuracy 0.3632\n",
      "Epoch 8 Batch 50 Loss 3.3991 Accuracy 0.3832\n",
      "Epoch 8 Batch 100 Loss 3.3795 Accuracy 0.3852\n",
      "Epoch 8 Batch 150 Loss 3.3783 Accuracy 0.3848\n",
      "Epoch 8 Batch 200 Loss 3.3728 Accuracy 0.3852\n",
      "Epoch 8 Batch 250 Loss 3.3712 Accuracy 0.3861\n",
      "Epoch 8 Batch 300 Loss 3.3709 Accuracy 0.3857\n",
      "Epoch 8 Batch 350 Loss 3.3751 Accuracy 0.3853\n",
      "Epoch 8 Batch 400 Loss 3.3730 Accuracy 0.3853\n",
      "Epoch 8 Batch 450 Loss 3.3720 Accuracy 0.3857\n",
      "Epoch 8 Batch 500 Loss 3.3720 Accuracy 0.3858\n",
      "Epoch 8 Batch 550 Loss 3.3750 Accuracy 0.3853\n",
      "Epoch 8 Batch 600 Loss 3.3764 Accuracy 0.3853\n",
      "Epoch 8 Batch 650 Loss 3.3767 Accuracy 0.3852\n",
      "Epoch 8 Batch 700 Loss 3.3767 Accuracy 0.3853\n",
      "Epoch 8 Batch 750 Loss 3.3765 Accuracy 0.3853\n",
      "Epoch 8 Batch 800 Loss 3.3760 Accuracy 0.3853\n",
      "Epoch 8 Batch 850 Loss 3.3767 Accuracy 0.3852\n",
      "Epoch 8 Batch 900 Loss 3.3741 Accuracy 0.3855\n",
      "Epoch 8 Batch 950 Loss 3.3734 Accuracy 0.3856\n",
      "Epoch 8 Batch 1000 Loss 3.3741 Accuracy 0.3854\n",
      "Epoch 8 Batch 1050 Loss 3.3734 Accuracy 0.3855\n",
      "Epoch 8 Batch 1100 Loss 3.3722 Accuracy 0.3856\n",
      "Epoch 8 Batch 1150 Loss 3.3722 Accuracy 0.3857\n",
      "Epoch 8 Batch 1200 Loss 3.3711 Accuracy 0.3859\n",
      "Epoch 8 Batch 1250 Loss 3.3696 Accuracy 0.3861\n",
      "Epoch 8 Batch 1300 Loss 3.3689 Accuracy 0.3861\n",
      "Epoch 8 Batch 1350 Loss 3.3692 Accuracy 0.3861\n",
      "Epoch 8 Batch 1400 Loss 3.3686 Accuracy 0.3861\n",
      "Epoch 8 Batch 1450 Loss 3.3683 Accuracy 0.3861\n",
      "Epoch 8 Batch 1500 Loss 3.3682 Accuracy 0.3862\n",
      "Epoch 8 Batch 1550 Loss 3.3672 Accuracy 0.3863\n",
      "Epoch 8 Batch 1600 Loss 3.3668 Accuracy 0.3863\n",
      "Epoch 8 Batch 1650 Loss 3.3664 Accuracy 0.3863\n",
      "Epoch 8 Batch 1700 Loss 3.3647 Accuracy 0.3865\n",
      "Epoch 8 Batch 1750 Loss 3.3646 Accuracy 0.3867\n",
      "Epoch 8 Batch 1800 Loss 3.3651 Accuracy 0.3866\n",
      "Epoch 8 Batch 1850 Loss 3.3648 Accuracy 0.3866\n",
      "Epoch 8 Batch 1900 Loss 3.3646 Accuracy 0.3866\n",
      "Epoch 8 Batch 1950 Loss 3.3646 Accuracy 0.3866\n",
      "Epoch 8 Batch 2000 Loss 3.3642 Accuracy 0.3866\n",
      "Epoch 8 Batch 2050 Loss 3.3631 Accuracy 0.3867\n",
      "Epoch 8 Batch 2100 Loss 3.3635 Accuracy 0.3867\n",
      "Epoch 8 Batch 2150 Loss 3.3631 Accuracy 0.3867\n",
      "Epoch 8 Batch 2200 Loss 3.3627 Accuracy 0.3867\n",
      "Epoch 8 Batch 2250 Loss 3.3621 Accuracy 0.3868\n",
      "Epoch 8 Batch 2300 Loss 3.3620 Accuracy 0.3868\n",
      "Epoch 8 Batch 2350 Loss 3.3613 Accuracy 0.3869\n",
      "Epoch 8 Batch 2400 Loss 3.3614 Accuracy 0.3869\n",
      "Epoch 8 Batch 2450 Loss 3.3610 Accuracy 0.3869\n",
      "Epoch 8 Batch 2500 Loss 3.3609 Accuracy 0.3869\n",
      "Epoch 8 Batch 2550 Loss 3.3605 Accuracy 0.3870\n",
      "Epoch 8 Batch 2600 Loss 3.3601 Accuracy 0.3870\n",
      "Epoch 8 Loss 3.3596 Accuracy 0.3871\n",
      "Time taken for 1 epoch: 470.01160526275635 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 3.2968 Accuracy 0.4017\n",
      "Epoch 9 Batch 50 Loss 3.2678 Accuracy 0.3960\n",
      "Epoch 9 Batch 100 Loss 3.2900 Accuracy 0.3935\n",
      "Epoch 9 Batch 150 Loss 3.2971 Accuracy 0.3925\n",
      "Epoch 9 Batch 200 Loss 3.3010 Accuracy 0.3920\n",
      "Epoch 9 Batch 250 Loss 3.3051 Accuracy 0.3914\n",
      "Epoch 9 Batch 300 Loss 3.3082 Accuracy 0.3907\n",
      "Epoch 9 Batch 350 Loss 3.3043 Accuracy 0.3909\n",
      "Epoch 9 Batch 400 Loss 3.3075 Accuracy 0.3908\n",
      "Epoch 9 Batch 450 Loss 3.3075 Accuracy 0.3911\n",
      "Epoch 9 Batch 500 Loss 3.3060 Accuracy 0.3912\n",
      "Epoch 9 Batch 550 Loss 3.3070 Accuracy 0.3911\n",
      "Epoch 9 Batch 600 Loss 3.3057 Accuracy 0.3912\n",
      "Epoch 9 Batch 650 Loss 3.3054 Accuracy 0.3914\n",
      "Epoch 9 Batch 700 Loss 3.3051 Accuracy 0.3914\n",
      "Epoch 9 Batch 750 Loss 3.3058 Accuracy 0.3914\n",
      "Epoch 9 Batch 800 Loss 3.3054 Accuracy 0.3915\n",
      "Epoch 9 Batch 850 Loss 3.3050 Accuracy 0.3915\n",
      "Epoch 9 Batch 900 Loss 3.3066 Accuracy 0.3914\n",
      "Epoch 9 Batch 950 Loss 3.3066 Accuracy 0.3914\n",
      "Epoch 9 Batch 1000 Loss 3.3050 Accuracy 0.3915\n",
      "Epoch 9 Batch 1050 Loss 3.3044 Accuracy 0.3916\n",
      "Epoch 9 Batch 1100 Loss 3.3044 Accuracy 0.3916\n",
      "Epoch 9 Batch 1150 Loss 3.3046 Accuracy 0.3916\n",
      "Epoch 9 Batch 1200 Loss 3.3039 Accuracy 0.3916\n",
      "Epoch 9 Batch 1250 Loss 3.3040 Accuracy 0.3917\n",
      "Epoch 9 Batch 1300 Loss 3.3049 Accuracy 0.3916\n",
      "Epoch 9 Batch 1350 Loss 3.3047 Accuracy 0.3915\n",
      "Epoch 9 Batch 1400 Loss 3.3045 Accuracy 0.3914\n",
      "Epoch 9 Batch 1450 Loss 3.3039 Accuracy 0.3915\n",
      "Epoch 9 Batch 1500 Loss 3.3031 Accuracy 0.3917\n",
      "Epoch 9 Batch 1550 Loss 3.3036 Accuracy 0.3916\n",
      "Epoch 9 Batch 1600 Loss 3.3033 Accuracy 0.3916\n",
      "Epoch 9 Batch 1650 Loss 3.3029 Accuracy 0.3917\n",
      "Epoch 9 Batch 1700 Loss 3.3029 Accuracy 0.3917\n",
      "Epoch 9 Batch 1750 Loss 3.3029 Accuracy 0.3917\n",
      "Epoch 9 Batch 1800 Loss 3.3027 Accuracy 0.3917\n",
      "Epoch 9 Batch 1850 Loss 3.3022 Accuracy 0.3918\n",
      "Epoch 9 Batch 1900 Loss 3.3019 Accuracy 0.3918\n",
      "Epoch 9 Batch 1950 Loss 3.3019 Accuracy 0.3918\n",
      "Epoch 9 Batch 2000 Loss 3.3011 Accuracy 0.3919\n",
      "Epoch 9 Batch 2050 Loss 3.3011 Accuracy 0.3919\n",
      "Epoch 9 Batch 2100 Loss 3.3005 Accuracy 0.3920\n",
      "Epoch 9 Batch 2150 Loss 3.3003 Accuracy 0.3919\n",
      "Epoch 9 Batch 2200 Loss 3.2999 Accuracy 0.3920\n",
      "Epoch 9 Batch 2250 Loss 3.2998 Accuracy 0.3920\n",
      "Epoch 9 Batch 2300 Loss 3.2995 Accuracy 0.3921\n",
      "Epoch 9 Batch 2350 Loss 3.2993 Accuracy 0.3921\n",
      "Epoch 9 Batch 2400 Loss 3.2992 Accuracy 0.3920\n",
      "Epoch 9 Batch 2450 Loss 3.2986 Accuracy 0.3921\n",
      "Epoch 9 Batch 2500 Loss 3.2981 Accuracy 0.3922\n",
      "Epoch 9 Batch 2550 Loss 3.2979 Accuracy 0.3922\n",
      "Epoch 9 Batch 2600 Loss 3.2979 Accuracy 0.3922\n",
      "Epoch 9 Loss 3.2980 Accuracy 0.3922\n",
      "Time taken for 1 epoch: 456.64456272125244 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 3.3364 Accuracy 0.3952\n",
      "Epoch 10 Batch 50 Loss 3.2754 Accuracy 0.3916\n",
      "Epoch 10 Batch 100 Loss 3.2609 Accuracy 0.3943\n",
      "Epoch 10 Batch 150 Loss 3.2487 Accuracy 0.3960\n",
      "Epoch 10 Batch 200 Loss 3.2471 Accuracy 0.3963\n",
      "Epoch 10 Batch 250 Loss 3.2496 Accuracy 0.3960\n",
      "Epoch 10 Batch 300 Loss 3.2485 Accuracy 0.3963\n",
      "Epoch 10 Batch 350 Loss 3.2513 Accuracy 0.3961\n",
      "Epoch 10 Batch 400 Loss 3.2523 Accuracy 0.3960\n",
      "Epoch 10 Batch 450 Loss 3.2540 Accuracy 0.3960\n",
      "Epoch 10 Batch 500 Loss 3.2541 Accuracy 0.3960\n",
      "Epoch 10 Batch 550 Loss 3.2539 Accuracy 0.3963\n",
      "Epoch 10 Batch 600 Loss 3.2525 Accuracy 0.3966\n",
      "Epoch 10 Batch 650 Loss 3.2533 Accuracy 0.3964\n",
      "Epoch 10 Batch 700 Loss 3.2541 Accuracy 0.3964\n",
      "Epoch 10 Batch 750 Loss 3.2541 Accuracy 0.3963\n",
      "Epoch 10 Batch 800 Loss 3.2543 Accuracy 0.3964\n",
      "Epoch 10 Batch 850 Loss 3.2538 Accuracy 0.3963\n",
      "Epoch 10 Batch 900 Loss 3.2548 Accuracy 0.3962\n",
      "Epoch 10 Batch 950 Loss 3.2538 Accuracy 0.3964\n",
      "Epoch 10 Batch 1000 Loss 3.2524 Accuracy 0.3967\n",
      "Epoch 10 Batch 1050 Loss 3.2520 Accuracy 0.3968\n",
      "Epoch 10 Batch 1100 Loss 3.2504 Accuracy 0.3969\n",
      "Epoch 10 Batch 1150 Loss 3.2502 Accuracy 0.3970\n",
      "Epoch 10 Batch 1200 Loss 3.2510 Accuracy 0.3969\n",
      "Epoch 10 Batch 1250 Loss 3.2504 Accuracy 0.3970\n",
      "Epoch 10 Batch 1300 Loss 3.2492 Accuracy 0.3971\n",
      "Epoch 10 Batch 1350 Loss 3.2484 Accuracy 0.3972\n",
      "Epoch 10 Batch 1400 Loss 3.2479 Accuracy 0.3973\n",
      "Epoch 10 Batch 1450 Loss 3.2475 Accuracy 0.3973\n",
      "Epoch 10 Batch 1500 Loss 3.2472 Accuracy 0.3973\n",
      "Epoch 10 Batch 1550 Loss 3.2474 Accuracy 0.3972\n",
      "Epoch 10 Batch 1600 Loss 3.2472 Accuracy 0.3972\n",
      "Epoch 10 Batch 1650 Loss 3.2467 Accuracy 0.3973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 1700 Loss 3.2470 Accuracy 0.3973\n",
      "Epoch 10 Batch 1750 Loss 3.2465 Accuracy 0.3974\n",
      "Epoch 10 Batch 1800 Loss 3.2457 Accuracy 0.3975\n",
      "Epoch 10 Batch 1850 Loss 3.2456 Accuracy 0.3975\n",
      "Epoch 10 Batch 1900 Loss 3.2448 Accuracy 0.3976\n",
      "Epoch 10 Batch 1950 Loss 3.2449 Accuracy 0.3977\n",
      "Epoch 10 Batch 2000 Loss 3.2447 Accuracy 0.3977\n",
      "Epoch 10 Batch 2050 Loss 3.2443 Accuracy 0.3976\n",
      "Epoch 10 Batch 2100 Loss 3.2440 Accuracy 0.3977\n",
      "Epoch 10 Batch 2150 Loss 3.2439 Accuracy 0.3976\n",
      "Epoch 10 Batch 2200 Loss 3.2440 Accuracy 0.3976\n",
      "Epoch 10 Batch 2250 Loss 3.2433 Accuracy 0.3977\n",
      "Epoch 10 Batch 2300 Loss 3.2429 Accuracy 0.3977\n",
      "Epoch 10 Batch 2350 Loss 3.2431 Accuracy 0.3976\n",
      "Epoch 10 Batch 2400 Loss 3.2431 Accuracy 0.3977\n",
      "Epoch 10 Batch 2450 Loss 3.2424 Accuracy 0.3977\n",
      "Epoch 10 Batch 2500 Loss 3.2422 Accuracy 0.3977\n",
      "Epoch 10 Batch 2550 Loss 3.2426 Accuracy 0.3976\n",
      "Epoch 10 Batch 2600 Loss 3.2425 Accuracy 0.3977\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train_full/ckpt-2\n",
      "Epoch 10 Loss 3.2422 Accuracy 0.3977\n",
      "Time taken for 1 epoch: 458.0003275871277 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "          print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "    history.append(train_loss.result())\n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.1811 Accuracy 0.4075\n",
      "Epoch 1 Batch 50 Loss 3.1700 Accuracy 0.4044\n",
      "Epoch 1 Batch 100 Loss 3.1891 Accuracy 0.4024\n",
      "Epoch 1 Batch 150 Loss 3.1902 Accuracy 0.4020\n",
      "Epoch 1 Batch 200 Loss 3.2003 Accuracy 0.4002\n",
      "Epoch 1 Batch 250 Loss 3.2005 Accuracy 0.3999\n",
      "Epoch 1 Batch 300 Loss 3.2023 Accuracy 0.4000\n",
      "Epoch 1 Batch 350 Loss 3.2025 Accuracy 0.4004\n",
      "Epoch 1 Batch 400 Loss 3.1986 Accuracy 0.4011\n",
      "Epoch 1 Batch 450 Loss 3.2013 Accuracy 0.4008\n",
      "Epoch 1 Batch 500 Loss 3.2007 Accuracy 0.4009\n",
      "Epoch 1 Batch 550 Loss 3.2010 Accuracy 0.4010\n",
      "Epoch 1 Batch 600 Loss 3.2008 Accuracy 0.4011\n",
      "Epoch 1 Batch 650 Loss 3.2006 Accuracy 0.4012\n",
      "Epoch 1 Batch 700 Loss 3.1993 Accuracy 0.4014\n",
      "Epoch 1 Batch 750 Loss 3.1986 Accuracy 0.4012\n",
      "Epoch 1 Batch 800 Loss 3.1979 Accuracy 0.4014\n",
      "Epoch 1 Batch 850 Loss 3.1978 Accuracy 0.4016\n",
      "Epoch 1 Batch 900 Loss 3.1987 Accuracy 0.4015\n",
      "Epoch 1 Batch 950 Loss 3.1992 Accuracy 0.4014\n",
      "Epoch 1 Batch 1000 Loss 3.1975 Accuracy 0.4016\n",
      "Epoch 1 Batch 1050 Loss 3.1968 Accuracy 0.4018\n",
      "Epoch 1 Batch 1100 Loss 3.1973 Accuracy 0.4017\n",
      "Epoch 1 Batch 1150 Loss 3.1970 Accuracy 0.4017\n",
      "Epoch 1 Batch 1200 Loss 3.1954 Accuracy 0.4020\n",
      "Epoch 1 Batch 1250 Loss 3.1954 Accuracy 0.4019\n",
      "Epoch 1 Batch 1300 Loss 3.1957 Accuracy 0.4019\n",
      "Epoch 1 Batch 1350 Loss 3.1951 Accuracy 0.4019\n",
      "Epoch 1 Batch 1400 Loss 3.1949 Accuracy 0.4021\n",
      "Epoch 1 Batch 1450 Loss 3.1950 Accuracy 0.4021\n",
      "Epoch 1 Batch 1500 Loss 3.1949 Accuracy 0.4022\n",
      "Epoch 1 Batch 1550 Loss 3.1946 Accuracy 0.4022\n",
      "Epoch 1 Batch 1600 Loss 3.1943 Accuracy 0.4023\n",
      "Epoch 1 Batch 1650 Loss 3.1944 Accuracy 0.4023\n",
      "Epoch 1 Batch 1700 Loss 3.1943 Accuracy 0.4023\n",
      "Epoch 1 Batch 1750 Loss 3.1934 Accuracy 0.4024\n",
      "Epoch 1 Batch 1800 Loss 3.1937 Accuracy 0.4024\n",
      "Epoch 1 Batch 1850 Loss 3.1936 Accuracy 0.4023\n",
      "Epoch 1 Batch 1900 Loss 3.1935 Accuracy 0.4023\n",
      "Epoch 1 Batch 1950 Loss 3.1932 Accuracy 0.4023\n",
      "Epoch 1 Batch 2000 Loss 3.1929 Accuracy 0.4024\n",
      "Epoch 1 Batch 2050 Loss 3.1926 Accuracy 0.4024\n",
      "Epoch 1 Batch 2100 Loss 3.1924 Accuracy 0.4023\n",
      "Epoch 1 Batch 2150 Loss 3.1919 Accuracy 0.4024\n",
      "Epoch 1 Batch 2200 Loss 3.1921 Accuracy 0.4023\n",
      "Epoch 1 Batch 2250 Loss 3.1919 Accuracy 0.4023\n",
      "Epoch 1 Batch 2300 Loss 3.1921 Accuracy 0.4023\n",
      "Epoch 1 Batch 2350 Loss 3.1916 Accuracy 0.4024\n",
      "Epoch 1 Batch 2400 Loss 3.1912 Accuracy 0.4024\n",
      "Epoch 1 Batch 2450 Loss 3.1910 Accuracy 0.4024\n",
      "Epoch 1 Batch 2500 Loss 3.1916 Accuracy 0.4023\n",
      "Epoch 1 Batch 2550 Loss 3.1914 Accuracy 0.4023\n",
      "Epoch 1 Batch 2600 Loss 3.1910 Accuracy 0.4024\n",
      "Epoch 1 Loss 3.1911 Accuracy 0.4023\n",
      "Time taken for 1 epoch: 459.5947940349579 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.1786 Accuracy 0.4013\n",
      "Epoch 2 Batch 50 Loss 3.1440 Accuracy 0.4059\n",
      "Epoch 2 Batch 100 Loss 3.1485 Accuracy 0.4047\n",
      "Epoch 2 Batch 150 Loss 3.1433 Accuracy 0.4060\n",
      "Epoch 2 Batch 200 Loss 3.1474 Accuracy 0.4058\n",
      "Epoch 2 Batch 250 Loss 3.1506 Accuracy 0.4058\n",
      "Epoch 2 Batch 300 Loss 3.1533 Accuracy 0.4058\n",
      "Epoch 2 Batch 350 Loss 3.1567 Accuracy 0.4051\n",
      "Epoch 2 Batch 400 Loss 3.1570 Accuracy 0.4054\n",
      "Epoch 2 Batch 450 Loss 3.1573 Accuracy 0.4057\n",
      "Epoch 2 Batch 500 Loss 3.1560 Accuracy 0.4055\n",
      "Epoch 2 Batch 550 Loss 3.1550 Accuracy 0.4054\n",
      "Epoch 2 Batch 600 Loss 3.1549 Accuracy 0.4054\n",
      "Epoch 2 Batch 650 Loss 3.1556 Accuracy 0.4053\n",
      "Epoch 2 Batch 700 Loss 3.1565 Accuracy 0.4052\n",
      "Epoch 2 Batch 750 Loss 3.1553 Accuracy 0.4053\n",
      "Epoch 2 Batch 800 Loss 3.1532 Accuracy 0.4055\n",
      "Epoch 2 Batch 850 Loss 3.1526 Accuracy 0.4056\n",
      "Epoch 2 Batch 900 Loss 3.1507 Accuracy 0.4059\n",
      "Epoch 2 Batch 950 Loss 3.1506 Accuracy 0.4059\n",
      "Epoch 2 Batch 1000 Loss 3.1511 Accuracy 0.4059\n",
      "Epoch 2 Batch 1050 Loss 3.1512 Accuracy 0.4059\n",
      "Epoch 2 Batch 1100 Loss 3.1508 Accuracy 0.4059\n",
      "Epoch 2 Batch 1150 Loss 3.1498 Accuracy 0.4060\n",
      "Epoch 2 Batch 1200 Loss 3.1495 Accuracy 0.4060\n",
      "Epoch 2 Batch 1250 Loss 3.1493 Accuracy 0.4060\n",
      "Epoch 2 Batch 1300 Loss 3.1488 Accuracy 0.4062\n",
      "Epoch 2 Batch 1350 Loss 3.1489 Accuracy 0.4062\n",
      "Epoch 2 Batch 1400 Loss 3.1490 Accuracy 0.4062\n",
      "Epoch 2 Batch 1450 Loss 3.1487 Accuracy 0.4062\n",
      "Epoch 2 Batch 1500 Loss 3.1481 Accuracy 0.4063\n",
      "Epoch 2 Batch 1550 Loss 3.1484 Accuracy 0.4064\n",
      "Epoch 2 Batch 1600 Loss 3.1479 Accuracy 0.4064\n",
      "Epoch 2 Batch 1650 Loss 3.1476 Accuracy 0.4064\n",
      "Epoch 2 Batch 1700 Loss 3.1476 Accuracy 0.4063\n",
      "Epoch 2 Batch 1750 Loss 3.1479 Accuracy 0.4063\n",
      "Epoch 2 Batch 1800 Loss 3.1476 Accuracy 0.4063\n",
      "Epoch 2 Batch 1850 Loss 3.1475 Accuracy 0.4063\n",
      "Epoch 2 Batch 1900 Loss 3.1479 Accuracy 0.4063\n",
      "Epoch 2 Batch 1950 Loss 3.1478 Accuracy 0.4063\n",
      "Epoch 2 Batch 2000 Loss 3.1481 Accuracy 0.4063\n",
      "Epoch 2 Batch 2050 Loss 3.1482 Accuracy 0.4063\n",
      "Epoch 2 Batch 2100 Loss 3.1480 Accuracy 0.4063\n",
      "Epoch 2 Batch 2150 Loss 3.1480 Accuracy 0.4063\n",
      "Epoch 2 Batch 2200 Loss 3.1478 Accuracy 0.4063\n",
      "Epoch 2 Batch 2250 Loss 3.1469 Accuracy 0.4064\n",
      "Epoch 2 Batch 2300 Loss 3.1468 Accuracy 0.4064\n",
      "Epoch 2 Batch 2350 Loss 3.1467 Accuracy 0.4065\n",
      "Epoch 2 Batch 2400 Loss 3.1462 Accuracy 0.4066\n",
      "Epoch 2 Batch 2450 Loss 3.1463 Accuracy 0.4065\n",
      "Epoch 2 Batch 2500 Loss 3.1458 Accuracy 0.4066\n",
      "Epoch 2 Batch 2550 Loss 3.1456 Accuracy 0.4066\n",
      "Epoch 2 Batch 2600 Loss 3.1459 Accuracy 0.4065\n",
      "Epoch 2 Loss 3.1461 Accuracy 0.4065\n",
      "Time taken for 1 epoch: 456.80522108078003 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.1510 Accuracy 0.3882\n",
      "Epoch 3 Batch 50 Loss 3.1205 Accuracy 0.4074\n",
      "Epoch 3 Batch 100 Loss 3.1113 Accuracy 0.4097\n",
      "Epoch 3 Batch 150 Loss 3.1073 Accuracy 0.4097\n",
      "Epoch 3 Batch 200 Loss 3.1068 Accuracy 0.4096\n",
      "Epoch 3 Batch 250 Loss 3.1075 Accuracy 0.4095\n",
      "Epoch 3 Batch 300 Loss 3.1087 Accuracy 0.4093\n",
      "Epoch 3 Batch 350 Loss 3.1063 Accuracy 0.4098\n",
      "Epoch 3 Batch 400 Loss 3.1082 Accuracy 0.4097\n",
      "Epoch 3 Batch 450 Loss 3.1069 Accuracy 0.4099\n",
      "Epoch 3 Batch 500 Loss 3.1075 Accuracy 0.4099\n",
      "Epoch 3 Batch 550 Loss 3.1102 Accuracy 0.4095\n",
      "Epoch 3 Batch 600 Loss 3.1100 Accuracy 0.4097\n",
      "Epoch 3 Batch 650 Loss 3.1100 Accuracy 0.4097\n",
      "Epoch 3 Batch 700 Loss 3.1081 Accuracy 0.4099\n",
      "Epoch 3 Batch 750 Loss 3.1083 Accuracy 0.4099\n",
      "Epoch 3 Batch 800 Loss 3.1083 Accuracy 0.4099\n",
      "Epoch 3 Batch 850 Loss 3.1078 Accuracy 0.4102\n",
      "Epoch 3 Batch 900 Loss 3.1078 Accuracy 0.4102\n",
      "Epoch 3 Batch 950 Loss 3.1069 Accuracy 0.4104\n",
      "Epoch 3 Batch 1000 Loss 3.1070 Accuracy 0.4104\n",
      "Epoch 3 Batch 1050 Loss 3.1067 Accuracy 0.4105\n",
      "Epoch 3 Batch 1100 Loss 3.1065 Accuracy 0.4104\n",
      "Epoch 3 Batch 1150 Loss 3.1067 Accuracy 0.4103\n",
      "Epoch 3 Batch 1200 Loss 3.1075 Accuracy 0.4101\n",
      "Epoch 3 Batch 1250 Loss 3.1066 Accuracy 0.4103\n",
      "Epoch 3 Batch 1300 Loss 3.1065 Accuracy 0.4103\n",
      "Epoch 3 Batch 1350 Loss 3.1068 Accuracy 0.4103\n",
      "Epoch 3 Batch 1400 Loss 3.1074 Accuracy 0.4103\n",
      "Epoch 3 Batch 1450 Loss 3.1074 Accuracy 0.4103\n",
      "Epoch 3 Batch 1500 Loss 3.1074 Accuracy 0.4103\n",
      "Epoch 3 Batch 1550 Loss 3.1072 Accuracy 0.4104\n",
      "Epoch 3 Batch 1600 Loss 3.1069 Accuracy 0.4105\n",
      "Epoch 3 Batch 1650 Loss 3.1066 Accuracy 0.4105\n",
      "Epoch 3 Batch 1700 Loss 3.1070 Accuracy 0.4105\n",
      "Epoch 3 Batch 1750 Loss 3.1062 Accuracy 0.4106\n",
      "Epoch 3 Batch 1800 Loss 3.1059 Accuracy 0.4107\n",
      "Epoch 3 Batch 1850 Loss 3.1063 Accuracy 0.4106\n",
      "Epoch 3 Batch 1900 Loss 3.1068 Accuracy 0.4105\n",
      "Epoch 3 Batch 1950 Loss 3.1064 Accuracy 0.4105\n",
      "Epoch 3 Batch 2000 Loss 3.1061 Accuracy 0.4106\n",
      "Epoch 3 Batch 2050 Loss 3.1056 Accuracy 0.4107\n",
      "Epoch 3 Batch 2100 Loss 3.1051 Accuracy 0.4107\n",
      "Epoch 3 Batch 2150 Loss 3.1054 Accuracy 0.4106\n",
      "Epoch 3 Batch 2200 Loss 3.1056 Accuracy 0.4105\n",
      "Epoch 3 Batch 2250 Loss 3.1059 Accuracy 0.4105\n",
      "Epoch 3 Batch 2300 Loss 3.1057 Accuracy 0.4105\n",
      "Epoch 3 Batch 2350 Loss 3.1053 Accuracy 0.4105\n",
      "Epoch 3 Batch 2400 Loss 3.1049 Accuracy 0.4105\n",
      "Epoch 3 Batch 2450 Loss 3.1052 Accuracy 0.4105\n",
      "Epoch 3 Batch 2500 Loss 3.1049 Accuracy 0.4105\n",
      "Epoch 3 Batch 2550 Loss 3.1051 Accuracy 0.4104\n",
      "Epoch 3 Batch 2600 Loss 3.1051 Accuracy 0.4105\n",
      "Epoch 3 Loss 3.1054 Accuracy 0.4104\n",
      "Time taken for 1 epoch: 452.8237681388855 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 3.0555 Accuracy 0.4295\n",
      "Epoch 4 Batch 50 Loss 3.0581 Accuracy 0.4161\n",
      "Epoch 4 Batch 100 Loss 3.0583 Accuracy 0.4147\n",
      "Epoch 4 Batch 150 Loss 3.0636 Accuracy 0.4135\n",
      "Epoch 4 Batch 200 Loss 3.0648 Accuracy 0.4131\n",
      "Epoch 4 Batch 250 Loss 3.0673 Accuracy 0.4132\n",
      "Epoch 4 Batch 300 Loss 3.0686 Accuracy 0.4132\n",
      "Epoch 4 Batch 350 Loss 3.0690 Accuracy 0.4130\n",
      "Epoch 4 Batch 400 Loss 3.0661 Accuracy 0.4134\n",
      "Epoch 4 Batch 450 Loss 3.0671 Accuracy 0.4132\n",
      "Epoch 4 Batch 500 Loss 3.0680 Accuracy 0.4129\n",
      "Epoch 4 Batch 550 Loss 3.0693 Accuracy 0.4129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 600 Loss 3.0701 Accuracy 0.4129\n",
      "Epoch 4 Batch 650 Loss 3.0714 Accuracy 0.4128\n",
      "Epoch 4 Batch 700 Loss 3.0712 Accuracy 0.4127\n",
      "Epoch 4 Batch 750 Loss 3.0724 Accuracy 0.4124\n",
      "Epoch 4 Batch 800 Loss 3.0726 Accuracy 0.4126\n",
      "Epoch 4 Batch 850 Loss 3.0715 Accuracy 0.4128\n",
      "Epoch 4 Batch 900 Loss 3.0716 Accuracy 0.4128\n",
      "Epoch 4 Batch 950 Loss 3.0712 Accuracy 0.4130\n",
      "Epoch 4 Batch 1000 Loss 3.0711 Accuracy 0.4131\n",
      "Epoch 4 Batch 1050 Loss 3.0711 Accuracy 0.4131\n",
      "Epoch 4 Batch 1100 Loss 3.0704 Accuracy 0.4131\n",
      "Epoch 4 Batch 1150 Loss 3.0702 Accuracy 0.4132\n",
      "Epoch 4 Batch 1200 Loss 3.0701 Accuracy 0.4132\n",
      "Epoch 4 Batch 1250 Loss 3.0701 Accuracy 0.4133\n",
      "Epoch 4 Batch 1300 Loss 3.0691 Accuracy 0.4134\n",
      "Epoch 4 Batch 1350 Loss 3.0700 Accuracy 0.4132\n",
      "Epoch 4 Batch 1400 Loss 3.0698 Accuracy 0.4133\n",
      "Epoch 4 Batch 1450 Loss 3.0701 Accuracy 0.4133\n",
      "Epoch 4 Batch 1500 Loss 3.0692 Accuracy 0.4134\n",
      "Epoch 4 Batch 1550 Loss 3.0686 Accuracy 0.4135\n",
      "Epoch 4 Batch 1600 Loss 3.0686 Accuracy 0.4135\n",
      "Epoch 4 Batch 1650 Loss 3.0687 Accuracy 0.4136\n",
      "Epoch 4 Batch 1700 Loss 3.0684 Accuracy 0.4136\n",
      "Epoch 4 Batch 1750 Loss 3.0684 Accuracy 0.4136\n",
      "Epoch 4 Batch 1800 Loss 3.0684 Accuracy 0.4137\n",
      "Epoch 4 Batch 1850 Loss 3.0680 Accuracy 0.4136\n",
      "Epoch 4 Batch 1900 Loss 3.0678 Accuracy 0.4136\n",
      "Epoch 4 Batch 1950 Loss 3.0678 Accuracy 0.4136\n",
      "Epoch 4 Batch 2000 Loss 3.0675 Accuracy 0.4136\n",
      "Epoch 4 Batch 2050 Loss 3.0673 Accuracy 0.4137\n",
      "Epoch 4 Batch 2100 Loss 3.0679 Accuracy 0.4136\n",
      "Epoch 4 Batch 2150 Loss 3.0679 Accuracy 0.4136\n",
      "Epoch 4 Batch 2200 Loss 3.0680 Accuracy 0.4136\n",
      "Epoch 4 Batch 2250 Loss 3.0683 Accuracy 0.4135\n",
      "Epoch 4 Batch 2300 Loss 3.0680 Accuracy 0.4136\n",
      "Epoch 4 Batch 2350 Loss 3.0687 Accuracy 0.4135\n",
      "Epoch 4 Batch 2400 Loss 3.0682 Accuracy 0.4136\n",
      "Epoch 4 Batch 2450 Loss 3.0684 Accuracy 0.4135\n",
      "Epoch 4 Batch 2500 Loss 3.0683 Accuracy 0.4135\n",
      "Epoch 4 Batch 2550 Loss 3.0684 Accuracy 0.4136\n",
      "Epoch 4 Batch 2600 Loss 3.0683 Accuracy 0.4136\n",
      "Epoch 4 Loss 3.0682 Accuracy 0.4137\n",
      "Time taken for 1 epoch: 452.10335063934326 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 3.0474 Accuracy 0.4457\n",
      "Epoch 5 Batch 50 Loss 3.0294 Accuracy 0.4188\n",
      "Epoch 5 Batch 100 Loss 3.0233 Accuracy 0.4187\n",
      "Epoch 5 Batch 150 Loss 3.0292 Accuracy 0.4165\n",
      "Epoch 5 Batch 200 Loss 3.0239 Accuracy 0.4168\n",
      "Epoch 5 Batch 250 Loss 3.0292 Accuracy 0.4161\n",
      "Epoch 5 Batch 300 Loss 3.0293 Accuracy 0.4164\n",
      "Epoch 5 Batch 350 Loss 3.0320 Accuracy 0.4161\n",
      "Epoch 5 Batch 400 Loss 3.0340 Accuracy 0.4160\n",
      "Epoch 5 Batch 450 Loss 3.0366 Accuracy 0.4155\n",
      "Epoch 5 Batch 500 Loss 3.0396 Accuracy 0.4152\n",
      "Epoch 5 Batch 550 Loss 3.0408 Accuracy 0.4152\n",
      "Epoch 5 Batch 600 Loss 3.0413 Accuracy 0.4154\n",
      "Epoch 5 Batch 650 Loss 3.0410 Accuracy 0.4153\n",
      "Epoch 5 Batch 700 Loss 3.0420 Accuracy 0.4151\n",
      "Epoch 5 Batch 750 Loss 3.0414 Accuracy 0.4153\n",
      "Epoch 5 Batch 800 Loss 3.0412 Accuracy 0.4153\n",
      "Epoch 5 Batch 850 Loss 3.0415 Accuracy 0.4153\n",
      "Epoch 5 Batch 900 Loss 3.0391 Accuracy 0.4156\n",
      "Epoch 5 Batch 950 Loss 3.0390 Accuracy 0.4157\n",
      "Epoch 5 Batch 1000 Loss 3.0384 Accuracy 0.4158\n",
      "Epoch 5 Batch 1050 Loss 3.0376 Accuracy 0.4159\n",
      "Epoch 5 Batch 1100 Loss 3.0377 Accuracy 0.4159\n",
      "Epoch 5 Batch 1150 Loss 3.0374 Accuracy 0.4159\n",
      "Epoch 5 Batch 1200 Loss 3.0367 Accuracy 0.4161\n",
      "Epoch 5 Batch 1250 Loss 3.0364 Accuracy 0.4161\n",
      "Epoch 5 Batch 1300 Loss 3.0366 Accuracy 0.4161\n",
      "Epoch 5 Batch 1350 Loss 3.0368 Accuracy 0.4162\n",
      "Epoch 5 Batch 1400 Loss 3.0365 Accuracy 0.4162\n",
      "Epoch 5 Batch 1450 Loss 3.0359 Accuracy 0.4163\n",
      "Epoch 5 Batch 1500 Loss 3.0360 Accuracy 0.4163\n",
      "Epoch 5 Batch 1550 Loss 3.0359 Accuracy 0.4164\n",
      "Epoch 5 Batch 1600 Loss 3.0357 Accuracy 0.4164\n",
      "Epoch 5 Batch 1650 Loss 3.0356 Accuracy 0.4164\n",
      "Epoch 5 Batch 1700 Loss 3.0364 Accuracy 0.4163\n",
      "Epoch 5 Batch 1750 Loss 3.0365 Accuracy 0.4164\n",
      "Epoch 5 Batch 1800 Loss 3.0369 Accuracy 0.4164\n",
      "Epoch 5 Batch 1850 Loss 3.0371 Accuracy 0.4164\n",
      "Epoch 5 Batch 1900 Loss 3.0363 Accuracy 0.4164\n",
      "Epoch 5 Batch 1950 Loss 3.0355 Accuracy 0.4166\n",
      "Epoch 5 Batch 2000 Loss 3.0356 Accuracy 0.4166\n",
      "Epoch 5 Batch 2050 Loss 3.0353 Accuracy 0.4166\n",
      "Epoch 5 Batch 2100 Loss 3.0349 Accuracy 0.4167\n",
      "Epoch 5 Batch 2150 Loss 3.0347 Accuracy 0.4167\n",
      "Epoch 5 Batch 2200 Loss 3.0347 Accuracy 0.4167\n",
      "Epoch 5 Batch 2250 Loss 3.0356 Accuracy 0.4165\n",
      "Epoch 5 Batch 2300 Loss 3.0359 Accuracy 0.4165\n",
      "Epoch 5 Batch 2350 Loss 3.0357 Accuracy 0.4165\n",
      "Epoch 5 Batch 2400 Loss 3.0354 Accuracy 0.4165\n",
      "Epoch 5 Batch 2450 Loss 3.0357 Accuracy 0.4165\n",
      "Epoch 5 Batch 2500 Loss 3.0357 Accuracy 0.4165\n",
      "Epoch 5 Batch 2550 Loss 3.0356 Accuracy 0.4165\n",
      "Epoch 5 Batch 2600 Loss 3.0360 Accuracy 0.4165\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train_full/ckpt-3\n",
      "Epoch 5 Loss 3.0361 Accuracy 0.4165\n",
      "Time taken for 1 epoch: 453.5449843406677 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.9392 Accuracy 0.4362\n",
      "Epoch 6 Batch 50 Loss 3.0035 Accuracy 0.4190\n",
      "Epoch 6 Batch 100 Loss 3.0008 Accuracy 0.4192\n",
      "Epoch 6 Batch 150 Loss 2.9994 Accuracy 0.4198\n",
      "Epoch 6 Batch 200 Loss 2.9944 Accuracy 0.4200\n",
      "Epoch 6 Batch 250 Loss 2.9966 Accuracy 0.4190\n",
      "Epoch 6 Batch 300 Loss 2.9958 Accuracy 0.4192\n",
      "Epoch 6 Batch 350 Loss 2.9966 Accuracy 0.4192\n",
      "Epoch 6 Batch 400 Loss 2.9986 Accuracy 0.4191\n",
      "Epoch 6 Batch 450 Loss 2.9990 Accuracy 0.4194\n",
      "Epoch 6 Batch 500 Loss 3.0004 Accuracy 0.4194\n",
      "Epoch 6 Batch 550 Loss 3.0010 Accuracy 0.4194\n",
      "Epoch 6 Batch 600 Loss 3.0016 Accuracy 0.4194\n",
      "Epoch 6 Batch 650 Loss 3.0026 Accuracy 0.4195\n",
      "Epoch 6 Batch 700 Loss 3.0040 Accuracy 0.4194\n",
      "Epoch 6 Batch 750 Loss 3.0036 Accuracy 0.4195\n",
      "Epoch 6 Batch 800 Loss 3.0041 Accuracy 0.4195\n",
      "Epoch 6 Batch 850 Loss 3.0050 Accuracy 0.4194\n",
      "Epoch 6 Batch 900 Loss 3.0054 Accuracy 0.4194\n",
      "Epoch 6 Batch 950 Loss 3.0050 Accuracy 0.4195\n",
      "Epoch 6 Batch 1000 Loss 3.0057 Accuracy 0.4193\n",
      "Epoch 6 Batch 1050 Loss 3.0062 Accuracy 0.4192\n",
      "Epoch 6 Batch 1100 Loss 3.0052 Accuracy 0.4193\n",
      "Epoch 6 Batch 1150 Loss 3.0047 Accuracy 0.4194\n",
      "Epoch 6 Batch 1200 Loss 3.0046 Accuracy 0.4195\n",
      "Epoch 6 Batch 1250 Loss 3.0039 Accuracy 0.4196\n",
      "Epoch 6 Batch 1300 Loss 3.0036 Accuracy 0.4197\n",
      "Epoch 6 Batch 1350 Loss 3.0040 Accuracy 0.4196\n",
      "Epoch 6 Batch 1400 Loss 3.0043 Accuracy 0.4197\n",
      "Epoch 6 Batch 1450 Loss 3.0043 Accuracy 0.4197\n",
      "Epoch 6 Batch 1500 Loss 3.0034 Accuracy 0.4199\n",
      "Epoch 6 Batch 1550 Loss 3.0028 Accuracy 0.4200\n",
      "Epoch 6 Batch 1600 Loss 3.0030 Accuracy 0.4200\n",
      "Epoch 6 Batch 1650 Loss 3.0024 Accuracy 0.4201\n",
      "Epoch 6 Batch 1700 Loss 3.0017 Accuracy 0.4202\n",
      "Epoch 6 Batch 1750 Loss 3.0014 Accuracy 0.4203\n",
      "Epoch 6 Batch 1800 Loss 3.0018 Accuracy 0.4202\n",
      "Epoch 6 Batch 1850 Loss 3.0019 Accuracy 0.4202\n",
      "Epoch 6 Batch 1900 Loss 3.0019 Accuracy 0.4202\n",
      "Epoch 6 Batch 1950 Loss 3.0025 Accuracy 0.4201\n",
      "Epoch 6 Batch 2000 Loss 3.0026 Accuracy 0.4201\n",
      "Epoch 6 Batch 2050 Loss 3.0026 Accuracy 0.4201\n",
      "Epoch 6 Batch 2100 Loss 3.0025 Accuracy 0.4201\n",
      "Epoch 6 Batch 2150 Loss 3.0029 Accuracy 0.4201\n",
      "Epoch 6 Batch 2200 Loss 3.0026 Accuracy 0.4200\n",
      "Epoch 6 Batch 2250 Loss 3.0029 Accuracy 0.4199\n",
      "Epoch 6 Batch 2300 Loss 3.0034 Accuracy 0.4199\n",
      "Epoch 6 Batch 2350 Loss 3.0034 Accuracy 0.4199\n",
      "Epoch 6 Batch 2400 Loss 3.0035 Accuracy 0.4199\n",
      "Epoch 6 Batch 2450 Loss 3.0033 Accuracy 0.4199\n",
      "Epoch 6 Batch 2500 Loss 3.0035 Accuracy 0.4199\n",
      "Epoch 6 Batch 2550 Loss 3.0032 Accuracy 0.4200\n",
      "Epoch 6 Batch 2600 Loss 3.0035 Accuracy 0.4200\n",
      "Epoch 6 Loss 3.0038 Accuracy 0.4199\n",
      "Time taken for 1 epoch: 454.9130961894989 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 3.0316 Accuracy 0.4202\n",
      "Epoch 7 Batch 50 Loss 2.9634 Accuracy 0.4245\n",
      "Epoch 7 Batch 100 Loss 2.9637 Accuracy 0.4245\n",
      "Epoch 7 Batch 150 Loss 2.9581 Accuracy 0.4247\n",
      "Epoch 7 Batch 200 Loss 2.9581 Accuracy 0.4246\n",
      "Epoch 7 Batch 250 Loss 2.9651 Accuracy 0.4232\n",
      "Epoch 7 Batch 300 Loss 2.9672 Accuracy 0.4225\n",
      "Epoch 7 Batch 350 Loss 2.9670 Accuracy 0.4227\n",
      "Epoch 7 Batch 400 Loss 2.9693 Accuracy 0.4227\n",
      "Epoch 7 Batch 450 Loss 2.9694 Accuracy 0.4229\n",
      "Epoch 7 Batch 500 Loss 2.9720 Accuracy 0.4224\n",
      "Epoch 7 Batch 550 Loss 2.9722 Accuracy 0.4223\n",
      "Epoch 7 Batch 600 Loss 2.9726 Accuracy 0.4224\n",
      "Epoch 7 Batch 650 Loss 2.9740 Accuracy 0.4221\n",
      "Epoch 7 Batch 700 Loss 2.9742 Accuracy 0.4223\n",
      "Epoch 7 Batch 750 Loss 2.9736 Accuracy 0.4223\n",
      "Epoch 7 Batch 800 Loss 2.9746 Accuracy 0.4222\n",
      "Epoch 7 Batch 850 Loss 2.9745 Accuracy 0.4221\n",
      "Epoch 7 Batch 900 Loss 2.9756 Accuracy 0.4220\n",
      "Epoch 7 Batch 950 Loss 2.9750 Accuracy 0.4222\n",
      "Epoch 7 Batch 1000 Loss 2.9736 Accuracy 0.4224\n",
      "Epoch 7 Batch 1050 Loss 2.9724 Accuracy 0.4225\n",
      "Epoch 7 Batch 1100 Loss 2.9718 Accuracy 0.4227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 1150 Loss 2.9723 Accuracy 0.4227\n",
      "Epoch 7 Batch 1200 Loss 2.9738 Accuracy 0.4224\n",
      "Epoch 7 Batch 1250 Loss 2.9738 Accuracy 0.4224\n",
      "Epoch 7 Batch 1300 Loss 2.9737 Accuracy 0.4225\n",
      "Epoch 7 Batch 1350 Loss 2.9736 Accuracy 0.4225\n",
      "Epoch 7 Batch 1400 Loss 2.9739 Accuracy 0.4225\n",
      "Epoch 7 Batch 1450 Loss 2.9743 Accuracy 0.4225\n",
      "Epoch 7 Batch 1500 Loss 2.9747 Accuracy 0.4225\n",
      "Epoch 7 Batch 1550 Loss 2.9753 Accuracy 0.4224\n",
      "Epoch 7 Batch 1600 Loss 2.9746 Accuracy 0.4225\n",
      "Epoch 7 Batch 1650 Loss 2.9745 Accuracy 0.4226\n",
      "Epoch 7 Batch 1700 Loss 2.9741 Accuracy 0.4227\n",
      "Epoch 7 Batch 1750 Loss 2.9738 Accuracy 0.4227\n",
      "Epoch 7 Batch 1800 Loss 2.9742 Accuracy 0.4227\n",
      "Epoch 7 Batch 1850 Loss 2.9745 Accuracy 0.4227\n",
      "Epoch 7 Batch 1900 Loss 2.9746 Accuracy 0.4227\n",
      "Epoch 7 Batch 1950 Loss 2.9742 Accuracy 0.4227\n",
      "Epoch 7 Batch 2000 Loss 2.9743 Accuracy 0.4227\n",
      "Epoch 7 Batch 2050 Loss 2.9740 Accuracy 0.4227\n",
      "Epoch 7 Batch 2100 Loss 2.9735 Accuracy 0.4227\n",
      "Epoch 7 Batch 2150 Loss 2.9738 Accuracy 0.4227\n",
      "Epoch 7 Batch 2200 Loss 2.9737 Accuracy 0.4227\n",
      "Epoch 7 Batch 2250 Loss 2.9738 Accuracy 0.4227\n",
      "Epoch 7 Batch 2300 Loss 2.9743 Accuracy 0.4226\n",
      "Epoch 7 Batch 2350 Loss 2.9744 Accuracy 0.4226\n",
      "Epoch 7 Batch 2400 Loss 2.9739 Accuracy 0.4226\n",
      "Epoch 7 Batch 2450 Loss 2.9742 Accuracy 0.4227\n",
      "Epoch 7 Batch 2500 Loss 2.9744 Accuracy 0.4226\n",
      "Epoch 7 Batch 2550 Loss 2.9747 Accuracy 0.4227\n",
      "Epoch 7 Batch 2600 Loss 2.9746 Accuracy 0.4227\n",
      "Epoch 7 Loss 2.9747 Accuracy 0.4226\n",
      "Time taken for 1 epoch: 453.39433789253235 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 3.0380 Accuracy 0.4101\n",
      "Epoch 8 Batch 50 Loss 2.9432 Accuracy 0.4262\n",
      "Epoch 8 Batch 100 Loss 2.9398 Accuracy 0.4267\n",
      "Epoch 8 Batch 150 Loss 2.9407 Accuracy 0.4258\n",
      "Epoch 8 Batch 200 Loss 2.9421 Accuracy 0.4253\n",
      "Epoch 8 Batch 250 Loss 2.9415 Accuracy 0.4250\n",
      "Epoch 8 Batch 300 Loss 2.9389 Accuracy 0.4255\n",
      "Epoch 8 Batch 350 Loss 2.9384 Accuracy 0.4256\n",
      "Epoch 8 Batch 400 Loss 2.9427 Accuracy 0.4253\n",
      "Epoch 8 Batch 450 Loss 2.9440 Accuracy 0.4251\n",
      "Epoch 8 Batch 500 Loss 2.9426 Accuracy 0.4251\n",
      "Epoch 8 Batch 550 Loss 2.9428 Accuracy 0.4253\n",
      "Epoch 8 Batch 600 Loss 2.9431 Accuracy 0.4254\n",
      "Epoch 8 Batch 650 Loss 2.9445 Accuracy 0.4253\n",
      "Epoch 8 Batch 700 Loss 2.9444 Accuracy 0.4255\n",
      "Epoch 8 Batch 750 Loss 2.9457 Accuracy 0.4254\n",
      "Epoch 8 Batch 800 Loss 2.9469 Accuracy 0.4252\n",
      "Epoch 8 Batch 850 Loss 2.9464 Accuracy 0.4253\n",
      "Epoch 8 Batch 900 Loss 2.9460 Accuracy 0.4254\n",
      "Epoch 8 Batch 950 Loss 2.9471 Accuracy 0.4251\n",
      "Epoch 8 Batch 1000 Loss 2.9463 Accuracy 0.4252\n",
      "Epoch 8 Batch 1050 Loss 2.9473 Accuracy 0.4251\n",
      "Epoch 8 Batch 1100 Loss 2.9479 Accuracy 0.4250\n",
      "Epoch 8 Batch 1150 Loss 2.9469 Accuracy 0.4252\n",
      "Epoch 8 Batch 1200 Loss 2.9470 Accuracy 0.4252\n",
      "Epoch 8 Batch 1250 Loss 2.9469 Accuracy 0.4252\n",
      "Epoch 8 Batch 1300 Loss 2.9471 Accuracy 0.4253\n",
      "Epoch 8 Batch 1350 Loss 2.9474 Accuracy 0.4253\n",
      "Epoch 8 Batch 1400 Loss 2.9468 Accuracy 0.4254\n",
      "Epoch 8 Batch 1450 Loss 2.9478 Accuracy 0.4252\n",
      "Epoch 8 Batch 1500 Loss 2.9472 Accuracy 0.4254\n",
      "Epoch 8 Batch 1550 Loss 2.9472 Accuracy 0.4254\n",
      "Epoch 8 Batch 1600 Loss 2.9466 Accuracy 0.4255\n",
      "Epoch 8 Batch 1650 Loss 2.9465 Accuracy 0.4255\n",
      "Epoch 8 Batch 1700 Loss 2.9463 Accuracy 0.4256\n",
      "Epoch 8 Batch 1750 Loss 2.9466 Accuracy 0.4255\n",
      "Epoch 8 Batch 1800 Loss 2.9466 Accuracy 0.4256\n",
      "Epoch 8 Batch 1850 Loss 2.9465 Accuracy 0.4256\n",
      "Epoch 8 Batch 1900 Loss 2.9470 Accuracy 0.4256\n",
      "Epoch 8 Batch 1950 Loss 2.9471 Accuracy 0.4255\n",
      "Epoch 8 Batch 2000 Loss 2.9467 Accuracy 0.4255\n",
      "Epoch 8 Batch 2050 Loss 2.9470 Accuracy 0.4255\n",
      "Epoch 8 Batch 2100 Loss 2.9468 Accuracy 0.4255\n",
      "Epoch 8 Batch 2150 Loss 2.9469 Accuracy 0.4254\n",
      "Epoch 8 Batch 2200 Loss 2.9470 Accuracy 0.4253\n",
      "Epoch 8 Batch 2250 Loss 2.9472 Accuracy 0.4253\n",
      "Epoch 8 Batch 2300 Loss 2.9476 Accuracy 0.4253\n",
      "Epoch 8 Batch 2350 Loss 2.9474 Accuracy 0.4253\n",
      "Epoch 8 Batch 2400 Loss 2.9473 Accuracy 0.4253\n",
      "Epoch 8 Batch 2450 Loss 2.9470 Accuracy 0.4253\n",
      "Epoch 8 Batch 2500 Loss 2.9474 Accuracy 0.4253\n",
      "Epoch 8 Batch 2550 Loss 2.9474 Accuracy 0.4254\n",
      "Epoch 8 Batch 2600 Loss 2.9472 Accuracy 0.4254\n",
      "Epoch 8 Loss 2.9475 Accuracy 0.4253\n",
      "Time taken for 1 epoch: 452.54753589630127 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.9635 Accuracy 0.4173\n",
      "Epoch 9 Batch 50 Loss 2.9031 Accuracy 0.4269\n",
      "Epoch 9 Batch 100 Loss 2.9011 Accuracy 0.4284\n",
      "Epoch 9 Batch 150 Loss 2.9135 Accuracy 0.4277\n",
      "Epoch 9 Batch 200 Loss 2.9075 Accuracy 0.4288\n",
      "Epoch 9 Batch 250 Loss 2.9100 Accuracy 0.4290\n",
      "Epoch 9 Batch 300 Loss 2.9122 Accuracy 0.4289\n",
      "Epoch 9 Batch 350 Loss 2.9145 Accuracy 0.4288\n",
      "Epoch 9 Batch 400 Loss 2.9163 Accuracy 0.4287\n",
      "Epoch 9 Batch 450 Loss 2.9181 Accuracy 0.4284\n",
      "Epoch 9 Batch 500 Loss 2.9198 Accuracy 0.4281\n",
      "Epoch 9 Batch 550 Loss 2.9203 Accuracy 0.4280\n",
      "Epoch 9 Batch 600 Loss 2.9200 Accuracy 0.4281\n",
      "Epoch 9 Batch 650 Loss 2.9186 Accuracy 0.4284\n",
      "Epoch 9 Batch 700 Loss 2.9198 Accuracy 0.4284\n",
      "Epoch 9 Batch 750 Loss 2.9212 Accuracy 0.4281\n",
      "Epoch 9 Batch 800 Loss 2.9208 Accuracy 0.4281\n",
      "Epoch 9 Batch 850 Loss 2.9209 Accuracy 0.4280\n",
      "Epoch 9 Batch 900 Loss 2.9192 Accuracy 0.4282\n",
      "Epoch 9 Batch 950 Loss 2.9191 Accuracy 0.4283\n",
      "Epoch 9 Batch 1000 Loss 2.9193 Accuracy 0.4283\n",
      "Epoch 9 Batch 1050 Loss 2.9187 Accuracy 0.4284\n",
      "Epoch 9 Batch 1100 Loss 2.9185 Accuracy 0.4284\n",
      "Epoch 9 Batch 1150 Loss 2.9181 Accuracy 0.4284\n",
      "Epoch 9 Batch 1200 Loss 2.9187 Accuracy 0.4284\n",
      "Epoch 9 Batch 1250 Loss 2.9186 Accuracy 0.4284\n",
      "Epoch 9 Batch 1300 Loss 2.9189 Accuracy 0.4283\n",
      "Epoch 9 Batch 1350 Loss 2.9188 Accuracy 0.4284\n",
      "Epoch 9 Batch 1400 Loss 2.9183 Accuracy 0.4285\n",
      "Epoch 9 Batch 1450 Loss 2.9181 Accuracy 0.4285\n",
      "Epoch 9 Batch 1500 Loss 2.9184 Accuracy 0.4285\n",
      "Epoch 9 Batch 1550 Loss 2.9184 Accuracy 0.4286\n",
      "Epoch 9 Batch 1600 Loss 2.9191 Accuracy 0.4285\n",
      "Epoch 9 Batch 1650 Loss 2.9191 Accuracy 0.4285\n",
      "Epoch 9 Batch 1700 Loss 2.9184 Accuracy 0.4286\n",
      "Epoch 9 Batch 1750 Loss 2.9181 Accuracy 0.4286\n",
      "Epoch 9 Batch 1800 Loss 2.9184 Accuracy 0.4286\n",
      "Epoch 9 Batch 1850 Loss 2.9181 Accuracy 0.4286\n",
      "Epoch 9 Batch 1900 Loss 2.9176 Accuracy 0.4286\n",
      "Epoch 9 Batch 1950 Loss 2.9176 Accuracy 0.4286\n",
      "Epoch 9 Batch 2000 Loss 2.9173 Accuracy 0.4287\n",
      "Epoch 9 Batch 2050 Loss 2.9179 Accuracy 0.4286\n",
      "Epoch 9 Batch 2100 Loss 2.9180 Accuracy 0.4286\n",
      "Epoch 9 Batch 2150 Loss 2.9176 Accuracy 0.4286\n",
      "Epoch 9 Batch 2200 Loss 2.9182 Accuracy 0.4285\n",
      "Epoch 9 Batch 2250 Loss 2.9189 Accuracy 0.4285\n",
      "Epoch 9 Batch 2300 Loss 2.9190 Accuracy 0.4284\n",
      "Epoch 9 Batch 2350 Loss 2.9190 Accuracy 0.4285\n",
      "Epoch 9 Batch 2400 Loss 2.9189 Accuracy 0.4285\n",
      "Epoch 9 Batch 2450 Loss 2.9191 Accuracy 0.4285\n",
      "Epoch 9 Batch 2500 Loss 2.9192 Accuracy 0.4285\n",
      "Epoch 9 Batch 2550 Loss 2.9191 Accuracy 0.4285\n",
      "Epoch 9 Batch 2600 Loss 2.9197 Accuracy 0.4284\n",
      "Epoch 9 Loss 2.9199 Accuracy 0.4284\n",
      "Time taken for 1 epoch: 452.8368122577667 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 2.9069 Accuracy 0.4241\n",
      "Epoch 10 Batch 50 Loss 2.8915 Accuracy 0.4314\n",
      "Epoch 10 Batch 100 Loss 2.8871 Accuracy 0.4309\n",
      "Epoch 10 Batch 150 Loss 2.8848 Accuracy 0.4312\n",
      "Epoch 10 Batch 200 Loss 2.8851 Accuracy 0.4316\n",
      "Epoch 10 Batch 250 Loss 2.8891 Accuracy 0.4308\n",
      "Epoch 10 Batch 300 Loss 2.8897 Accuracy 0.4310\n",
      "Epoch 10 Batch 350 Loss 2.8882 Accuracy 0.4314\n",
      "Epoch 10 Batch 400 Loss 2.8914 Accuracy 0.4312\n",
      "Epoch 10 Batch 450 Loss 2.8922 Accuracy 0.4311\n",
      "Epoch 10 Batch 500 Loss 2.8931 Accuracy 0.4308\n",
      "Epoch 10 Batch 550 Loss 2.8931 Accuracy 0.4306\n",
      "Epoch 10 Batch 600 Loss 2.8927 Accuracy 0.4309\n",
      "Epoch 10 Batch 650 Loss 2.8942 Accuracy 0.4308\n",
      "Epoch 10 Batch 700 Loss 2.8943 Accuracy 0.4309\n",
      "Epoch 10 Batch 750 Loss 2.8946 Accuracy 0.4307\n",
      "Epoch 10 Batch 800 Loss 2.8952 Accuracy 0.4306\n",
      "Epoch 10 Batch 850 Loss 2.8951 Accuracy 0.4307\n",
      "Epoch 10 Batch 900 Loss 2.8949 Accuracy 0.4307\n",
      "Epoch 10 Batch 950 Loss 2.8959 Accuracy 0.4306\n",
      "Epoch 10 Batch 1000 Loss 2.8958 Accuracy 0.4307\n",
      "Epoch 10 Batch 1050 Loss 2.8968 Accuracy 0.4306\n",
      "Epoch 10 Batch 1100 Loss 2.8965 Accuracy 0.4307\n",
      "Epoch 10 Batch 1150 Loss 2.8963 Accuracy 0.4307\n",
      "Epoch 10 Batch 1200 Loss 2.8947 Accuracy 0.4310\n",
      "Epoch 10 Batch 1250 Loss 2.8951 Accuracy 0.4309\n",
      "Epoch 10 Batch 1300 Loss 2.8951 Accuracy 0.4308\n",
      "Epoch 10 Batch 1350 Loss 2.8949 Accuracy 0.4308\n",
      "Epoch 10 Batch 1400 Loss 2.8944 Accuracy 0.4309\n",
      "Epoch 10 Batch 1450 Loss 2.8936 Accuracy 0.4311\n",
      "Epoch 10 Batch 1500 Loss 2.8927 Accuracy 0.4312\n",
      "Epoch 10 Batch 1550 Loss 2.8935 Accuracy 0.4312\n",
      "Epoch 10 Batch 1600 Loss 2.8936 Accuracy 0.4311\n",
      "Epoch 10 Batch 1650 Loss 2.8944 Accuracy 0.4311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 1700 Loss 2.8944 Accuracy 0.4311\n",
      "Epoch 10 Batch 1750 Loss 2.8945 Accuracy 0.4311\n",
      "Epoch 10 Batch 1800 Loss 2.8949 Accuracy 0.4310\n",
      "Epoch 10 Batch 1850 Loss 2.8957 Accuracy 0.4309\n",
      "Epoch 10 Batch 1900 Loss 2.8956 Accuracy 0.4310\n",
      "Epoch 10 Batch 1950 Loss 2.8960 Accuracy 0.4309\n",
      "Epoch 10 Batch 2000 Loss 2.8963 Accuracy 0.4309\n",
      "Epoch 10 Batch 2050 Loss 2.8962 Accuracy 0.4309\n",
      "Epoch 10 Batch 2100 Loss 2.8961 Accuracy 0.4310\n",
      "Epoch 10 Batch 2150 Loss 2.8956 Accuracy 0.4310\n",
      "Epoch 10 Batch 2200 Loss 2.8952 Accuracy 0.4311\n",
      "Epoch 10 Batch 2250 Loss 2.8948 Accuracy 0.4311\n",
      "Epoch 10 Batch 2300 Loss 2.8951 Accuracy 0.4311\n",
      "Epoch 10 Batch 2350 Loss 2.8952 Accuracy 0.4310\n",
      "Epoch 10 Batch 2400 Loss 2.8946 Accuracy 0.4311\n",
      "Epoch 10 Batch 2450 Loss 2.8951 Accuracy 0.4310\n",
      "Epoch 10 Batch 2500 Loss 2.8952 Accuracy 0.4311\n",
      "Epoch 10 Batch 2550 Loss 2.8957 Accuracy 0.4310\n",
      "Epoch 10 Batch 2600 Loss 2.8962 Accuracy 0.4309\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train_full/ckpt-4\n",
      "Epoch 10 Loss 2.8962 Accuracy 0.4310\n",
      "Time taken for 1 epoch: 453.47364830970764 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 2.9163 Accuracy 0.4215\n",
      "Epoch 11 Batch 50 Loss 2.8693 Accuracy 0.4316\n",
      "Epoch 11 Batch 100 Loss 2.8710 Accuracy 0.4310\n",
      "Epoch 11 Batch 150 Loss 2.8686 Accuracy 0.4317\n",
      "Epoch 11 Batch 200 Loss 2.8672 Accuracy 0.4327\n",
      "Epoch 11 Batch 250 Loss 2.8688 Accuracy 0.4326\n",
      "Epoch 11 Batch 300 Loss 2.8689 Accuracy 0.4329\n",
      "Epoch 11 Batch 350 Loss 2.8699 Accuracy 0.4328\n",
      "Epoch 11 Batch 400 Loss 2.8704 Accuracy 0.4329\n",
      "Epoch 11 Batch 450 Loss 2.8696 Accuracy 0.4329\n",
      "Epoch 11 Batch 500 Loss 2.8699 Accuracy 0.4329\n",
      "Epoch 11 Batch 550 Loss 2.8691 Accuracy 0.4333\n",
      "Epoch 11 Batch 600 Loss 2.8670 Accuracy 0.4335\n",
      "Epoch 11 Batch 650 Loss 2.8666 Accuracy 0.4336\n",
      "Epoch 11 Batch 700 Loss 2.8670 Accuracy 0.4336\n",
      "Epoch 11 Batch 750 Loss 2.8687 Accuracy 0.4334\n",
      "Epoch 11 Batch 800 Loss 2.8699 Accuracy 0.4332\n",
      "Epoch 11 Batch 850 Loss 2.8701 Accuracy 0.4333\n",
      "Epoch 11 Batch 900 Loss 2.8699 Accuracy 0.4333\n",
      "Epoch 11 Batch 950 Loss 2.8688 Accuracy 0.4335\n",
      "Epoch 11 Batch 1000 Loss 2.8691 Accuracy 0.4335\n",
      "Epoch 11 Batch 1050 Loss 2.8698 Accuracy 0.4333\n",
      "Epoch 11 Batch 1100 Loss 2.8699 Accuracy 0.4333\n",
      "Epoch 11 Batch 1150 Loss 2.8696 Accuracy 0.4334\n",
      "Epoch 11 Batch 1200 Loss 2.8703 Accuracy 0.4334\n",
      "Epoch 11 Batch 1250 Loss 2.8695 Accuracy 0.4334\n",
      "Epoch 11 Batch 1300 Loss 2.8707 Accuracy 0.4333\n",
      "Epoch 11 Batch 1350 Loss 2.8703 Accuracy 0.4334\n",
      "Epoch 11 Batch 1400 Loss 2.8713 Accuracy 0.4333\n",
      "Epoch 11 Batch 1450 Loss 2.8707 Accuracy 0.4334\n",
      "Epoch 11 Batch 1500 Loss 2.8700 Accuracy 0.4335\n",
      "Epoch 11 Batch 1550 Loss 2.8697 Accuracy 0.4336\n",
      "Epoch 11 Batch 1600 Loss 2.8694 Accuracy 0.4337\n",
      "Epoch 11 Batch 1650 Loss 2.8699 Accuracy 0.4336\n",
      "Epoch 11 Batch 1700 Loss 2.8699 Accuracy 0.4337\n",
      "Epoch 11 Batch 1750 Loss 2.8700 Accuracy 0.4337\n",
      "Epoch 11 Batch 1800 Loss 2.8704 Accuracy 0.4337\n",
      "Epoch 11 Batch 1850 Loss 2.8706 Accuracy 0.4337\n",
      "Epoch 11 Batch 1900 Loss 2.8709 Accuracy 0.4336\n",
      "Epoch 11 Batch 1950 Loss 2.8714 Accuracy 0.4335\n",
      "Epoch 11 Batch 2000 Loss 2.8719 Accuracy 0.4334\n",
      "Epoch 11 Batch 2050 Loss 2.8720 Accuracy 0.4334\n",
      "Epoch 11 Batch 2100 Loss 2.8714 Accuracy 0.4334\n",
      "Epoch 11 Batch 2150 Loss 2.8719 Accuracy 0.4333\n",
      "Epoch 11 Batch 2200 Loss 2.8722 Accuracy 0.4333\n",
      "Epoch 11 Batch 2250 Loss 2.8720 Accuracy 0.4333\n",
      "Epoch 11 Batch 2300 Loss 2.8725 Accuracy 0.4332\n",
      "Epoch 11 Batch 2350 Loss 2.8722 Accuracy 0.4333\n",
      "Epoch 11 Batch 2400 Loss 2.8720 Accuracy 0.4333\n",
      "Epoch 11 Batch 2450 Loss 2.8725 Accuracy 0.4332\n",
      "Epoch 11 Batch 2500 Loss 2.8726 Accuracy 0.4332\n",
      "Epoch 11 Batch 2550 Loss 2.8727 Accuracy 0.4332\n",
      "Epoch 11 Batch 2600 Loss 2.8727 Accuracy 0.4332\n",
      "Epoch 11 Loss 2.8725 Accuracy 0.4333\n",
      "Time taken for 1 epoch: 454.9811854362488 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 2.8533 Accuracy 0.4417\n",
      "Epoch 12 Batch 50 Loss 2.8457 Accuracy 0.4317\n",
      "Epoch 12 Batch 100 Loss 2.8487 Accuracy 0.4328\n",
      "Epoch 12 Batch 150 Loss 2.8425 Accuracy 0.4342\n",
      "Epoch 12 Batch 200 Loss 2.8494 Accuracy 0.4338\n",
      "Epoch 12 Batch 250 Loss 2.8497 Accuracy 0.4339\n",
      "Epoch 12 Batch 300 Loss 2.8474 Accuracy 0.4346\n",
      "Epoch 12 Batch 350 Loss 2.8428 Accuracy 0.4355\n",
      "Epoch 12 Batch 400 Loss 2.8425 Accuracy 0.4353\n",
      "Epoch 12 Batch 450 Loss 2.8445 Accuracy 0.4352\n",
      "Epoch 12 Batch 500 Loss 2.8451 Accuracy 0.4352\n",
      "Epoch 12 Batch 550 Loss 2.8450 Accuracy 0.4353\n",
      "Epoch 12 Batch 600 Loss 2.8440 Accuracy 0.4356\n",
      "Epoch 12 Batch 650 Loss 2.8431 Accuracy 0.4357\n",
      "Epoch 12 Batch 700 Loss 2.8430 Accuracy 0.4358\n",
      "Epoch 12 Batch 750 Loss 2.8427 Accuracy 0.4360\n",
      "Epoch 12 Batch 800 Loss 2.8440 Accuracy 0.4358\n",
      "Epoch 12 Batch 850 Loss 2.8436 Accuracy 0.4359\n",
      "Epoch 12 Batch 900 Loss 2.8440 Accuracy 0.4360\n",
      "Epoch 12 Batch 950 Loss 2.8443 Accuracy 0.4359\n",
      "Epoch 12 Batch 1000 Loss 2.8444 Accuracy 0.4360\n",
      "Epoch 12 Batch 1050 Loss 2.8454 Accuracy 0.4358\n",
      "Epoch 12 Batch 1100 Loss 2.8454 Accuracy 0.4358\n",
      "Epoch 12 Batch 1150 Loss 2.8454 Accuracy 0.4358\n",
      "Epoch 12 Batch 1200 Loss 2.8456 Accuracy 0.4358\n",
      "Epoch 12 Batch 1250 Loss 2.8452 Accuracy 0.4359\n",
      "Epoch 12 Batch 1300 Loss 2.8451 Accuracy 0.4359\n",
      "Epoch 12 Batch 1350 Loss 2.8462 Accuracy 0.4356\n",
      "Epoch 12 Batch 1400 Loss 2.8463 Accuracy 0.4357\n",
      "Epoch 12 Batch 1450 Loss 2.8460 Accuracy 0.4358\n",
      "Epoch 12 Batch 1500 Loss 2.8466 Accuracy 0.4358\n",
      "Epoch 12 Batch 1550 Loss 2.8467 Accuracy 0.4359\n",
      "Epoch 12 Batch 1600 Loss 2.8468 Accuracy 0.4359\n",
      "Epoch 12 Batch 1650 Loss 2.8471 Accuracy 0.4359\n",
      "Epoch 12 Batch 1700 Loss 2.8475 Accuracy 0.4358\n",
      "Epoch 12 Batch 1750 Loss 2.8477 Accuracy 0.4358\n",
      "Epoch 12 Batch 1800 Loss 2.8482 Accuracy 0.4357\n",
      "Epoch 12 Batch 1850 Loss 2.8480 Accuracy 0.4358\n",
      "Epoch 12 Batch 1900 Loss 2.8472 Accuracy 0.4359\n",
      "Epoch 12 Batch 1950 Loss 2.8473 Accuracy 0.4359\n",
      "Epoch 12 Batch 2000 Loss 2.8472 Accuracy 0.4359\n",
      "Epoch 12 Batch 2050 Loss 2.8472 Accuracy 0.4359\n",
      "Epoch 12 Batch 2100 Loss 2.8474 Accuracy 0.4359\n",
      "Epoch 12 Batch 2150 Loss 2.8481 Accuracy 0.4358\n",
      "Epoch 12 Batch 2200 Loss 2.8476 Accuracy 0.4358\n",
      "Epoch 12 Batch 2250 Loss 2.8476 Accuracy 0.4358\n",
      "Epoch 12 Batch 2300 Loss 2.8481 Accuracy 0.4358\n",
      "Epoch 12 Batch 2350 Loss 2.8483 Accuracy 0.4357\n",
      "Epoch 12 Batch 2400 Loss 2.8484 Accuracy 0.4358\n",
      "Epoch 12 Batch 2450 Loss 2.8486 Accuracy 0.4358\n",
      "Epoch 12 Batch 2500 Loss 2.8489 Accuracy 0.4357\n",
      "Epoch 12 Batch 2550 Loss 2.8491 Accuracy 0.4358\n",
      "Epoch 12 Batch 2600 Loss 2.8498 Accuracy 0.4357\n",
      "Epoch 12 Loss 2.8498 Accuracy 0.4357\n",
      "Time taken for 1 epoch: 453.05119347572327 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 2.7751 Accuracy 0.4439\n",
      "Epoch 13 Batch 50 Loss 2.8162 Accuracy 0.4391\n",
      "Epoch 13 Batch 100 Loss 2.8119 Accuracy 0.4395\n",
      "Epoch 13 Batch 150 Loss 2.8158 Accuracy 0.4394\n",
      "Epoch 13 Batch 200 Loss 2.8174 Accuracy 0.4392\n",
      "Epoch 13 Batch 250 Loss 2.8179 Accuracy 0.4390\n",
      "Epoch 13 Batch 300 Loss 2.8193 Accuracy 0.4387\n",
      "Epoch 13 Batch 350 Loss 2.8195 Accuracy 0.4386\n",
      "Epoch 13 Batch 400 Loss 2.8218 Accuracy 0.4387\n",
      "Epoch 13 Batch 450 Loss 2.8210 Accuracy 0.4388\n",
      "Epoch 13 Batch 500 Loss 2.8237 Accuracy 0.4385\n",
      "Epoch 13 Batch 550 Loss 2.8249 Accuracy 0.4384\n",
      "Epoch 13 Batch 600 Loss 2.8242 Accuracy 0.4386\n",
      "Epoch 13 Batch 650 Loss 2.8241 Accuracy 0.4386\n",
      "Epoch 13 Batch 700 Loss 2.8257 Accuracy 0.4382\n",
      "Epoch 13 Batch 750 Loss 2.8256 Accuracy 0.4382\n",
      "Epoch 13 Batch 800 Loss 2.8253 Accuracy 0.4382\n",
      "Epoch 13 Batch 850 Loss 2.8249 Accuracy 0.4384\n",
      "Epoch 13 Batch 900 Loss 2.8239 Accuracy 0.4386\n",
      "Epoch 13 Batch 950 Loss 2.8238 Accuracy 0.4386\n",
      "Epoch 13 Batch 1000 Loss 2.8247 Accuracy 0.4384\n",
      "Epoch 13 Batch 1050 Loss 2.8253 Accuracy 0.4383\n",
      "Epoch 13 Batch 1100 Loss 2.8248 Accuracy 0.4383\n",
      "Epoch 13 Batch 1150 Loss 2.8254 Accuracy 0.4382\n",
      "Epoch 13 Batch 1200 Loss 2.8246 Accuracy 0.4383\n",
      "Epoch 13 Batch 1250 Loss 2.8248 Accuracy 0.4383\n",
      "Epoch 13 Batch 1300 Loss 2.8245 Accuracy 0.4383\n",
      "Epoch 13 Batch 1350 Loss 2.8240 Accuracy 0.4384\n",
      "Epoch 13 Batch 1400 Loss 2.8243 Accuracy 0.4384\n",
      "Epoch 13 Batch 1450 Loss 2.8243 Accuracy 0.4384\n",
      "Epoch 13 Batch 1500 Loss 2.8249 Accuracy 0.4384\n",
      "Epoch 13 Batch 1550 Loss 2.8250 Accuracy 0.4384\n",
      "Epoch 13 Batch 1600 Loss 2.8247 Accuracy 0.4385\n",
      "Epoch 13 Batch 1650 Loss 2.8250 Accuracy 0.4384\n",
      "Epoch 13 Batch 1700 Loss 2.8253 Accuracy 0.4384\n",
      "Epoch 13 Batch 1750 Loss 2.8258 Accuracy 0.4383\n",
      "Epoch 13 Batch 1800 Loss 2.8256 Accuracy 0.4383\n",
      "Epoch 13 Batch 1850 Loss 2.8256 Accuracy 0.4383\n",
      "Epoch 13 Batch 1900 Loss 2.8261 Accuracy 0.4381\n",
      "Epoch 13 Batch 1950 Loss 2.8263 Accuracy 0.4381\n",
      "Epoch 13 Batch 2000 Loss 2.8266 Accuracy 0.4380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 2050 Loss 2.8269 Accuracy 0.4380\n",
      "Epoch 13 Batch 2100 Loss 2.8274 Accuracy 0.4380\n",
      "Epoch 13 Batch 2150 Loss 2.8275 Accuracy 0.4380\n",
      "Epoch 13 Batch 2200 Loss 2.8273 Accuracy 0.4380\n",
      "Epoch 13 Batch 2250 Loss 2.8275 Accuracy 0.4379\n",
      "Epoch 13 Batch 2300 Loss 2.8275 Accuracy 0.4379\n",
      "Epoch 13 Batch 2350 Loss 2.8278 Accuracy 0.4379\n",
      "Epoch 13 Batch 2400 Loss 2.8275 Accuracy 0.4379\n",
      "Epoch 13 Batch 2450 Loss 2.8281 Accuracy 0.4379\n",
      "Epoch 13 Batch 2500 Loss 2.8284 Accuracy 0.4378\n",
      "Epoch 13 Batch 2550 Loss 2.8283 Accuracy 0.4379\n",
      "Epoch 13 Batch 2600 Loss 2.8287 Accuracy 0.4378\n",
      "Epoch 13 Loss 2.8287 Accuracy 0.4378\n",
      "Time taken for 1 epoch: 452.609117269516 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 2.7794 Accuracy 0.4534\n",
      "Epoch 14 Batch 50 Loss 2.7867 Accuracy 0.4408\n",
      "Epoch 14 Batch 100 Loss 2.7978 Accuracy 0.4387\n",
      "Epoch 14 Batch 150 Loss 2.7974 Accuracy 0.4388\n",
      "Epoch 14 Batch 200 Loss 2.7925 Accuracy 0.4397\n",
      "Epoch 14 Batch 250 Loss 2.7906 Accuracy 0.4404\n",
      "Epoch 14 Batch 300 Loss 2.7965 Accuracy 0.4397\n",
      "Epoch 14 Batch 350 Loss 2.7982 Accuracy 0.4403\n",
      "Epoch 14 Batch 400 Loss 2.7991 Accuracy 0.4401\n",
      "Epoch 14 Batch 450 Loss 2.8006 Accuracy 0.4400\n",
      "Epoch 14 Batch 500 Loss 2.8026 Accuracy 0.4398\n",
      "Epoch 14 Batch 550 Loss 2.8019 Accuracy 0.4397\n",
      "Epoch 14 Batch 600 Loss 2.8024 Accuracy 0.4399\n",
      "Epoch 14 Batch 650 Loss 2.8023 Accuracy 0.4401\n",
      "Epoch 14 Batch 700 Loss 2.8035 Accuracy 0.4401\n",
      "Epoch 14 Batch 750 Loss 2.8050 Accuracy 0.4400\n",
      "Epoch 14 Batch 800 Loss 2.8037 Accuracy 0.4402\n",
      "Epoch 14 Batch 850 Loss 2.8050 Accuracy 0.4400\n",
      "Epoch 14 Batch 900 Loss 2.8045 Accuracy 0.4402\n",
      "Epoch 14 Batch 950 Loss 2.8043 Accuracy 0.4401\n",
      "Epoch 14 Batch 1000 Loss 2.8057 Accuracy 0.4400\n",
      "Epoch 14 Batch 1050 Loss 2.8056 Accuracy 0.4400\n",
      "Epoch 14 Batch 1100 Loss 2.8052 Accuracy 0.4400\n",
      "Epoch 14 Batch 1150 Loss 2.8049 Accuracy 0.4401\n",
      "Epoch 14 Batch 1200 Loss 2.8051 Accuracy 0.4401\n",
      "Epoch 14 Batch 1250 Loss 2.8056 Accuracy 0.4401\n",
      "Epoch 14 Batch 1300 Loss 2.8049 Accuracy 0.4402\n",
      "Epoch 14 Batch 1350 Loss 2.8057 Accuracy 0.4400\n",
      "Epoch 14 Batch 1400 Loss 2.8063 Accuracy 0.4399\n",
      "Epoch 14 Batch 1450 Loss 2.8063 Accuracy 0.4399\n",
      "Epoch 14 Batch 1500 Loss 2.8063 Accuracy 0.4400\n",
      "Epoch 14 Batch 1550 Loss 2.8059 Accuracy 0.4400\n",
      "Epoch 14 Batch 1600 Loss 2.8060 Accuracy 0.4400\n",
      "Epoch 14 Batch 1650 Loss 2.8058 Accuracy 0.4401\n",
      "Epoch 14 Batch 1700 Loss 2.8062 Accuracy 0.4401\n",
      "Epoch 14 Batch 1750 Loss 2.8066 Accuracy 0.4400\n",
      "Epoch 14 Batch 1800 Loss 2.8071 Accuracy 0.4400\n",
      "Epoch 14 Batch 1850 Loss 2.8075 Accuracy 0.4400\n",
      "Epoch 14 Batch 1900 Loss 2.8080 Accuracy 0.4399\n",
      "Epoch 14 Batch 1950 Loss 2.8079 Accuracy 0.4400\n",
      "Epoch 14 Batch 2000 Loss 2.8081 Accuracy 0.4400\n",
      "Epoch 14 Batch 2050 Loss 2.8080 Accuracy 0.4400\n",
      "Epoch 14 Batch 2100 Loss 2.8076 Accuracy 0.4401\n",
      "Epoch 14 Batch 2150 Loss 2.8079 Accuracy 0.4400\n",
      "Epoch 14 Batch 2200 Loss 2.8079 Accuracy 0.4400\n",
      "Epoch 14 Batch 2250 Loss 2.8083 Accuracy 0.4400\n",
      "Epoch 14 Batch 2300 Loss 2.8080 Accuracy 0.4400\n",
      "Epoch 14 Batch 2350 Loss 2.8082 Accuracy 0.4400\n",
      "Epoch 14 Batch 2400 Loss 2.8080 Accuracy 0.4400\n",
      "Epoch 14 Batch 2450 Loss 2.8085 Accuracy 0.4400\n",
      "Epoch 14 Batch 2500 Loss 2.8090 Accuracy 0.4399\n",
      "Epoch 14 Batch 2550 Loss 2.8092 Accuracy 0.4399\n",
      "Epoch 14 Batch 2600 Loss 2.8097 Accuracy 0.4398\n",
      "Epoch 14 Loss 2.8096 Accuracy 0.4398\n",
      "Time taken for 1 epoch: 452.74096035957336 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 2.7854 Accuracy 0.4331\n",
      "Epoch 15 Batch 50 Loss 2.7650 Accuracy 0.4468\n",
      "Epoch 15 Batch 100 Loss 2.7671 Accuracy 0.4454\n",
      "Epoch 15 Batch 150 Loss 2.7784 Accuracy 0.4436\n",
      "Epoch 15 Batch 200 Loss 2.7798 Accuracy 0.4432\n",
      "Epoch 15 Batch 250 Loss 2.7807 Accuracy 0.4433\n",
      "Epoch 15 Batch 300 Loss 2.7824 Accuracy 0.4432\n",
      "Epoch 15 Batch 350 Loss 2.7850 Accuracy 0.4426\n",
      "Epoch 15 Batch 400 Loss 2.7863 Accuracy 0.4421\n",
      "Epoch 15 Batch 450 Loss 2.7833 Accuracy 0.4425\n",
      "Epoch 15 Batch 500 Loss 2.7858 Accuracy 0.4423\n",
      "Epoch 15 Batch 550 Loss 2.7852 Accuracy 0.4425\n",
      "Epoch 15 Batch 600 Loss 2.7863 Accuracy 0.4425\n",
      "Epoch 15 Batch 650 Loss 2.7892 Accuracy 0.4421\n",
      "Epoch 15 Batch 700 Loss 2.7891 Accuracy 0.4420\n",
      "Epoch 15 Batch 750 Loss 2.7894 Accuracy 0.4418\n",
      "Epoch 15 Batch 800 Loss 2.7903 Accuracy 0.4416\n",
      "Epoch 15 Batch 850 Loss 2.7898 Accuracy 0.4417\n",
      "Epoch 15 Batch 900 Loss 2.7901 Accuracy 0.4417\n",
      "Epoch 15 Batch 950 Loss 2.7913 Accuracy 0.4415\n",
      "Epoch 15 Batch 1000 Loss 2.7916 Accuracy 0.4414\n",
      "Epoch 15 Batch 1050 Loss 2.7912 Accuracy 0.4415\n",
      "Epoch 15 Batch 1100 Loss 2.7904 Accuracy 0.4416\n",
      "Epoch 15 Batch 1150 Loss 2.7906 Accuracy 0.4416\n",
      "Epoch 15 Batch 1200 Loss 2.7896 Accuracy 0.4418\n",
      "Epoch 15 Batch 1250 Loss 2.7895 Accuracy 0.4419\n",
      "Epoch 15 Batch 1300 Loss 2.7886 Accuracy 0.4421\n",
      "Epoch 15 Batch 1350 Loss 2.7880 Accuracy 0.4422\n",
      "Epoch 15 Batch 1400 Loss 2.7887 Accuracy 0.4421\n",
      "Epoch 15 Batch 1450 Loss 2.7882 Accuracy 0.4422\n",
      "Epoch 15 Batch 1500 Loss 2.7882 Accuracy 0.4423\n",
      "Epoch 15 Batch 1550 Loss 2.7884 Accuracy 0.4423\n",
      "Epoch 15 Batch 1600 Loss 2.7882 Accuracy 0.4423\n",
      "Epoch 15 Batch 1650 Loss 2.7878 Accuracy 0.4423\n",
      "Epoch 15 Batch 1700 Loss 2.7876 Accuracy 0.4423\n",
      "Epoch 15 Batch 1750 Loss 2.7877 Accuracy 0.4423\n",
      "Epoch 15 Batch 1800 Loss 2.7879 Accuracy 0.4423\n",
      "Epoch 15 Batch 1850 Loss 2.7882 Accuracy 0.4422\n",
      "Epoch 15 Batch 1900 Loss 2.7882 Accuracy 0.4422\n",
      "Epoch 15 Batch 1950 Loss 2.7883 Accuracy 0.4422\n",
      "Epoch 15 Batch 2000 Loss 2.7887 Accuracy 0.4421\n",
      "Epoch 15 Batch 2050 Loss 2.7889 Accuracy 0.4421\n",
      "Epoch 15 Batch 2100 Loss 2.7889 Accuracy 0.4421\n",
      "Epoch 15 Batch 2150 Loss 2.7889 Accuracy 0.4421\n",
      "Epoch 15 Batch 2200 Loss 2.7893 Accuracy 0.4420\n",
      "Epoch 15 Batch 2250 Loss 2.7892 Accuracy 0.4421\n",
      "Epoch 15 Batch 2300 Loss 2.7894 Accuracy 0.4421\n",
      "Epoch 15 Batch 2350 Loss 2.7895 Accuracy 0.4420\n",
      "Epoch 15 Batch 2400 Loss 2.7892 Accuracy 0.4421\n",
      "Epoch 15 Batch 2450 Loss 2.7895 Accuracy 0.4420\n",
      "Epoch 15 Batch 2500 Loss 2.7901 Accuracy 0.4420\n",
      "Epoch 15 Batch 2550 Loss 2.7904 Accuracy 0.4419\n",
      "Epoch 15 Batch 2600 Loss 2.7906 Accuracy 0.4420\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train_full/ckpt-5\n",
      "Epoch 15 Loss 2.7910 Accuracy 0.4419\n",
      "Time taken for 1 epoch: 453.6480202674866 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 2.8475 Accuracy 0.4199\n",
      "Epoch 16 Batch 50 Loss 2.7646 Accuracy 0.4433\n",
      "Epoch 16 Batch 100 Loss 2.7591 Accuracy 0.4446\n",
      "Epoch 16 Batch 150 Loss 2.7640 Accuracy 0.4437\n",
      "Epoch 16 Batch 200 Loss 2.7607 Accuracy 0.4443\n",
      "Epoch 16 Batch 250 Loss 2.7608 Accuracy 0.4447\n",
      "Epoch 16 Batch 300 Loss 2.7640 Accuracy 0.4441\n",
      "Epoch 16 Batch 350 Loss 2.7668 Accuracy 0.4435\n",
      "Epoch 16 Batch 400 Loss 2.7678 Accuracy 0.4434\n",
      "Epoch 16 Batch 450 Loss 2.7675 Accuracy 0.4437\n",
      "Epoch 16 Batch 500 Loss 2.7675 Accuracy 0.4438\n",
      "Epoch 16 Batch 550 Loss 2.7680 Accuracy 0.4438\n",
      "Epoch 16 Batch 600 Loss 2.7675 Accuracy 0.4438\n",
      "Epoch 16 Batch 650 Loss 2.7681 Accuracy 0.4437\n",
      "Epoch 16 Batch 700 Loss 2.7674 Accuracy 0.4438\n",
      "Epoch 16 Batch 750 Loss 2.7672 Accuracy 0.4440\n",
      "Epoch 16 Batch 800 Loss 2.7674 Accuracy 0.4439\n",
      "Epoch 16 Batch 850 Loss 2.7686 Accuracy 0.4438\n",
      "Epoch 16 Batch 900 Loss 2.7690 Accuracy 0.4439\n",
      "Epoch 16 Batch 950 Loss 2.7682 Accuracy 0.4440\n",
      "Epoch 16 Batch 1000 Loss 2.7686 Accuracy 0.4440\n",
      "Epoch 16 Batch 1050 Loss 2.7681 Accuracy 0.4440\n",
      "Epoch 16 Batch 1100 Loss 2.7680 Accuracy 0.4440\n",
      "Epoch 16 Batch 1150 Loss 2.7680 Accuracy 0.4439\n",
      "Epoch 16 Batch 1200 Loss 2.7667 Accuracy 0.4440\n",
      "Epoch 16 Batch 1250 Loss 2.7675 Accuracy 0.4439\n",
      "Epoch 16 Batch 1300 Loss 2.7679 Accuracy 0.4438\n",
      "Epoch 16 Batch 1350 Loss 2.7686 Accuracy 0.4438\n",
      "Epoch 16 Batch 1400 Loss 2.7690 Accuracy 0.4438\n",
      "Epoch 16 Batch 1450 Loss 2.7692 Accuracy 0.4439\n",
      "Epoch 16 Batch 1500 Loss 2.7695 Accuracy 0.4438\n",
      "Epoch 16 Batch 1550 Loss 2.7694 Accuracy 0.4438\n",
      "Epoch 16 Batch 1600 Loss 2.7693 Accuracy 0.4439\n",
      "Epoch 16 Batch 1650 Loss 2.7692 Accuracy 0.4439\n",
      "Epoch 16 Batch 1700 Loss 2.7689 Accuracy 0.4439\n",
      "Epoch 16 Batch 1750 Loss 2.7693 Accuracy 0.4440\n",
      "Epoch 16 Batch 1800 Loss 2.7700 Accuracy 0.4440\n",
      "Epoch 16 Batch 1850 Loss 2.7703 Accuracy 0.4439\n",
      "Epoch 16 Batch 1900 Loss 2.7703 Accuracy 0.4439\n",
      "Epoch 16 Batch 1950 Loss 2.7706 Accuracy 0.4439\n",
      "Epoch 16 Batch 2000 Loss 2.7711 Accuracy 0.4438\n",
      "Epoch 16 Batch 2050 Loss 2.7709 Accuracy 0.4439\n",
      "Epoch 16 Batch 2100 Loss 2.7714 Accuracy 0.4438\n",
      "Epoch 16 Batch 2150 Loss 2.7713 Accuracy 0.4438\n",
      "Epoch 16 Batch 2200 Loss 2.7713 Accuracy 0.4438\n",
      "Epoch 16 Batch 2250 Loss 2.7709 Accuracy 0.4438\n",
      "Epoch 16 Batch 2300 Loss 2.7713 Accuracy 0.4438\n",
      "Epoch 16 Batch 2350 Loss 2.7713 Accuracy 0.4438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Batch 2400 Loss 2.7712 Accuracy 0.4438\n",
      "Epoch 16 Batch 2450 Loss 2.7717 Accuracy 0.4438\n",
      "Epoch 16 Batch 2500 Loss 2.7716 Accuracy 0.4438\n",
      "Epoch 16 Batch 2550 Loss 2.7719 Accuracy 0.4438\n",
      "Epoch 16 Batch 2600 Loss 2.7718 Accuracy 0.4439\n",
      "Epoch 16 Loss 2.7721 Accuracy 0.4438\n",
      "Time taken for 1 epoch: 454.6567151546478 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 2.6899 Accuracy 0.4391\n",
      "Epoch 17 Batch 50 Loss 2.7409 Accuracy 0.4462\n",
      "Epoch 17 Batch 100 Loss 2.7453 Accuracy 0.4462\n",
      "Epoch 17 Batch 150 Loss 2.7418 Accuracy 0.4475\n",
      "Epoch 17 Batch 200 Loss 2.7414 Accuracy 0.4472\n",
      "Epoch 17 Batch 250 Loss 2.7476 Accuracy 0.4465\n",
      "Epoch 17 Batch 300 Loss 2.7453 Accuracy 0.4473\n",
      "Epoch 17 Batch 350 Loss 2.7460 Accuracy 0.4473\n",
      "Epoch 17 Batch 400 Loss 2.7480 Accuracy 0.4468\n",
      "Epoch 17 Batch 450 Loss 2.7496 Accuracy 0.4465\n",
      "Epoch 17 Batch 500 Loss 2.7501 Accuracy 0.4463\n",
      "Epoch 17 Batch 550 Loss 2.7480 Accuracy 0.4466\n",
      "Epoch 17 Batch 600 Loss 2.7502 Accuracy 0.4462\n",
      "Epoch 17 Batch 650 Loss 2.7515 Accuracy 0.4462\n",
      "Epoch 17 Batch 700 Loss 2.7529 Accuracy 0.4459\n",
      "Epoch 17 Batch 750 Loss 2.7532 Accuracy 0.4457\n",
      "Epoch 17 Batch 800 Loss 2.7540 Accuracy 0.4456\n",
      "Epoch 17 Batch 850 Loss 2.7525 Accuracy 0.4457\n",
      "Epoch 17 Batch 900 Loss 2.7504 Accuracy 0.4461\n",
      "Epoch 17 Batch 950 Loss 2.7506 Accuracy 0.4461\n",
      "Epoch 17 Batch 1000 Loss 2.7504 Accuracy 0.4461\n",
      "Epoch 17 Batch 1050 Loss 2.7510 Accuracy 0.4460\n",
      "Epoch 17 Batch 1100 Loss 2.7514 Accuracy 0.4460\n",
      "Epoch 17 Batch 1150 Loss 2.7519 Accuracy 0.4459\n",
      "Epoch 17 Batch 1200 Loss 2.7514 Accuracy 0.4460\n",
      "Epoch 17 Batch 1250 Loss 2.7518 Accuracy 0.4461\n",
      "Epoch 17 Batch 1300 Loss 2.7519 Accuracy 0.4461\n",
      "Epoch 17 Batch 1350 Loss 2.7520 Accuracy 0.4461\n",
      "Epoch 17 Batch 1400 Loss 2.7526 Accuracy 0.4461\n",
      "Epoch 17 Batch 1450 Loss 2.7527 Accuracy 0.4461\n",
      "Epoch 17 Batch 1500 Loss 2.7528 Accuracy 0.4461\n",
      "Epoch 17 Batch 1550 Loss 2.7531 Accuracy 0.4460\n",
      "Epoch 17 Batch 1600 Loss 2.7525 Accuracy 0.4462\n",
      "Epoch 17 Batch 1650 Loss 2.7524 Accuracy 0.4463\n",
      "Epoch 17 Batch 1700 Loss 2.7526 Accuracy 0.4462\n",
      "Epoch 17 Batch 1750 Loss 2.7529 Accuracy 0.4462\n",
      "Epoch 17 Batch 1800 Loss 2.7537 Accuracy 0.4462\n",
      "Epoch 17 Batch 1850 Loss 2.7538 Accuracy 0.4462\n",
      "Epoch 17 Batch 1900 Loss 2.7540 Accuracy 0.4461\n",
      "Epoch 17 Batch 1950 Loss 2.7540 Accuracy 0.4461\n",
      "Epoch 17 Batch 2000 Loss 2.7538 Accuracy 0.4462\n",
      "Epoch 17 Batch 2050 Loss 2.7540 Accuracy 0.4462\n",
      "Epoch 17 Batch 2100 Loss 2.7538 Accuracy 0.4462\n",
      "Epoch 17 Batch 2150 Loss 2.7536 Accuracy 0.4462\n",
      "Epoch 17 Batch 2200 Loss 2.7540 Accuracy 0.4462\n",
      "Epoch 17 Batch 2250 Loss 2.7544 Accuracy 0.4462\n",
      "Epoch 17 Batch 2300 Loss 2.7545 Accuracy 0.4461\n",
      "Epoch 17 Batch 2350 Loss 2.7550 Accuracy 0.4461\n",
      "Epoch 17 Batch 2400 Loss 2.7550 Accuracy 0.4461\n",
      "Epoch 17 Batch 2450 Loss 2.7551 Accuracy 0.4461\n",
      "Epoch 17 Batch 2500 Loss 2.7553 Accuracy 0.4461\n",
      "Epoch 17 Batch 2550 Loss 2.7562 Accuracy 0.4459\n",
      "Epoch 17 Batch 2600 Loss 2.7564 Accuracy 0.4459\n",
      "Epoch 17 Loss 2.7562 Accuracy 0.4460\n",
      "Time taken for 1 epoch: 453.2511534690857 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 2.7613 Accuracy 0.4398\n",
      "Epoch 18 Batch 50 Loss 2.7136 Accuracy 0.4510\n",
      "Epoch 18 Batch 100 Loss 2.7158 Accuracy 0.4507\n",
      "Epoch 18 Batch 150 Loss 2.7221 Accuracy 0.4493\n",
      "Epoch 18 Batch 200 Loss 2.7240 Accuracy 0.4490\n",
      "Epoch 18 Batch 250 Loss 2.7253 Accuracy 0.4491\n",
      "Epoch 18 Batch 300 Loss 2.7259 Accuracy 0.4491\n",
      "Epoch 18 Batch 350 Loss 2.7268 Accuracy 0.4489\n",
      "Epoch 18 Batch 400 Loss 2.7275 Accuracy 0.4488\n",
      "Epoch 18 Batch 450 Loss 2.7283 Accuracy 0.4487\n",
      "Epoch 18 Batch 500 Loss 2.7301 Accuracy 0.4485\n",
      "Epoch 18 Batch 550 Loss 2.7319 Accuracy 0.4481\n",
      "Epoch 18 Batch 600 Loss 2.7319 Accuracy 0.4482\n",
      "Epoch 18 Batch 650 Loss 2.7325 Accuracy 0.4482\n",
      "Epoch 18 Batch 700 Loss 2.7343 Accuracy 0.4480\n",
      "Epoch 18 Batch 750 Loss 2.7347 Accuracy 0.4479\n",
      "Epoch 18 Batch 800 Loss 2.7342 Accuracy 0.4479\n",
      "Epoch 18 Batch 850 Loss 2.7343 Accuracy 0.4478\n",
      "Epoch 18 Batch 900 Loss 2.7339 Accuracy 0.4477\n",
      "Epoch 18 Batch 950 Loss 2.7347 Accuracy 0.4477\n",
      "Epoch 18 Batch 1000 Loss 2.7351 Accuracy 0.4477\n",
      "Epoch 18 Batch 1050 Loss 2.7359 Accuracy 0.4476\n",
      "Epoch 18 Batch 1100 Loss 2.7358 Accuracy 0.4476\n",
      "Epoch 18 Batch 1150 Loss 2.7366 Accuracy 0.4475\n",
      "Epoch 18 Batch 1200 Loss 2.7367 Accuracy 0.4476\n",
      "Epoch 18 Batch 1250 Loss 2.7373 Accuracy 0.4474\n",
      "Epoch 18 Batch 1300 Loss 2.7370 Accuracy 0.4475\n",
      "Epoch 18 Batch 1350 Loss 2.7372 Accuracy 0.4474\n",
      "Epoch 18 Batch 1400 Loss 2.7371 Accuracy 0.4475\n",
      "Epoch 18 Batch 1450 Loss 2.7374 Accuracy 0.4474\n",
      "Epoch 18 Batch 1500 Loss 2.7373 Accuracy 0.4475\n",
      "Epoch 18 Batch 1550 Loss 2.7376 Accuracy 0.4475\n",
      "Epoch 18 Batch 1600 Loss 2.7384 Accuracy 0.4474\n",
      "Epoch 18 Batch 1650 Loss 2.7383 Accuracy 0.4474\n",
      "Epoch 18 Batch 1700 Loss 2.7380 Accuracy 0.4474\n",
      "Epoch 18 Batch 1750 Loss 2.7376 Accuracy 0.4474\n",
      "Epoch 18 Batch 1800 Loss 2.7377 Accuracy 0.4475\n",
      "Epoch 18 Batch 1850 Loss 2.7385 Accuracy 0.4474\n",
      "Epoch 18 Batch 1900 Loss 2.7391 Accuracy 0.4474\n",
      "Epoch 18 Batch 1950 Loss 2.7390 Accuracy 0.4474\n",
      "Epoch 18 Batch 2000 Loss 2.7391 Accuracy 0.4474\n",
      "Epoch 18 Batch 2050 Loss 2.7392 Accuracy 0.4474\n",
      "Epoch 18 Batch 2100 Loss 2.7393 Accuracy 0.4474\n",
      "Epoch 18 Batch 2150 Loss 2.7395 Accuracy 0.4474\n",
      "Epoch 18 Batch 2200 Loss 2.7397 Accuracy 0.4474\n",
      "Epoch 18 Batch 2250 Loss 2.7396 Accuracy 0.4474\n",
      "Epoch 18 Batch 2300 Loss 2.7396 Accuracy 0.4474\n",
      "Epoch 18 Batch 2350 Loss 2.7394 Accuracy 0.4475\n",
      "Epoch 18 Batch 2400 Loss 2.7391 Accuracy 0.4475\n",
      "Epoch 18 Batch 2450 Loss 2.7391 Accuracy 0.4476\n",
      "Epoch 18 Batch 2500 Loss 2.7389 Accuracy 0.4476\n",
      "Epoch 18 Batch 2550 Loss 2.7388 Accuracy 0.4476\n",
      "Epoch 18 Batch 2600 Loss 2.7392 Accuracy 0.4476\n",
      "Epoch 18 Loss 2.7396 Accuracy 0.4475\n",
      "Time taken for 1 epoch: 452.47053694725037 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 2.6373 Accuracy 0.4727\n",
      "Epoch 19 Batch 50 Loss 2.7073 Accuracy 0.4496\n",
      "Epoch 19 Batch 100 Loss 2.7057 Accuracy 0.4500\n",
      "Epoch 19 Batch 150 Loss 2.7137 Accuracy 0.4490\n",
      "Epoch 19 Batch 200 Loss 2.7085 Accuracy 0.4499\n",
      "Epoch 19 Batch 250 Loss 2.7134 Accuracy 0.4493\n",
      "Epoch 19 Batch 300 Loss 2.7127 Accuracy 0.4498\n",
      "Epoch 19 Batch 350 Loss 2.7131 Accuracy 0.4501\n",
      "Epoch 19 Batch 400 Loss 2.7169 Accuracy 0.4495\n",
      "Epoch 19 Batch 450 Loss 2.7175 Accuracy 0.4495\n",
      "Epoch 19 Batch 500 Loss 2.7191 Accuracy 0.4492\n",
      "Epoch 19 Batch 550 Loss 2.7198 Accuracy 0.4490\n",
      "Epoch 19 Batch 600 Loss 2.7190 Accuracy 0.4495\n",
      "Epoch 19 Batch 650 Loss 2.7208 Accuracy 0.4492\n",
      "Epoch 19 Batch 700 Loss 2.7213 Accuracy 0.4492\n",
      "Epoch 19 Batch 750 Loss 2.7222 Accuracy 0.4491\n",
      "Epoch 19 Batch 800 Loss 2.7218 Accuracy 0.4493\n",
      "Epoch 19 Batch 850 Loss 2.7222 Accuracy 0.4492\n",
      "Epoch 19 Batch 900 Loss 2.7219 Accuracy 0.4492\n",
      "Epoch 19 Batch 950 Loss 2.7225 Accuracy 0.4491\n",
      "Epoch 19 Batch 1000 Loss 2.7222 Accuracy 0.4492\n",
      "Epoch 19 Batch 1050 Loss 2.7232 Accuracy 0.4491\n",
      "Epoch 19 Batch 1100 Loss 2.7232 Accuracy 0.4491\n",
      "Epoch 19 Batch 1150 Loss 2.7226 Accuracy 0.4492\n",
      "Epoch 19 Batch 1200 Loss 2.7225 Accuracy 0.4492\n",
      "Epoch 19 Batch 1250 Loss 2.7223 Accuracy 0.4493\n",
      "Epoch 19 Batch 1300 Loss 2.7224 Accuracy 0.4493\n",
      "Epoch 19 Batch 1350 Loss 2.7218 Accuracy 0.4494\n",
      "Epoch 19 Batch 1400 Loss 2.7223 Accuracy 0.4494\n",
      "Epoch 19 Batch 1450 Loss 2.7222 Accuracy 0.4494\n",
      "Epoch 19 Batch 1500 Loss 2.7224 Accuracy 0.4494\n",
      "Epoch 19 Batch 1550 Loss 2.7227 Accuracy 0.4494\n",
      "Epoch 19 Batch 1600 Loss 2.7224 Accuracy 0.4495\n",
      "Epoch 19 Batch 1650 Loss 2.7221 Accuracy 0.4495\n",
      "Epoch 19 Batch 1700 Loss 2.7221 Accuracy 0.4495\n",
      "Epoch 19 Batch 1750 Loss 2.7221 Accuracy 0.4495\n",
      "Epoch 19 Batch 1800 Loss 2.7223 Accuracy 0.4495\n",
      "Epoch 19 Batch 1850 Loss 2.7225 Accuracy 0.4495\n",
      "Epoch 19 Batch 1900 Loss 2.7229 Accuracy 0.4495\n",
      "Epoch 19 Batch 1950 Loss 2.7233 Accuracy 0.4494\n",
      "Epoch 19 Batch 2000 Loss 2.7236 Accuracy 0.4494\n",
      "Epoch 19 Batch 2050 Loss 2.7231 Accuracy 0.4495\n",
      "Epoch 19 Batch 2100 Loss 2.7230 Accuracy 0.4495\n",
      "Epoch 19 Batch 2150 Loss 2.7231 Accuracy 0.4494\n",
      "Epoch 19 Batch 2200 Loss 2.7240 Accuracy 0.4493\n",
      "Epoch 19 Batch 2250 Loss 2.7241 Accuracy 0.4492\n",
      "Epoch 19 Batch 2300 Loss 2.7244 Accuracy 0.4491\n",
      "Epoch 19 Batch 2350 Loss 2.7240 Accuracy 0.4492\n",
      "Epoch 19 Batch 2400 Loss 2.7240 Accuracy 0.4493\n",
      "Epoch 19 Batch 2450 Loss 2.7246 Accuracy 0.4492\n",
      "Epoch 19 Batch 2500 Loss 2.7247 Accuracy 0.4493\n",
      "Epoch 19 Batch 2550 Loss 2.7254 Accuracy 0.4492\n",
      "Epoch 19 Batch 2600 Loss 2.7259 Accuracy 0.4492\n",
      "Epoch 19 Loss 2.7258 Accuracy 0.4492\n",
      "Time taken for 1 epoch: 452.6399188041687 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 2.7038 Accuracy 0.4462\n",
      "Epoch 20 Batch 50 Loss 2.6823 Accuracy 0.4513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 100 Loss 2.6803 Accuracy 0.4533\n",
      "Epoch 20 Batch 150 Loss 2.6832 Accuracy 0.4533\n",
      "Epoch 20 Batch 200 Loss 2.6930 Accuracy 0.4520\n",
      "Epoch 20 Batch 250 Loss 2.6937 Accuracy 0.4521\n",
      "Epoch 20 Batch 300 Loss 2.6926 Accuracy 0.4524\n",
      "Epoch 20 Batch 350 Loss 2.6938 Accuracy 0.4521\n",
      "Epoch 20 Batch 400 Loss 2.6983 Accuracy 0.4514\n",
      "Epoch 20 Batch 450 Loss 2.7003 Accuracy 0.4512\n",
      "Epoch 20 Batch 500 Loss 2.7024 Accuracy 0.4511\n",
      "Epoch 20 Batch 550 Loss 2.7032 Accuracy 0.4512\n",
      "Epoch 20 Batch 600 Loss 2.7033 Accuracy 0.4513\n",
      "Epoch 20 Batch 650 Loss 2.7048 Accuracy 0.4512\n",
      "Epoch 20 Batch 700 Loss 2.7052 Accuracy 0.4513\n",
      "Epoch 20 Batch 750 Loss 2.7044 Accuracy 0.4515\n",
      "Epoch 20 Batch 800 Loss 2.7026 Accuracy 0.4517\n",
      "Epoch 20 Batch 850 Loss 2.7030 Accuracy 0.4517\n",
      "Epoch 20 Batch 900 Loss 2.7034 Accuracy 0.4517\n",
      "Epoch 20 Batch 950 Loss 2.7035 Accuracy 0.4517\n",
      "Epoch 20 Batch 1000 Loss 2.7037 Accuracy 0.4516\n",
      "Epoch 20 Batch 1050 Loss 2.7033 Accuracy 0.4517\n",
      "Epoch 20 Batch 1100 Loss 2.7037 Accuracy 0.4518\n",
      "Epoch 20 Batch 1150 Loss 2.7043 Accuracy 0.4516\n",
      "Epoch 20 Batch 1200 Loss 2.7044 Accuracy 0.4516\n",
      "Epoch 20 Batch 1250 Loss 2.7040 Accuracy 0.4517\n",
      "Epoch 20 Batch 1300 Loss 2.7039 Accuracy 0.4516\n",
      "Epoch 20 Batch 1350 Loss 2.7042 Accuracy 0.4516\n",
      "Epoch 20 Batch 1400 Loss 2.7043 Accuracy 0.4516\n",
      "Epoch 20 Batch 1450 Loss 2.7042 Accuracy 0.4516\n",
      "Epoch 20 Batch 1500 Loss 2.7040 Accuracy 0.4517\n",
      "Epoch 20 Batch 1550 Loss 2.7042 Accuracy 0.4517\n",
      "Epoch 20 Batch 1600 Loss 2.7045 Accuracy 0.4517\n",
      "Epoch 20 Batch 1650 Loss 2.7048 Accuracy 0.4517\n",
      "Epoch 20 Batch 1700 Loss 2.7053 Accuracy 0.4516\n",
      "Epoch 20 Batch 1750 Loss 2.7053 Accuracy 0.4517\n",
      "Epoch 20 Batch 1800 Loss 2.7056 Accuracy 0.4516\n",
      "Epoch 20 Batch 1850 Loss 2.7063 Accuracy 0.4515\n",
      "Epoch 20 Batch 1900 Loss 2.7059 Accuracy 0.4515\n",
      "Epoch 20 Batch 1950 Loss 2.7059 Accuracy 0.4515\n",
      "Epoch 20 Batch 2000 Loss 2.7061 Accuracy 0.4515\n",
      "Epoch 20 Batch 2050 Loss 2.7054 Accuracy 0.4516\n",
      "Epoch 20 Batch 2100 Loss 2.7056 Accuracy 0.4516\n",
      "Epoch 20 Batch 2150 Loss 2.7063 Accuracy 0.4515\n",
      "Epoch 20 Batch 2200 Loss 2.7059 Accuracy 0.4516\n",
      "Epoch 20 Batch 2250 Loss 2.7064 Accuracy 0.4514\n",
      "Epoch 20 Batch 2300 Loss 2.7061 Accuracy 0.4515\n",
      "Epoch 20 Batch 2350 Loss 2.7066 Accuracy 0.4514\n",
      "Epoch 20 Batch 2400 Loss 2.7072 Accuracy 0.4514\n",
      "Epoch 20 Batch 2450 Loss 2.7072 Accuracy 0.4514\n",
      "Epoch 20 Batch 2500 Loss 2.7075 Accuracy 0.4514\n",
      "Epoch 20 Batch 2550 Loss 2.7078 Accuracy 0.4513\n",
      "Epoch 20 Batch 2600 Loss 2.7082 Accuracy 0.4513\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train_full/ckpt-6\n",
      "Epoch 20 Loss 2.7087 Accuracy 0.4512\n",
      "Time taken for 1 epoch: 453.1209251880646 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 2.7103 Accuracy 0.4673\n",
      "Epoch 21 Batch 50 Loss 2.6781 Accuracy 0.4550\n",
      "Epoch 21 Batch 100 Loss 2.6823 Accuracy 0.4535\n",
      "Epoch 21 Batch 150 Loss 2.6793 Accuracy 0.4536\n",
      "Epoch 21 Batch 200 Loss 2.6811 Accuracy 0.4532\n",
      "Epoch 21 Batch 250 Loss 2.6825 Accuracy 0.4533\n",
      "Epoch 21 Batch 300 Loss 2.6831 Accuracy 0.4531\n",
      "Epoch 21 Batch 350 Loss 2.6845 Accuracy 0.4529\n",
      "Epoch 21 Batch 400 Loss 2.6851 Accuracy 0.4530\n",
      "Epoch 21 Batch 450 Loss 2.6858 Accuracy 0.4530\n",
      "Epoch 21 Batch 500 Loss 2.6874 Accuracy 0.4529\n",
      "Epoch 21 Batch 550 Loss 2.6872 Accuracy 0.4530\n",
      "Epoch 21 Batch 600 Loss 2.6868 Accuracy 0.4529\n",
      "Epoch 21 Batch 650 Loss 2.6875 Accuracy 0.4530\n",
      "Epoch 21 Batch 700 Loss 2.6883 Accuracy 0.4529\n",
      "Epoch 21 Batch 750 Loss 2.6895 Accuracy 0.4528\n",
      "Epoch 21 Batch 800 Loss 2.6905 Accuracy 0.4526\n",
      "Epoch 21 Batch 850 Loss 2.6904 Accuracy 0.4527\n",
      "Epoch 21 Batch 900 Loss 2.6916 Accuracy 0.4525\n",
      "Epoch 21 Batch 950 Loss 2.6902 Accuracy 0.4527\n",
      "Epoch 21 Batch 1000 Loss 2.6897 Accuracy 0.4529\n",
      "Epoch 21 Batch 1050 Loss 2.6903 Accuracy 0.4528\n",
      "Epoch 21 Batch 1100 Loss 2.6907 Accuracy 0.4528\n",
      "Epoch 21 Batch 1150 Loss 2.6901 Accuracy 0.4530\n",
      "Epoch 21 Batch 1200 Loss 2.6906 Accuracy 0.4529\n",
      "Epoch 21 Batch 1250 Loss 2.6920 Accuracy 0.4527\n",
      "Epoch 21 Batch 1300 Loss 2.6913 Accuracy 0.4528\n",
      "Epoch 21 Batch 1350 Loss 2.6910 Accuracy 0.4528\n",
      "Epoch 21 Batch 1400 Loss 2.6912 Accuracy 0.4529\n",
      "Epoch 21 Batch 1450 Loss 2.6915 Accuracy 0.4529\n",
      "Epoch 21 Batch 1500 Loss 2.6924 Accuracy 0.4528\n",
      "Epoch 21 Batch 1550 Loss 2.6919 Accuracy 0.4528\n",
      "Epoch 21 Batch 1600 Loss 2.6919 Accuracy 0.4530\n",
      "Epoch 21 Batch 1650 Loss 2.6922 Accuracy 0.4529\n",
      "Epoch 21 Batch 1700 Loss 2.6921 Accuracy 0.4529\n",
      "Epoch 21 Batch 1750 Loss 2.6922 Accuracy 0.4530\n",
      "Epoch 21 Batch 1800 Loss 2.6925 Accuracy 0.4530\n",
      "Epoch 21 Batch 1850 Loss 2.6929 Accuracy 0.4530\n",
      "Epoch 21 Batch 1900 Loss 2.6925 Accuracy 0.4530\n",
      "Epoch 21 Batch 1950 Loss 2.6928 Accuracy 0.4530\n",
      "Epoch 21 Batch 2000 Loss 2.6935 Accuracy 0.4529\n",
      "Epoch 21 Batch 2050 Loss 2.6937 Accuracy 0.4529\n",
      "Epoch 21 Batch 2100 Loss 2.6937 Accuracy 0.4528\n",
      "Epoch 21 Batch 2150 Loss 2.6941 Accuracy 0.4528\n",
      "Epoch 21 Batch 2200 Loss 2.6941 Accuracy 0.4528\n",
      "Epoch 21 Batch 2250 Loss 2.6940 Accuracy 0.4528\n",
      "Epoch 21 Batch 2300 Loss 2.6945 Accuracy 0.4528\n",
      "Epoch 21 Batch 2350 Loss 2.6948 Accuracy 0.4527\n",
      "Epoch 21 Batch 2400 Loss 2.6950 Accuracy 0.4527\n",
      "Epoch 21 Batch 2450 Loss 2.6949 Accuracy 0.4527\n",
      "Epoch 21 Batch 2500 Loss 2.6948 Accuracy 0.4527\n",
      "Epoch 21 Batch 2550 Loss 2.6953 Accuracy 0.4527\n",
      "Epoch 21 Batch 2600 Loss 2.6955 Accuracy 0.4527\n",
      "Epoch 21 Loss 2.6955 Accuracy 0.4527\n",
      "Time taken for 1 epoch: 456.22117352485657 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss 2.5863 Accuracy 0.4493\n",
      "Epoch 22 Batch 50 Loss 2.6724 Accuracy 0.4536\n",
      "Epoch 22 Batch 100 Loss 2.6641 Accuracy 0.4559\n",
      "Epoch 22 Batch 150 Loss 2.6632 Accuracy 0.4561\n",
      "Epoch 22 Batch 200 Loss 2.6654 Accuracy 0.4554\n",
      "Epoch 22 Batch 250 Loss 2.6665 Accuracy 0.4549\n",
      "Epoch 22 Batch 300 Loss 2.6692 Accuracy 0.4546\n",
      "Epoch 22 Batch 350 Loss 2.6694 Accuracy 0.4546\n",
      "Epoch 22 Batch 400 Loss 2.6685 Accuracy 0.4551\n",
      "Epoch 22 Batch 450 Loss 2.6694 Accuracy 0.4551\n",
      "Epoch 22 Batch 500 Loss 2.6714 Accuracy 0.4548\n",
      "Epoch 22 Batch 550 Loss 2.6723 Accuracy 0.4547\n",
      "Epoch 22 Batch 600 Loss 2.6726 Accuracy 0.4549\n",
      "Epoch 22 Batch 650 Loss 2.6737 Accuracy 0.4547\n",
      "Epoch 22 Batch 700 Loss 2.6727 Accuracy 0.4550\n",
      "Epoch 22 Batch 750 Loss 2.6745 Accuracy 0.4547\n",
      "Epoch 22 Batch 800 Loss 2.6749 Accuracy 0.4546\n",
      "Epoch 22 Batch 850 Loss 2.6759 Accuracy 0.4545\n",
      "Epoch 22 Batch 900 Loss 2.6764 Accuracy 0.4544\n",
      "Epoch 22 Batch 950 Loss 2.6763 Accuracy 0.4545\n",
      "Epoch 22 Batch 1000 Loss 2.6768 Accuracy 0.4544\n",
      "Epoch 22 Batch 1050 Loss 2.6771 Accuracy 0.4544\n",
      "Epoch 22 Batch 1100 Loss 2.6764 Accuracy 0.4545\n",
      "Epoch 22 Batch 1150 Loss 2.6768 Accuracy 0.4544\n",
      "Epoch 22 Batch 1200 Loss 2.6767 Accuracy 0.4544\n",
      "Epoch 22 Batch 1250 Loss 2.6767 Accuracy 0.4544\n",
      "Epoch 22 Batch 1300 Loss 2.6770 Accuracy 0.4544\n",
      "Epoch 22 Batch 1350 Loss 2.6774 Accuracy 0.4544\n",
      "Epoch 22 Batch 1400 Loss 2.6775 Accuracy 0.4544\n",
      "Epoch 22 Batch 1450 Loss 2.6776 Accuracy 0.4544\n",
      "Epoch 22 Batch 1500 Loss 2.6776 Accuracy 0.4545\n",
      "Epoch 22 Batch 1550 Loss 2.6775 Accuracy 0.4546\n",
      "Epoch 22 Batch 1600 Loss 2.6780 Accuracy 0.4545\n",
      "Epoch 22 Batch 1650 Loss 2.6769 Accuracy 0.4547\n",
      "Epoch 22 Batch 1700 Loss 2.6768 Accuracy 0.4547\n",
      "Epoch 22 Batch 1750 Loss 2.6765 Accuracy 0.4548\n",
      "Epoch 22 Batch 1800 Loss 2.6769 Accuracy 0.4547\n",
      "Epoch 22 Batch 1850 Loss 2.6776 Accuracy 0.4546\n",
      "Epoch 22 Batch 1900 Loss 2.6777 Accuracy 0.4546\n",
      "Epoch 22 Batch 1950 Loss 2.6778 Accuracy 0.4546\n",
      "Epoch 22 Batch 2000 Loss 2.6775 Accuracy 0.4547\n",
      "Epoch 22 Batch 2050 Loss 2.6776 Accuracy 0.4547\n",
      "Epoch 22 Batch 2100 Loss 2.6780 Accuracy 0.4546\n",
      "Epoch 22 Batch 2150 Loss 2.6781 Accuracy 0.4546\n",
      "Epoch 22 Batch 2200 Loss 2.6783 Accuracy 0.4545\n",
      "Epoch 22 Batch 2250 Loss 2.6786 Accuracy 0.4545\n",
      "Epoch 22 Batch 2300 Loss 2.6789 Accuracy 0.4544\n",
      "Epoch 22 Batch 2350 Loss 2.6790 Accuracy 0.4544\n",
      "Epoch 22 Batch 2400 Loss 2.6793 Accuracy 0.4544\n",
      "Epoch 22 Batch 2450 Loss 2.6796 Accuracy 0.4544\n",
      "Epoch 22 Batch 2500 Loss 2.6798 Accuracy 0.4544\n",
      "Epoch 22 Batch 2550 Loss 2.6805 Accuracy 0.4543\n",
      "Epoch 22 Batch 2600 Loss 2.6812 Accuracy 0.4542\n",
      "Epoch 22 Loss 2.6812 Accuracy 0.4542\n",
      "Time taken for 1 epoch: 452.74843096733093 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss 2.7482 Accuracy 0.4471\n",
      "Epoch 23 Batch 50 Loss 2.6961 Accuracy 0.4502\n",
      "Epoch 23 Batch 100 Loss 2.6626 Accuracy 0.4546\n",
      "Epoch 23 Batch 150 Loss 2.6640 Accuracy 0.4545\n",
      "Epoch 23 Batch 200 Loss 2.6624 Accuracy 0.4551\n",
      "Epoch 23 Batch 250 Loss 2.6615 Accuracy 0.4554\n",
      "Epoch 23 Batch 300 Loss 2.6624 Accuracy 0.4552\n",
      "Epoch 23 Batch 350 Loss 2.6624 Accuracy 0.4557\n",
      "Epoch 23 Batch 400 Loss 2.6634 Accuracy 0.4557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Batch 450 Loss 2.6638 Accuracy 0.4556\n",
      "Epoch 23 Batch 500 Loss 2.6644 Accuracy 0.4554\n",
      "Epoch 23 Batch 550 Loss 2.6648 Accuracy 0.4556\n",
      "Epoch 23 Batch 600 Loss 2.6649 Accuracy 0.4555\n",
      "Epoch 23 Batch 650 Loss 2.6650 Accuracy 0.4556\n",
      "Epoch 23 Batch 700 Loss 2.6641 Accuracy 0.4558\n",
      "Epoch 23 Batch 750 Loss 2.6647 Accuracy 0.4559\n",
      "Epoch 23 Batch 800 Loss 2.6647 Accuracy 0.4559\n",
      "Epoch 23 Batch 850 Loss 2.6641 Accuracy 0.4559\n",
      "Epoch 23 Batch 900 Loss 2.6641 Accuracy 0.4559\n",
      "Epoch 23 Batch 950 Loss 2.6642 Accuracy 0.4559\n",
      "Epoch 23 Batch 1000 Loss 2.6653 Accuracy 0.4559\n",
      "Epoch 23 Batch 1050 Loss 2.6644 Accuracy 0.4560\n",
      "Epoch 23 Batch 1100 Loss 2.6641 Accuracy 0.4561\n",
      "Epoch 23 Batch 1150 Loss 2.6634 Accuracy 0.4562\n",
      "Epoch 23 Batch 1200 Loss 2.6641 Accuracy 0.4561\n",
      "Epoch 23 Batch 1250 Loss 2.6648 Accuracy 0.4559\n",
      "Epoch 23 Batch 1300 Loss 2.6642 Accuracy 0.4561\n",
      "Epoch 23 Batch 1350 Loss 2.6648 Accuracy 0.4560\n",
      "Epoch 23 Batch 1400 Loss 2.6654 Accuracy 0.4559\n",
      "Epoch 23 Batch 1450 Loss 2.6656 Accuracy 0.4559\n",
      "Epoch 23 Batch 1500 Loss 2.6648 Accuracy 0.4559\n",
      "Epoch 23 Batch 1550 Loss 2.6645 Accuracy 0.4560\n",
      "Epoch 23 Batch 1600 Loss 2.6642 Accuracy 0.4561\n",
      "Epoch 23 Batch 1650 Loss 2.6645 Accuracy 0.4561\n",
      "Epoch 23 Batch 1700 Loss 2.6653 Accuracy 0.4561\n",
      "Epoch 23 Batch 1750 Loss 2.6652 Accuracy 0.4561\n",
      "Epoch 23 Batch 1800 Loss 2.6650 Accuracy 0.4562\n",
      "Epoch 23 Batch 1850 Loss 2.6658 Accuracy 0.4561\n",
      "Epoch 23 Batch 1900 Loss 2.6666 Accuracy 0.4561\n",
      "Epoch 23 Batch 1950 Loss 2.6662 Accuracy 0.4562\n",
      "Epoch 23 Batch 2000 Loss 2.6659 Accuracy 0.4562\n",
      "Epoch 23 Batch 2050 Loss 2.6662 Accuracy 0.4561\n",
      "Epoch 23 Batch 2100 Loss 2.6668 Accuracy 0.4560\n",
      "Epoch 23 Batch 2150 Loss 2.6668 Accuracy 0.4560\n",
      "Epoch 23 Batch 2200 Loss 2.6670 Accuracy 0.4559\n",
      "Epoch 23 Batch 2250 Loss 2.6673 Accuracy 0.4558\n",
      "Epoch 23 Batch 2300 Loss 2.6677 Accuracy 0.4558\n",
      "Epoch 23 Batch 2350 Loss 2.6678 Accuracy 0.4557\n",
      "Epoch 23 Batch 2400 Loss 2.6678 Accuracy 0.4558\n",
      "Epoch 23 Batch 2450 Loss 2.6683 Accuracy 0.4557\n",
      "Epoch 23 Batch 2500 Loss 2.6689 Accuracy 0.4557\n",
      "Epoch 23 Batch 2550 Loss 2.6690 Accuracy 0.4557\n",
      "Epoch 23 Batch 2600 Loss 2.6691 Accuracy 0.4557\n",
      "Epoch 23 Loss 2.6691 Accuracy 0.4557\n",
      "Time taken for 1 epoch: 452.86033058166504 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss 2.7332 Accuracy 0.4361\n",
      "Epoch 24 Batch 50 Loss 2.6224 Accuracy 0.4579\n",
      "Epoch 24 Batch 100 Loss 2.6277 Accuracy 0.4595\n",
      "Epoch 24 Batch 150 Loss 2.6322 Accuracy 0.4593\n",
      "Epoch 24 Batch 200 Loss 2.6376 Accuracy 0.4590\n",
      "Epoch 24 Batch 250 Loss 2.6387 Accuracy 0.4588\n",
      "Epoch 24 Batch 300 Loss 2.6422 Accuracy 0.4585\n",
      "Epoch 24 Batch 350 Loss 2.6451 Accuracy 0.4580\n",
      "Epoch 24 Batch 400 Loss 2.6456 Accuracy 0.4580\n",
      "Epoch 24 Batch 450 Loss 2.6480 Accuracy 0.4576\n",
      "Epoch 24 Batch 500 Loss 2.6464 Accuracy 0.4581\n",
      "Epoch 24 Batch 550 Loss 2.6495 Accuracy 0.4578\n",
      "Epoch 24 Batch 600 Loss 2.6494 Accuracy 0.4580\n",
      "Epoch 24 Batch 650 Loss 2.6495 Accuracy 0.4580\n",
      "Epoch 24 Batch 700 Loss 2.6500 Accuracy 0.4578\n",
      "Epoch 24 Batch 750 Loss 2.6497 Accuracy 0.4578\n",
      "Epoch 24 Batch 800 Loss 2.6500 Accuracy 0.4576\n",
      "Epoch 24 Batch 850 Loss 2.6502 Accuracy 0.4576\n",
      "Epoch 24 Batch 900 Loss 2.6516 Accuracy 0.4575\n",
      "Epoch 24 Batch 950 Loss 2.6514 Accuracy 0.4575\n",
      "Epoch 24 Batch 1000 Loss 2.6510 Accuracy 0.4576\n",
      "Epoch 24 Batch 1050 Loss 2.6513 Accuracy 0.4577\n",
      "Epoch 24 Batch 1100 Loss 2.6511 Accuracy 0.4577\n",
      "Epoch 24 Batch 1150 Loss 2.6519 Accuracy 0.4576\n",
      "Epoch 24 Batch 1200 Loss 2.6518 Accuracy 0.4576\n",
      "Epoch 24 Batch 1250 Loss 2.6523 Accuracy 0.4576\n",
      "Epoch 24 Batch 1300 Loss 2.6521 Accuracy 0.4576\n",
      "Epoch 24 Batch 1350 Loss 2.6527 Accuracy 0.4575\n",
      "Epoch 24 Batch 1400 Loss 2.6533 Accuracy 0.4575\n",
      "Epoch 24 Batch 1450 Loss 2.6534 Accuracy 0.4576\n",
      "Epoch 24 Batch 1500 Loss 2.6532 Accuracy 0.4576\n",
      "Epoch 24 Batch 1550 Loss 2.6539 Accuracy 0.4575\n",
      "Epoch 24 Batch 1600 Loss 2.6540 Accuracy 0.4575\n",
      "Epoch 24 Batch 1650 Loss 2.6541 Accuracy 0.4575\n",
      "Epoch 24 Batch 1700 Loss 2.6535 Accuracy 0.4576\n",
      "Epoch 24 Batch 1750 Loss 2.6536 Accuracy 0.4576\n",
      "Epoch 24 Batch 1800 Loss 2.6532 Accuracy 0.4578\n",
      "Epoch 24 Batch 1850 Loss 2.6528 Accuracy 0.4579\n",
      "Epoch 24 Batch 1900 Loss 2.6524 Accuracy 0.4579\n",
      "Epoch 24 Batch 1950 Loss 2.6531 Accuracy 0.4578\n",
      "Epoch 24 Batch 2000 Loss 2.6534 Accuracy 0.4577\n",
      "Epoch 24 Batch 2050 Loss 2.6539 Accuracy 0.4577\n",
      "Epoch 24 Batch 2100 Loss 2.6542 Accuracy 0.4577\n",
      "Epoch 24 Batch 2150 Loss 2.6546 Accuracy 0.4576\n",
      "Epoch 24 Batch 2200 Loss 2.6550 Accuracy 0.4576\n",
      "Epoch 24 Batch 2250 Loss 2.6552 Accuracy 0.4575\n",
      "Epoch 24 Batch 2300 Loss 2.6555 Accuracy 0.4575\n",
      "Epoch 24 Batch 2350 Loss 2.6555 Accuracy 0.4574\n",
      "Epoch 24 Batch 2400 Loss 2.6559 Accuracy 0.4574\n",
      "Epoch 24 Batch 2450 Loss 2.6558 Accuracy 0.4574\n",
      "Epoch 24 Batch 2500 Loss 2.6558 Accuracy 0.4575\n",
      "Epoch 24 Batch 2550 Loss 2.6563 Accuracy 0.4574\n",
      "Epoch 24 Batch 2600 Loss 2.6567 Accuracy 0.4574\n",
      "Epoch 24 Loss 2.6568 Accuracy 0.4573\n",
      "Time taken for 1 epoch: 452.66763496398926 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss 2.5462 Accuracy 0.4648\n",
      "Epoch 25 Batch 50 Loss 2.6370 Accuracy 0.4586\n",
      "Epoch 25 Batch 100 Loss 2.6380 Accuracy 0.4589\n",
      "Epoch 25 Batch 150 Loss 2.6320 Accuracy 0.4594\n",
      "Epoch 25 Batch 200 Loss 2.6350 Accuracy 0.4590\n",
      "Epoch 25 Batch 250 Loss 2.6340 Accuracy 0.4589\n",
      "Epoch 25 Batch 300 Loss 2.6338 Accuracy 0.4590\n",
      "Epoch 25 Batch 350 Loss 2.6330 Accuracy 0.4592\n",
      "Epoch 25 Batch 400 Loss 2.6358 Accuracy 0.4590\n",
      "Epoch 25 Batch 450 Loss 2.6333 Accuracy 0.4596\n",
      "Epoch 25 Batch 500 Loss 2.6339 Accuracy 0.4594\n",
      "Epoch 25 Batch 550 Loss 2.6339 Accuracy 0.4593\n",
      "Epoch 25 Batch 600 Loss 2.6344 Accuracy 0.4594\n",
      "Epoch 25 Batch 650 Loss 2.6346 Accuracy 0.4594\n",
      "Epoch 25 Batch 700 Loss 2.6364 Accuracy 0.4592\n",
      "Epoch 25 Batch 750 Loss 2.6359 Accuracy 0.4592\n",
      "Epoch 25 Batch 800 Loss 2.6375 Accuracy 0.4589\n",
      "Epoch 25 Batch 850 Loss 2.6382 Accuracy 0.4589\n",
      "Epoch 25 Batch 900 Loss 2.6381 Accuracy 0.4589\n",
      "Epoch 25 Batch 950 Loss 2.6388 Accuracy 0.4587\n",
      "Epoch 25 Batch 1000 Loss 2.6387 Accuracy 0.4587\n",
      "Epoch 25 Batch 1050 Loss 2.6388 Accuracy 0.4588\n",
      "Epoch 25 Batch 1100 Loss 2.6383 Accuracy 0.4588\n",
      "Epoch 25 Batch 1150 Loss 2.6387 Accuracy 0.4589\n",
      "Epoch 25 Batch 1200 Loss 2.6400 Accuracy 0.4587\n",
      "Epoch 25 Batch 1250 Loss 2.6394 Accuracy 0.4588\n",
      "Epoch 25 Batch 1300 Loss 2.6395 Accuracy 0.4588\n",
      "Epoch 25 Batch 1350 Loss 2.6394 Accuracy 0.4589\n",
      "Epoch 25 Batch 1400 Loss 2.6395 Accuracy 0.4589\n",
      "Epoch 25 Batch 1450 Loss 2.6407 Accuracy 0.4587\n",
      "Epoch 25 Batch 1500 Loss 2.6408 Accuracy 0.4588\n",
      "Epoch 25 Batch 1550 Loss 2.6414 Accuracy 0.4587\n",
      "Epoch 25 Batch 1600 Loss 2.6419 Accuracy 0.4586\n",
      "Epoch 25 Batch 1650 Loss 2.6422 Accuracy 0.4586\n",
      "Epoch 25 Batch 1700 Loss 2.6423 Accuracy 0.4587\n",
      "Epoch 25 Batch 1750 Loss 2.6425 Accuracy 0.4587\n",
      "Epoch 25 Batch 1800 Loss 2.6426 Accuracy 0.4587\n",
      "Epoch 25 Batch 1850 Loss 2.6431 Accuracy 0.4586\n",
      "Epoch 25 Batch 1900 Loss 2.6428 Accuracy 0.4587\n",
      "Epoch 25 Batch 1950 Loss 2.6429 Accuracy 0.4586\n",
      "Epoch 25 Batch 2000 Loss 2.6426 Accuracy 0.4587\n",
      "Epoch 25 Batch 2050 Loss 2.6429 Accuracy 0.4586\n",
      "Epoch 25 Batch 2100 Loss 2.6437 Accuracy 0.4585\n",
      "Epoch 25 Batch 2150 Loss 2.6441 Accuracy 0.4585\n",
      "Epoch 25 Batch 2200 Loss 2.6447 Accuracy 0.4584\n",
      "Epoch 25 Batch 2250 Loss 2.6447 Accuracy 0.4584\n",
      "Epoch 25 Batch 2300 Loss 2.6445 Accuracy 0.4585\n",
      "Epoch 25 Batch 2350 Loss 2.6443 Accuracy 0.4585\n",
      "Epoch 25 Batch 2400 Loss 2.6443 Accuracy 0.4585\n",
      "Epoch 25 Batch 2450 Loss 2.6447 Accuracy 0.4585\n",
      "Epoch 25 Batch 2500 Loss 2.6448 Accuracy 0.4586\n",
      "Epoch 25 Batch 2550 Loss 2.6452 Accuracy 0.4585\n",
      "Epoch 25 Batch 2600 Loss 2.6458 Accuracy 0.4584\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/train_full/ckpt-7\n",
      "Epoch 25 Loss 2.6459 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 453.9821090698242 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss 2.4667 Accuracy 0.4723\n",
      "Epoch 26 Batch 50 Loss 2.6253 Accuracy 0.4593\n",
      "Epoch 26 Batch 100 Loss 2.6133 Accuracy 0.4614\n",
      "Epoch 26 Batch 150 Loss 2.6184 Accuracy 0.4599\n",
      "Epoch 26 Batch 200 Loss 2.6240 Accuracy 0.4591\n",
      "Epoch 26 Batch 250 Loss 2.6251 Accuracy 0.4596\n",
      "Epoch 26 Batch 300 Loss 2.6225 Accuracy 0.4600\n",
      "Epoch 26 Batch 350 Loss 2.6245 Accuracy 0.4599\n",
      "Epoch 26 Batch 400 Loss 2.6276 Accuracy 0.4595\n",
      "Epoch 26 Batch 450 Loss 2.6267 Accuracy 0.4599\n",
      "Epoch 26 Batch 500 Loss 2.6274 Accuracy 0.4599\n",
      "Epoch 26 Batch 550 Loss 2.6279 Accuracy 0.4598\n",
      "Epoch 26 Batch 600 Loss 2.6267 Accuracy 0.4601\n",
      "Epoch 26 Batch 650 Loss 2.6270 Accuracy 0.4602\n",
      "Epoch 26 Batch 700 Loss 2.6272 Accuracy 0.4602\n",
      "Epoch 26 Batch 750 Loss 2.6285 Accuracy 0.4602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Batch 800 Loss 2.6265 Accuracy 0.4604\n",
      "Epoch 26 Batch 850 Loss 2.6259 Accuracy 0.4607\n",
      "Epoch 26 Batch 900 Loss 2.6263 Accuracy 0.4607\n",
      "Epoch 26 Batch 950 Loss 2.6266 Accuracy 0.4607\n",
      "Epoch 26 Batch 1000 Loss 2.6268 Accuracy 0.4607\n",
      "Epoch 26 Batch 1050 Loss 2.6269 Accuracy 0.4608\n",
      "Epoch 26 Batch 1100 Loss 2.6265 Accuracy 0.4609\n",
      "Epoch 26 Batch 1150 Loss 2.6276 Accuracy 0.4607\n",
      "Epoch 26 Batch 1200 Loss 2.6277 Accuracy 0.4606\n",
      "Epoch 26 Batch 1250 Loss 2.6277 Accuracy 0.4606\n",
      "Epoch 26 Batch 1300 Loss 2.6277 Accuracy 0.4606\n",
      "Epoch 26 Batch 1350 Loss 2.6276 Accuracy 0.4606\n",
      "Epoch 26 Batch 1400 Loss 2.6283 Accuracy 0.4605\n",
      "Epoch 26 Batch 1450 Loss 2.6281 Accuracy 0.4606\n",
      "Epoch 26 Batch 1500 Loss 2.6276 Accuracy 0.4607\n",
      "Epoch 26 Batch 1550 Loss 2.6280 Accuracy 0.4607\n",
      "Epoch 26 Batch 1600 Loss 2.6286 Accuracy 0.4607\n",
      "Epoch 26 Batch 1650 Loss 2.6294 Accuracy 0.4606\n",
      "Epoch 26 Batch 1700 Loss 2.6290 Accuracy 0.4607\n",
      "Epoch 26 Batch 1750 Loss 2.6299 Accuracy 0.4606\n",
      "Epoch 26 Batch 1800 Loss 2.6296 Accuracy 0.4606\n",
      "Epoch 26 Batch 1850 Loss 2.6300 Accuracy 0.4605\n",
      "Epoch 26 Batch 1900 Loss 2.6302 Accuracy 0.4604\n",
      "Epoch 26 Batch 1950 Loss 2.6304 Accuracy 0.4604\n",
      "Epoch 26 Batch 2000 Loss 2.6306 Accuracy 0.4603\n",
      "Epoch 26 Batch 2050 Loss 2.6303 Accuracy 0.4604\n",
      "Epoch 26 Batch 2100 Loss 2.6303 Accuracy 0.4604\n",
      "Epoch 26 Batch 2150 Loss 2.6302 Accuracy 0.4604\n",
      "Epoch 26 Batch 2200 Loss 2.6309 Accuracy 0.4604\n",
      "Epoch 26 Batch 2250 Loss 2.6310 Accuracy 0.4604\n",
      "Epoch 26 Batch 2300 Loss 2.6311 Accuracy 0.4604\n",
      "Epoch 26 Batch 2350 Loss 2.6312 Accuracy 0.4604\n",
      "Epoch 26 Batch 2400 Loss 2.6313 Accuracy 0.4603\n",
      "Epoch 26 Batch 2450 Loss 2.6316 Accuracy 0.4604\n",
      "Epoch 26 Batch 2500 Loss 2.6323 Accuracy 0.4602\n",
      "Epoch 26 Batch 2550 Loss 2.6327 Accuracy 0.4602\n",
      "Epoch 26 Batch 2600 Loss 2.6332 Accuracy 0.4601\n",
      "Epoch 26 Loss 2.6335 Accuracy 0.4601\n",
      "Time taken for 1 epoch: 456.0143265724182 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss 2.6907 Accuracy 0.4606\n",
      "Epoch 27 Batch 50 Loss 2.5810 Accuracy 0.4679\n",
      "Epoch 27 Batch 100 Loss 2.5991 Accuracy 0.4638\n",
      "Epoch 27 Batch 150 Loss 2.6054 Accuracy 0.4625\n",
      "Epoch 27 Batch 200 Loss 2.6089 Accuracy 0.4624\n",
      "Epoch 27 Batch 250 Loss 2.6066 Accuracy 0.4626\n",
      "Epoch 27 Batch 300 Loss 2.6036 Accuracy 0.4630\n",
      "Epoch 27 Batch 350 Loss 2.6081 Accuracy 0.4622\n",
      "Epoch 27 Batch 400 Loss 2.6092 Accuracy 0.4622\n",
      "Epoch 27 Batch 450 Loss 2.6111 Accuracy 0.4620\n",
      "Epoch 27 Batch 500 Loss 2.6103 Accuracy 0.4625\n",
      "Epoch 27 Batch 550 Loss 2.6111 Accuracy 0.4623\n",
      "Epoch 27 Batch 600 Loss 2.6117 Accuracy 0.4622\n",
      "Epoch 27 Batch 650 Loss 2.6144 Accuracy 0.4619\n",
      "Epoch 27 Batch 700 Loss 2.6139 Accuracy 0.4619\n",
      "Epoch 27 Batch 750 Loss 2.6132 Accuracy 0.4621\n",
      "Epoch 27 Batch 800 Loss 2.6141 Accuracy 0.4619\n",
      "Epoch 27 Batch 850 Loss 2.6142 Accuracy 0.4620\n",
      "Epoch 27 Batch 900 Loss 2.6146 Accuracy 0.4619\n",
      "Epoch 27 Batch 950 Loss 2.6147 Accuracy 0.4620\n",
      "Epoch 27 Batch 1000 Loss 2.6152 Accuracy 0.4619\n",
      "Epoch 27 Batch 1050 Loss 2.6154 Accuracy 0.4619\n",
      "Epoch 27 Batch 1100 Loss 2.6162 Accuracy 0.4618\n",
      "Epoch 27 Batch 1150 Loss 2.6166 Accuracy 0.4619\n",
      "Epoch 27 Batch 1200 Loss 2.6164 Accuracy 0.4621\n",
      "Epoch 27 Batch 1250 Loss 2.6159 Accuracy 0.4621\n",
      "Epoch 27 Batch 1300 Loss 2.6164 Accuracy 0.4621\n",
      "Epoch 27 Batch 1350 Loss 2.6173 Accuracy 0.4621\n",
      "Epoch 27 Batch 1400 Loss 2.6172 Accuracy 0.4621\n",
      "Epoch 27 Batch 1450 Loss 2.6168 Accuracy 0.4622\n",
      "Epoch 27 Batch 1500 Loss 2.6169 Accuracy 0.4624\n",
      "Epoch 27 Batch 1550 Loss 2.6170 Accuracy 0.4623\n",
      "Epoch 27 Batch 1600 Loss 2.6175 Accuracy 0.4622\n",
      "Epoch 27 Batch 1650 Loss 2.6175 Accuracy 0.4622\n",
      "Epoch 27 Batch 1700 Loss 2.6176 Accuracy 0.4622\n",
      "Epoch 27 Batch 1750 Loss 2.6176 Accuracy 0.4621\n",
      "Epoch 27 Batch 1800 Loss 2.6179 Accuracy 0.4621\n",
      "Epoch 27 Batch 1850 Loss 2.6182 Accuracy 0.4620\n",
      "Epoch 27 Batch 1900 Loss 2.6183 Accuracy 0.4620\n",
      "Epoch 27 Batch 1950 Loss 2.6191 Accuracy 0.4618\n",
      "Epoch 27 Batch 2000 Loss 2.6191 Accuracy 0.4618\n",
      "Epoch 27 Batch 2050 Loss 2.6197 Accuracy 0.4618\n",
      "Epoch 27 Batch 2100 Loss 2.6194 Accuracy 0.4618\n",
      "Epoch 27 Batch 2150 Loss 2.6196 Accuracy 0.4618\n",
      "Epoch 27 Batch 2200 Loss 2.6204 Accuracy 0.4617\n",
      "Epoch 27 Batch 2250 Loss 2.6204 Accuracy 0.4618\n",
      "Epoch 27 Batch 2300 Loss 2.6205 Accuracy 0.4617\n",
      "Epoch 27 Batch 2350 Loss 2.6210 Accuracy 0.4617\n",
      "Epoch 27 Batch 2400 Loss 2.6207 Accuracy 0.4617\n",
      "Epoch 27 Batch 2450 Loss 2.6209 Accuracy 0.4617\n",
      "Epoch 27 Batch 2500 Loss 2.6212 Accuracy 0.4616\n",
      "Epoch 27 Batch 2550 Loss 2.6217 Accuracy 0.4616\n",
      "Epoch 27 Batch 2600 Loss 2.6223 Accuracy 0.4615\n",
      "Epoch 27 Loss 2.6225 Accuracy 0.4615\n",
      "Time taken for 1 epoch: 453.05429220199585 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss 2.5346 Accuracy 0.4739\n",
      "Epoch 28 Batch 50 Loss 2.6001 Accuracy 0.4668\n",
      "Epoch 28 Batch 100 Loss 2.5911 Accuracy 0.4657\n",
      "Epoch 28 Batch 150 Loss 2.5953 Accuracy 0.4642\n",
      "Epoch 28 Batch 200 Loss 2.5944 Accuracy 0.4643\n",
      "Epoch 28 Batch 250 Loss 2.5935 Accuracy 0.4642\n",
      "Epoch 28 Batch 300 Loss 2.5946 Accuracy 0.4642\n",
      "Epoch 28 Batch 350 Loss 2.5964 Accuracy 0.4640\n",
      "Epoch 28 Batch 400 Loss 2.5994 Accuracy 0.4635\n",
      "Epoch 28 Batch 450 Loss 2.6021 Accuracy 0.4631\n",
      "Epoch 28 Batch 500 Loss 2.6017 Accuracy 0.4631\n",
      "Epoch 28 Batch 550 Loss 2.6020 Accuracy 0.4631\n",
      "Epoch 28 Batch 600 Loss 2.6013 Accuracy 0.4633\n",
      "Epoch 28 Batch 650 Loss 2.6015 Accuracy 0.4633\n",
      "Epoch 28 Batch 700 Loss 2.6019 Accuracy 0.4634\n",
      "Epoch 28 Batch 750 Loss 2.6031 Accuracy 0.4632\n",
      "Epoch 28 Batch 800 Loss 2.6032 Accuracy 0.4633\n",
      "Epoch 28 Batch 850 Loss 2.6041 Accuracy 0.4632\n",
      "Epoch 28 Batch 900 Loss 2.6035 Accuracy 0.4633\n",
      "Epoch 28 Batch 950 Loss 2.6038 Accuracy 0.4634\n",
      "Epoch 28 Batch 1000 Loss 2.6040 Accuracy 0.4634\n",
      "Epoch 28 Batch 1050 Loss 2.6042 Accuracy 0.4635\n",
      "Epoch 28 Batch 1100 Loss 2.6053 Accuracy 0.4634\n",
      "Epoch 28 Batch 1150 Loss 2.6054 Accuracy 0.4634\n",
      "Epoch 28 Batch 1200 Loss 2.6038 Accuracy 0.4637\n",
      "Epoch 28 Batch 1250 Loss 2.6042 Accuracy 0.4637\n",
      "Epoch 28 Batch 1300 Loss 2.6045 Accuracy 0.4637\n",
      "Epoch 28 Batch 1350 Loss 2.6046 Accuracy 0.4637\n",
      "Epoch 28 Batch 1400 Loss 2.6049 Accuracy 0.4637\n",
      "Epoch 28 Batch 1450 Loss 2.6057 Accuracy 0.4636\n",
      "Epoch 28 Batch 1500 Loss 2.6065 Accuracy 0.4635\n",
      "Epoch 28 Batch 1550 Loss 2.6066 Accuracy 0.4634\n",
      "Epoch 28 Batch 1600 Loss 2.6070 Accuracy 0.4635\n",
      "Epoch 28 Batch 1650 Loss 2.6067 Accuracy 0.4635\n",
      "Epoch 28 Batch 1700 Loss 2.6070 Accuracy 0.4634\n",
      "Epoch 28 Batch 1750 Loss 2.6067 Accuracy 0.4635\n",
      "Epoch 28 Batch 1800 Loss 2.6072 Accuracy 0.4634\n",
      "Epoch 28 Batch 1850 Loss 2.6078 Accuracy 0.4633\n",
      "Epoch 28 Batch 1900 Loss 2.6077 Accuracy 0.4633\n",
      "Epoch 28 Batch 1950 Loss 2.6073 Accuracy 0.4634\n",
      "Epoch 28 Batch 2000 Loss 2.6075 Accuracy 0.4634\n",
      "Epoch 28 Batch 2050 Loss 2.6077 Accuracy 0.4634\n",
      "Epoch 28 Batch 2100 Loss 2.6085 Accuracy 0.4633\n",
      "Epoch 28 Batch 2150 Loss 2.6084 Accuracy 0.4633\n",
      "Epoch 28 Batch 2200 Loss 2.6083 Accuracy 0.4634\n",
      "Epoch 28 Batch 2250 Loss 2.6087 Accuracy 0.4633\n",
      "Epoch 28 Batch 2300 Loss 2.6089 Accuracy 0.4632\n",
      "Epoch 28 Batch 2350 Loss 2.6090 Accuracy 0.4632\n",
      "Epoch 28 Batch 2400 Loss 2.6094 Accuracy 0.4631\n",
      "Epoch 28 Batch 2450 Loss 2.6100 Accuracy 0.4630\n",
      "Epoch 28 Batch 2500 Loss 2.6103 Accuracy 0.4629\n",
      "Epoch 28 Batch 2550 Loss 2.6105 Accuracy 0.4630\n",
      "Epoch 28 Batch 2600 Loss 2.6108 Accuracy 0.4630\n",
      "Epoch 28 Loss 2.6111 Accuracy 0.4630\n",
      "Time taken for 1 epoch: 452.7905056476593 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss 2.5165 Accuracy 0.4883\n",
      "Epoch 29 Batch 50 Loss 2.5524 Accuracy 0.4703\n",
      "Epoch 29 Batch 100 Loss 2.5740 Accuracy 0.4660\n",
      "Epoch 29 Batch 150 Loss 2.5687 Accuracy 0.4670\n",
      "Epoch 29 Batch 200 Loss 2.5793 Accuracy 0.4655\n",
      "Epoch 29 Batch 250 Loss 2.5816 Accuracy 0.4654\n",
      "Epoch 29 Batch 300 Loss 2.5841 Accuracy 0.4654\n",
      "Epoch 29 Batch 350 Loss 2.5839 Accuracy 0.4656\n",
      "Epoch 29 Batch 400 Loss 2.5871 Accuracy 0.4654\n",
      "Epoch 29 Batch 450 Loss 2.5844 Accuracy 0.4660\n",
      "Epoch 29 Batch 500 Loss 2.5848 Accuracy 0.4657\n",
      "Epoch 29 Batch 550 Loss 2.5872 Accuracy 0.4655\n",
      "Epoch 29 Batch 600 Loss 2.5871 Accuracy 0.4656\n",
      "Epoch 29 Batch 650 Loss 2.5868 Accuracy 0.4656\n",
      "Epoch 29 Batch 700 Loss 2.5871 Accuracy 0.4653\n",
      "Epoch 29 Batch 750 Loss 2.5893 Accuracy 0.4650\n",
      "Epoch 29 Batch 800 Loss 2.5904 Accuracy 0.4648\n",
      "Epoch 29 Batch 850 Loss 2.5914 Accuracy 0.4646\n",
      "Epoch 29 Batch 900 Loss 2.5909 Accuracy 0.4649\n",
      "Epoch 29 Batch 950 Loss 2.5905 Accuracy 0.4649\n",
      "Epoch 29 Batch 1000 Loss 2.5910 Accuracy 0.4650\n",
      "Epoch 29 Batch 1050 Loss 2.5920 Accuracy 0.4648\n",
      "Epoch 29 Batch 1100 Loss 2.5914 Accuracy 0.4649\n",
      "Epoch 29 Batch 1150 Loss 2.5921 Accuracy 0.4647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Batch 1200 Loss 2.5909 Accuracy 0.4650\n",
      "Epoch 29 Batch 1250 Loss 2.5918 Accuracy 0.4648\n",
      "Epoch 29 Batch 1300 Loss 2.5923 Accuracy 0.4648\n",
      "Epoch 29 Batch 1350 Loss 2.5928 Accuracy 0.4647\n",
      "Epoch 29 Batch 1400 Loss 2.5935 Accuracy 0.4646\n",
      "Epoch 29 Batch 1450 Loss 2.5941 Accuracy 0.4646\n",
      "Epoch 29 Batch 1500 Loss 2.5947 Accuracy 0.4645\n",
      "Epoch 29 Batch 1550 Loss 2.5943 Accuracy 0.4645\n",
      "Epoch 29 Batch 1600 Loss 2.5950 Accuracy 0.4646\n",
      "Epoch 29 Batch 1650 Loss 2.5948 Accuracy 0.4647\n",
      "Epoch 29 Batch 1700 Loss 2.5953 Accuracy 0.4647\n",
      "Epoch 29 Batch 1750 Loss 2.5952 Accuracy 0.4647\n",
      "Epoch 29 Batch 1800 Loss 2.5956 Accuracy 0.4647\n",
      "Epoch 29 Batch 1850 Loss 2.5958 Accuracy 0.4646\n",
      "Epoch 29 Batch 1900 Loss 2.5962 Accuracy 0.4646\n",
      "Epoch 29 Batch 1950 Loss 2.5964 Accuracy 0.4645\n",
      "Epoch 29 Batch 2000 Loss 2.5959 Accuracy 0.4646\n",
      "Epoch 29 Batch 2050 Loss 2.5957 Accuracy 0.4647\n",
      "Epoch 29 Batch 2100 Loss 2.5964 Accuracy 0.4646\n",
      "Epoch 29 Batch 2150 Loss 2.5966 Accuracy 0.4646\n",
      "Epoch 29 Batch 2200 Loss 2.5971 Accuracy 0.4646\n",
      "Epoch 29 Batch 2250 Loss 2.5975 Accuracy 0.4645\n",
      "Epoch 29 Batch 2300 Loss 2.5971 Accuracy 0.4646\n",
      "Epoch 29 Batch 2350 Loss 2.5975 Accuracy 0.4646\n",
      "Epoch 29 Batch 2400 Loss 2.5979 Accuracy 0.4645\n",
      "Epoch 29 Batch 2450 Loss 2.5977 Accuracy 0.4645\n",
      "Epoch 29 Batch 2500 Loss 2.5981 Accuracy 0.4645\n",
      "Epoch 29 Batch 2550 Loss 2.5984 Accuracy 0.4644\n",
      "Epoch 29 Batch 2600 Loss 2.5991 Accuracy 0.4643\n",
      "Epoch 29 Loss 2.5994 Accuracy 0.4643\n",
      "Time taken for 1 epoch: 452.78012704849243 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss 2.6417 Accuracy 0.4405\n",
      "Epoch 30 Batch 50 Loss 2.5977 Accuracy 0.4625\n",
      "Epoch 30 Batch 100 Loss 2.5848 Accuracy 0.4637\n",
      "Epoch 30 Batch 150 Loss 2.5795 Accuracy 0.4652\n",
      "Epoch 30 Batch 200 Loss 2.5749 Accuracy 0.4658\n",
      "Epoch 30 Batch 250 Loss 2.5729 Accuracy 0.4667\n",
      "Epoch 30 Batch 300 Loss 2.5777 Accuracy 0.4662\n",
      "Epoch 30 Batch 350 Loss 2.5736 Accuracy 0.4669\n",
      "Epoch 30 Batch 400 Loss 2.5745 Accuracy 0.4665\n",
      "Epoch 30 Batch 450 Loss 2.5771 Accuracy 0.4663\n",
      "Epoch 30 Batch 500 Loss 2.5782 Accuracy 0.4660\n",
      "Epoch 30 Batch 550 Loss 2.5775 Accuracy 0.4661\n",
      "Epoch 30 Batch 600 Loss 2.5792 Accuracy 0.4659\n",
      "Epoch 30 Batch 650 Loss 2.5789 Accuracy 0.4658\n",
      "Epoch 30 Batch 700 Loss 2.5794 Accuracy 0.4657\n",
      "Epoch 30 Batch 750 Loss 2.5809 Accuracy 0.4655\n",
      "Epoch 30 Batch 800 Loss 2.5794 Accuracy 0.4659\n",
      "Epoch 30 Batch 850 Loss 2.5803 Accuracy 0.4657\n",
      "Epoch 30 Batch 900 Loss 2.5804 Accuracy 0.4657\n",
      "Epoch 30 Batch 950 Loss 2.5809 Accuracy 0.4657\n",
      "Epoch 30 Batch 1000 Loss 2.5825 Accuracy 0.4656\n",
      "Epoch 30 Batch 1050 Loss 2.5830 Accuracy 0.4656\n",
      "Epoch 30 Batch 1100 Loss 2.5832 Accuracy 0.4656\n",
      "Epoch 30 Batch 1150 Loss 2.5834 Accuracy 0.4656\n",
      "Epoch 30 Batch 1200 Loss 2.5837 Accuracy 0.4656\n",
      "Epoch 30 Batch 1250 Loss 2.5841 Accuracy 0.4656\n",
      "Epoch 30 Batch 1300 Loss 2.5833 Accuracy 0.4656\n",
      "Epoch 30 Batch 1350 Loss 2.5840 Accuracy 0.4655\n",
      "Epoch 30 Batch 1400 Loss 2.5839 Accuracy 0.4655\n",
      "Epoch 30 Batch 1450 Loss 2.5837 Accuracy 0.4657\n",
      "Epoch 30 Batch 1500 Loss 2.5842 Accuracy 0.4656\n",
      "Epoch 30 Batch 1550 Loss 2.5843 Accuracy 0.4656\n",
      "Epoch 30 Batch 1600 Loss 2.5842 Accuracy 0.4656\n",
      "Epoch 30 Batch 1650 Loss 2.5839 Accuracy 0.4657\n",
      "Epoch 30 Batch 1700 Loss 2.5840 Accuracy 0.4658\n",
      "Epoch 30 Batch 1750 Loss 2.5840 Accuracy 0.4658\n",
      "Epoch 30 Batch 1800 Loss 2.5845 Accuracy 0.4658\n",
      "Epoch 30 Batch 1850 Loss 2.5848 Accuracy 0.4658\n",
      "Epoch 30 Batch 1900 Loss 2.5848 Accuracy 0.4658\n",
      "Epoch 30 Batch 1950 Loss 2.5853 Accuracy 0.4657\n",
      "Epoch 30 Batch 2000 Loss 2.5857 Accuracy 0.4657\n",
      "Epoch 30 Batch 2050 Loss 2.5862 Accuracy 0.4656\n",
      "Epoch 30 Batch 2100 Loss 2.5858 Accuracy 0.4656\n",
      "Epoch 30 Batch 2150 Loss 2.5862 Accuracy 0.4656\n",
      "Epoch 30 Batch 2200 Loss 2.5864 Accuracy 0.4656\n",
      "Epoch 30 Batch 2250 Loss 2.5860 Accuracy 0.4657\n",
      "Epoch 30 Batch 2300 Loss 2.5864 Accuracy 0.4657\n",
      "Epoch 30 Batch 2350 Loss 2.5868 Accuracy 0.4657\n",
      "Epoch 30 Batch 2400 Loss 2.5872 Accuracy 0.4656\n",
      "Epoch 30 Batch 2450 Loss 2.5877 Accuracy 0.4655\n",
      "Epoch 30 Batch 2500 Loss 2.5882 Accuracy 0.4654\n",
      "Epoch 30 Batch 2550 Loss 2.5887 Accuracy 0.4654\n",
      "Epoch 30 Batch 2600 Loss 2.5891 Accuracy 0.4654\n",
      "Saving checkpoint for epoch 30 at ./checkpoints/train_full/ckpt-8\n",
      "Epoch 30 Loss 2.5891 Accuracy 0.4654\n",
      "Time taken for 1 epoch: 455.59667348861694 secs\n",
      "\n",
      "Epoch 31 Batch 0 Loss 2.5399 Accuracy 0.4683\n",
      "Epoch 31 Batch 50 Loss 2.5662 Accuracy 0.4650\n",
      "Epoch 31 Batch 100 Loss 2.5608 Accuracy 0.4685\n",
      "Epoch 31 Batch 150 Loss 2.5590 Accuracy 0.4689\n",
      "Epoch 31 Batch 200 Loss 2.5625 Accuracy 0.4684\n",
      "Epoch 31 Batch 250 Loss 2.5667 Accuracy 0.4676\n",
      "Epoch 31 Batch 300 Loss 2.5662 Accuracy 0.4676\n",
      "Epoch 31 Batch 350 Loss 2.5655 Accuracy 0.4681\n",
      "Epoch 31 Batch 400 Loss 2.5651 Accuracy 0.4682\n",
      "Epoch 31 Batch 450 Loss 2.5655 Accuracy 0.4682\n",
      "Epoch 31 Batch 500 Loss 2.5655 Accuracy 0.4681\n",
      "Epoch 31 Batch 550 Loss 2.5669 Accuracy 0.4678\n",
      "Epoch 31 Batch 600 Loss 2.5672 Accuracy 0.4678\n",
      "Epoch 31 Batch 650 Loss 2.5692 Accuracy 0.4675\n",
      "Epoch 31 Batch 700 Loss 2.5698 Accuracy 0.4673\n",
      "Epoch 31 Batch 750 Loss 2.5694 Accuracy 0.4674\n",
      "Epoch 31 Batch 800 Loss 2.5700 Accuracy 0.4673\n",
      "Epoch 31 Batch 850 Loss 2.5713 Accuracy 0.4672\n",
      "Epoch 31 Batch 900 Loss 2.5722 Accuracy 0.4672\n",
      "Epoch 31 Batch 950 Loss 2.5723 Accuracy 0.4672\n",
      "Epoch 31 Batch 1000 Loss 2.5717 Accuracy 0.4674\n",
      "Epoch 31 Batch 1050 Loss 2.5711 Accuracy 0.4675\n",
      "Epoch 31 Batch 1100 Loss 2.5713 Accuracy 0.4675\n",
      "Epoch 31 Batch 1150 Loss 2.5714 Accuracy 0.4675\n",
      "Epoch 31 Batch 1200 Loss 2.5706 Accuracy 0.4676\n",
      "Epoch 31 Batch 1250 Loss 2.5714 Accuracy 0.4675\n",
      "Epoch 31 Batch 1300 Loss 2.5713 Accuracy 0.4675\n",
      "Epoch 31 Batch 1350 Loss 2.5715 Accuracy 0.4675\n",
      "Epoch 31 Batch 1400 Loss 2.5718 Accuracy 0.4675\n",
      "Epoch 31 Batch 1450 Loss 2.5723 Accuracy 0.4675\n",
      "Epoch 31 Batch 1500 Loss 2.5720 Accuracy 0.4676\n",
      "Epoch 31 Batch 1550 Loss 2.5720 Accuracy 0.4676\n",
      "Epoch 31 Batch 1600 Loss 2.5725 Accuracy 0.4675\n",
      "Epoch 31 Batch 1650 Loss 2.5730 Accuracy 0.4675\n",
      "Epoch 31 Batch 1700 Loss 2.5734 Accuracy 0.4674\n",
      "Epoch 31 Batch 1750 Loss 2.5729 Accuracy 0.4675\n",
      "Epoch 31 Batch 1800 Loss 2.5736 Accuracy 0.4674\n",
      "Epoch 31 Batch 1850 Loss 2.5738 Accuracy 0.4674\n",
      "Epoch 31 Batch 1900 Loss 2.5744 Accuracy 0.4673\n",
      "Epoch 31 Batch 1950 Loss 2.5743 Accuracy 0.4673\n",
      "Epoch 31 Batch 2000 Loss 2.5748 Accuracy 0.4673\n",
      "Epoch 31 Batch 2050 Loss 2.5751 Accuracy 0.4672\n",
      "Epoch 31 Batch 2100 Loss 2.5752 Accuracy 0.4672\n",
      "Epoch 31 Batch 2150 Loss 2.5752 Accuracy 0.4672\n",
      "Epoch 31 Batch 2200 Loss 2.5753 Accuracy 0.4672\n",
      "Epoch 31 Batch 2250 Loss 2.5763 Accuracy 0.4671\n",
      "Epoch 31 Batch 2300 Loss 2.5765 Accuracy 0.4670\n",
      "Epoch 31 Batch 2350 Loss 2.5768 Accuracy 0.4669\n",
      "Epoch 31 Batch 2400 Loss 2.5769 Accuracy 0.4669\n",
      "Epoch 31 Batch 2450 Loss 2.5768 Accuracy 0.4669\n",
      "Epoch 31 Batch 2500 Loss 2.5773 Accuracy 0.4668\n",
      "Epoch 31 Batch 2550 Loss 2.5776 Accuracy 0.4668\n",
      "Epoch 31 Batch 2600 Loss 2.5782 Accuracy 0.4667\n",
      "Epoch 31 Loss 2.5784 Accuracy 0.4667\n",
      "Time taken for 1 epoch: 454.3150563240051 secs\n",
      "\n",
      "Epoch 32 Batch 0 Loss 2.4738 Accuracy 0.4940\n",
      "Epoch 32 Batch 50 Loss 2.5596 Accuracy 0.4672\n",
      "Epoch 32 Batch 100 Loss 2.5627 Accuracy 0.4677\n",
      "Epoch 32 Batch 150 Loss 2.5540 Accuracy 0.4698\n",
      "Epoch 32 Batch 200 Loss 2.5524 Accuracy 0.4697\n",
      "Epoch 32 Batch 250 Loss 2.5542 Accuracy 0.4694\n",
      "Epoch 32 Batch 300 Loss 2.5546 Accuracy 0.4693\n",
      "Epoch 32 Batch 350 Loss 2.5574 Accuracy 0.4690\n",
      "Epoch 32 Batch 400 Loss 2.5594 Accuracy 0.4686\n",
      "Epoch 32 Batch 450 Loss 2.5595 Accuracy 0.4687\n",
      "Epoch 32 Batch 500 Loss 2.5574 Accuracy 0.4691\n",
      "Epoch 32 Batch 550 Loss 2.5581 Accuracy 0.4690\n",
      "Epoch 32 Batch 600 Loss 2.5575 Accuracy 0.4693\n",
      "Epoch 32 Batch 650 Loss 2.5581 Accuracy 0.4693\n",
      "Epoch 32 Batch 700 Loss 2.5589 Accuracy 0.4692\n",
      "Epoch 32 Batch 750 Loss 2.5601 Accuracy 0.4691\n",
      "Epoch 32 Batch 800 Loss 2.5604 Accuracy 0.4690\n",
      "Epoch 32 Batch 850 Loss 2.5595 Accuracy 0.4692\n",
      "Epoch 32 Batch 900 Loss 2.5601 Accuracy 0.4692\n",
      "Epoch 32 Batch 950 Loss 2.5602 Accuracy 0.4691\n",
      "Epoch 32 Batch 1000 Loss 2.5607 Accuracy 0.4691\n",
      "Epoch 32 Batch 1050 Loss 2.5624 Accuracy 0.4688\n",
      "Epoch 32 Batch 1100 Loss 2.5626 Accuracy 0.4687\n",
      "Epoch 32 Batch 1150 Loss 2.5624 Accuracy 0.4689\n",
      "Epoch 32 Batch 1200 Loss 2.5622 Accuracy 0.4689\n",
      "Epoch 32 Batch 1250 Loss 2.5621 Accuracy 0.4689\n",
      "Epoch 32 Batch 1300 Loss 2.5626 Accuracy 0.4688\n",
      "Epoch 32 Batch 1350 Loss 2.5624 Accuracy 0.4688\n",
      "Epoch 32 Batch 1400 Loss 2.5620 Accuracy 0.4690\n",
      "Epoch 32 Batch 1450 Loss 2.5621 Accuracy 0.4689\n",
      "Epoch 32 Batch 1500 Loss 2.5621 Accuracy 0.4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Batch 1550 Loss 2.5628 Accuracy 0.4687\n",
      "Epoch 32 Batch 1600 Loss 2.5634 Accuracy 0.4687\n",
      "Epoch 32 Batch 1650 Loss 2.5638 Accuracy 0.4687\n",
      "Epoch 32 Batch 1700 Loss 2.5635 Accuracy 0.4688\n",
      "Epoch 32 Batch 1750 Loss 2.5632 Accuracy 0.4688\n",
      "Epoch 32 Batch 1800 Loss 2.5635 Accuracy 0.4688\n",
      "Epoch 32 Batch 1850 Loss 2.5641 Accuracy 0.4688\n",
      "Epoch 32 Batch 1900 Loss 2.5646 Accuracy 0.4687\n",
      "Epoch 32 Batch 1950 Loss 2.5648 Accuracy 0.4687\n",
      "Epoch 32 Batch 2000 Loss 2.5648 Accuracy 0.4688\n",
      "Epoch 32 Batch 2050 Loss 2.5645 Accuracy 0.4688\n",
      "Epoch 32 Batch 2100 Loss 2.5647 Accuracy 0.4688\n",
      "Epoch 32 Batch 2150 Loss 2.5651 Accuracy 0.4687\n",
      "Epoch 32 Batch 2200 Loss 2.5653 Accuracy 0.4687\n",
      "Epoch 32 Batch 2250 Loss 2.5654 Accuracy 0.4686\n",
      "Epoch 32 Batch 2300 Loss 2.5655 Accuracy 0.4686\n",
      "Epoch 32 Batch 2350 Loss 2.5657 Accuracy 0.4686\n",
      "Epoch 32 Batch 2400 Loss 2.5655 Accuracy 0.4686\n",
      "Epoch 32 Batch 2450 Loss 2.5660 Accuracy 0.4686\n",
      "Epoch 32 Batch 2500 Loss 2.5664 Accuracy 0.4685\n",
      "Epoch 32 Batch 2550 Loss 2.5669 Accuracy 0.4684\n",
      "Epoch 32 Batch 2600 Loss 2.5678 Accuracy 0.4683\n",
      "Epoch 32 Loss 2.5678 Accuracy 0.4684\n",
      "Time taken for 1 epoch: 453.6474299430847 secs\n",
      "\n",
      "Epoch 33 Batch 0 Loss 2.4863 Accuracy 0.4717\n",
      "Epoch 33 Batch 50 Loss 2.5509 Accuracy 0.4713\n",
      "Epoch 33 Batch 100 Loss 2.5490 Accuracy 0.4706\n",
      "Epoch 33 Batch 150 Loss 2.5445 Accuracy 0.4706\n",
      "Epoch 33 Batch 200 Loss 2.5475 Accuracy 0.4700\n",
      "Epoch 33 Batch 250 Loss 2.5450 Accuracy 0.4703\n",
      "Epoch 33 Batch 300 Loss 2.5456 Accuracy 0.4704\n",
      "Epoch 33 Batch 350 Loss 2.5451 Accuracy 0.4703\n",
      "Epoch 33 Batch 400 Loss 2.5457 Accuracy 0.4699\n",
      "Epoch 33 Batch 450 Loss 2.5470 Accuracy 0.4696\n",
      "Epoch 33 Batch 500 Loss 2.5492 Accuracy 0.4696\n",
      "Epoch 33 Batch 550 Loss 2.5489 Accuracy 0.4700\n",
      "Epoch 33 Batch 600 Loss 2.5488 Accuracy 0.4700\n",
      "Epoch 33 Batch 650 Loss 2.5495 Accuracy 0.4700\n",
      "Epoch 33 Batch 700 Loss 2.5509 Accuracy 0.4699\n",
      "Epoch 33 Batch 750 Loss 2.5501 Accuracy 0.4700\n",
      "Epoch 33 Batch 800 Loss 2.5503 Accuracy 0.4700\n",
      "Epoch 33 Batch 850 Loss 2.5494 Accuracy 0.4701\n",
      "Epoch 33 Batch 900 Loss 2.5506 Accuracy 0.4700\n",
      "Epoch 33 Batch 950 Loss 2.5508 Accuracy 0.4699\n",
      "Epoch 33 Batch 1000 Loss 2.5500 Accuracy 0.4702\n",
      "Epoch 33 Batch 1050 Loss 2.5498 Accuracy 0.4701\n",
      "Epoch 33 Batch 1100 Loss 2.5496 Accuracy 0.4701\n",
      "Epoch 33 Batch 1150 Loss 2.5499 Accuracy 0.4701\n",
      "Epoch 33 Batch 1200 Loss 2.5499 Accuracy 0.4700\n",
      "Epoch 33 Batch 1250 Loss 2.5504 Accuracy 0.4701\n",
      "Epoch 33 Batch 1300 Loss 2.5507 Accuracy 0.4700\n",
      "Epoch 33 Batch 1350 Loss 2.5516 Accuracy 0.4699\n",
      "Epoch 33 Batch 1400 Loss 2.5521 Accuracy 0.4698\n",
      "Epoch 33 Batch 1450 Loss 2.5526 Accuracy 0.4698\n",
      "Epoch 33 Batch 1500 Loss 2.5530 Accuracy 0.4697\n",
      "Epoch 33 Batch 1550 Loss 2.5530 Accuracy 0.4697\n",
      "Epoch 33 Batch 1600 Loss 2.5539 Accuracy 0.4696\n",
      "Epoch 33 Batch 1650 Loss 2.5538 Accuracy 0.4696\n",
      "Epoch 33 Batch 1700 Loss 2.5536 Accuracy 0.4696\n",
      "Epoch 33 Batch 1750 Loss 2.5544 Accuracy 0.4695\n",
      "Epoch 33 Batch 1800 Loss 2.5551 Accuracy 0.4694\n",
      "Epoch 33 Batch 1850 Loss 2.5557 Accuracy 0.4693\n",
      "Epoch 33 Batch 1900 Loss 2.5553 Accuracy 0.4694\n",
      "Epoch 33 Batch 1950 Loss 2.5553 Accuracy 0.4694\n",
      "Epoch 33 Batch 2000 Loss 2.5557 Accuracy 0.4694\n",
      "Epoch 33 Batch 2050 Loss 2.5558 Accuracy 0.4694\n",
      "Epoch 33 Batch 2100 Loss 2.5556 Accuracy 0.4695\n",
      "Epoch 33 Batch 2150 Loss 2.5560 Accuracy 0.4694\n",
      "Epoch 33 Batch 2200 Loss 2.5564 Accuracy 0.4694\n",
      "Epoch 33 Batch 2250 Loss 2.5568 Accuracy 0.4693\n",
      "Epoch 33 Batch 2300 Loss 2.5570 Accuracy 0.4693\n",
      "Epoch 33 Batch 2350 Loss 2.5572 Accuracy 0.4693\n",
      "Epoch 33 Batch 2400 Loss 2.5572 Accuracy 0.4693\n",
      "Epoch 33 Batch 2450 Loss 2.5577 Accuracy 0.4692\n",
      "Epoch 33 Batch 2500 Loss 2.5580 Accuracy 0.4692\n",
      "Epoch 33 Batch 2550 Loss 2.5581 Accuracy 0.4693\n",
      "Epoch 33 Batch 2600 Loss 2.5584 Accuracy 0.4693\n",
      "Epoch 33 Loss 2.5588 Accuracy 0.4692\n",
      "Time taken for 1 epoch: 452.3914017677307 secs\n",
      "\n",
      "Epoch 34 Batch 0 Loss 2.7983 Accuracy 0.4375\n",
      "Epoch 34 Batch 50 Loss 2.5427 Accuracy 0.4707\n",
      "Epoch 34 Batch 100 Loss 2.5260 Accuracy 0.4721\n",
      "Epoch 34 Batch 150 Loss 2.5233 Accuracy 0.4731\n",
      "Epoch 34 Batch 200 Loss 2.5300 Accuracy 0.4719\n",
      "Epoch 34 Batch 250 Loss 2.5328 Accuracy 0.4717\n",
      "Epoch 34 Batch 300 Loss 2.5342 Accuracy 0.4716\n",
      "Epoch 34 Batch 350 Loss 2.5369 Accuracy 0.4710\n",
      "Epoch 34 Batch 400 Loss 2.5369 Accuracy 0.4712\n",
      "Epoch 34 Batch 450 Loss 2.5364 Accuracy 0.4715\n",
      "Epoch 34 Batch 500 Loss 2.5362 Accuracy 0.4713\n",
      "Epoch 34 Batch 550 Loss 2.5391 Accuracy 0.4711\n",
      "Epoch 34 Batch 600 Loss 2.5390 Accuracy 0.4713\n",
      "Epoch 34 Batch 650 Loss 2.5389 Accuracy 0.4713\n",
      "Epoch 34 Batch 700 Loss 2.5386 Accuracy 0.4714\n",
      "Epoch 34 Batch 750 Loss 2.5395 Accuracy 0.4713\n",
      "Epoch 34 Batch 800 Loss 2.5398 Accuracy 0.4714\n",
      "Epoch 34 Batch 850 Loss 2.5405 Accuracy 0.4713\n",
      "Epoch 34 Batch 900 Loss 2.5411 Accuracy 0.4712\n",
      "Epoch 34 Batch 950 Loss 2.5413 Accuracy 0.4712\n",
      "Epoch 34 Batch 1000 Loss 2.5421 Accuracy 0.4712\n",
      "Epoch 34 Batch 1050 Loss 2.5419 Accuracy 0.4712\n",
      "Epoch 34 Batch 1100 Loss 2.5422 Accuracy 0.4712\n",
      "Epoch 34 Batch 1150 Loss 2.5418 Accuracy 0.4713\n",
      "Epoch 34 Batch 1200 Loss 2.5418 Accuracy 0.4715\n",
      "Epoch 34 Batch 1250 Loss 2.5423 Accuracy 0.4714\n",
      "Epoch 34 Batch 1300 Loss 2.5426 Accuracy 0.4713\n",
      "Epoch 34 Batch 1350 Loss 2.5430 Accuracy 0.4713\n",
      "Epoch 34 Batch 1400 Loss 2.5434 Accuracy 0.4713\n",
      "Epoch 34 Batch 1450 Loss 2.5432 Accuracy 0.4713\n",
      "Epoch 34 Batch 1500 Loss 2.5436 Accuracy 0.4713\n",
      "Epoch 34 Batch 1550 Loss 2.5439 Accuracy 0.4712\n",
      "Epoch 34 Batch 1600 Loss 2.5443 Accuracy 0.4712\n",
      "Epoch 34 Batch 1650 Loss 2.5446 Accuracy 0.4711\n",
      "Epoch 34 Batch 1700 Loss 2.5446 Accuracy 0.4711\n",
      "Epoch 34 Batch 1750 Loss 2.5447 Accuracy 0.4712\n",
      "Epoch 34 Batch 1800 Loss 2.5449 Accuracy 0.4711\n",
      "Epoch 34 Batch 1850 Loss 2.5455 Accuracy 0.4710\n",
      "Epoch 34 Batch 1900 Loss 2.5451 Accuracy 0.4711\n",
      "Epoch 34 Batch 1950 Loss 2.5455 Accuracy 0.4710\n",
      "Epoch 34 Batch 2000 Loss 2.5455 Accuracy 0.4710\n",
      "Epoch 34 Batch 2050 Loss 2.5455 Accuracy 0.4710\n",
      "Epoch 34 Batch 2100 Loss 2.5459 Accuracy 0.4709\n",
      "Epoch 34 Batch 2150 Loss 2.5459 Accuracy 0.4709\n",
      "Epoch 34 Batch 2200 Loss 2.5464 Accuracy 0.4708\n",
      "Epoch 34 Batch 2250 Loss 2.5467 Accuracy 0.4708\n",
      "Epoch 34 Batch 2300 Loss 2.5468 Accuracy 0.4708\n",
      "Epoch 34 Batch 2350 Loss 2.5472 Accuracy 0.4707\n",
      "Epoch 34 Batch 2400 Loss 2.5474 Accuracy 0.4707\n",
      "Epoch 34 Batch 2450 Loss 2.5481 Accuracy 0.4706\n",
      "Epoch 34 Batch 2500 Loss 2.5484 Accuracy 0.4705\n",
      "Epoch 34 Batch 2550 Loss 2.5491 Accuracy 0.4704\n",
      "Epoch 34 Batch 2600 Loss 2.5491 Accuracy 0.4705\n",
      "Epoch 34 Loss 2.5492 Accuracy 0.4704\n",
      "Time taken for 1 epoch: 452.48975348472595 secs\n",
      "\n",
      "Epoch 35 Batch 0 Loss 2.6813 Accuracy 0.4496\n",
      "Epoch 35 Batch 50 Loss 2.5117 Accuracy 0.4745\n",
      "Epoch 35 Batch 100 Loss 2.5242 Accuracy 0.4733\n",
      "Epoch 35 Batch 150 Loss 2.5248 Accuracy 0.4726\n",
      "Epoch 35 Batch 200 Loss 2.5226 Accuracy 0.4735\n",
      "Epoch 35 Batch 250 Loss 2.5277 Accuracy 0.4729\n",
      "Epoch 35 Batch 300 Loss 2.5296 Accuracy 0.4729\n",
      "Epoch 35 Batch 350 Loss 2.5308 Accuracy 0.4726\n",
      "Epoch 35 Batch 400 Loss 2.5318 Accuracy 0.4722\n",
      "Epoch 35 Batch 450 Loss 2.5321 Accuracy 0.4722\n",
      "Epoch 35 Batch 500 Loss 2.5325 Accuracy 0.4720\n",
      "Epoch 35 Batch 550 Loss 2.5319 Accuracy 0.4720\n",
      "Epoch 35 Batch 600 Loss 2.5342 Accuracy 0.4717\n",
      "Epoch 35 Batch 650 Loss 2.5349 Accuracy 0.4717\n",
      "Epoch 35 Batch 700 Loss 2.5350 Accuracy 0.4715\n",
      "Epoch 35 Batch 750 Loss 2.5357 Accuracy 0.4716\n",
      "Epoch 35 Batch 800 Loss 2.5362 Accuracy 0.4715\n",
      "Epoch 35 Batch 850 Loss 2.5353 Accuracy 0.4716\n",
      "Epoch 35 Batch 900 Loss 2.5345 Accuracy 0.4718\n",
      "Epoch 35 Batch 950 Loss 2.5342 Accuracy 0.4719\n",
      "Epoch 35 Batch 1000 Loss 2.5342 Accuracy 0.4720\n",
      "Epoch 35 Batch 1050 Loss 2.5346 Accuracy 0.4719\n",
      "Epoch 35 Batch 1100 Loss 2.5349 Accuracy 0.4719\n",
      "Epoch 35 Batch 1150 Loss 2.5354 Accuracy 0.4719\n",
      "Epoch 35 Batch 1200 Loss 2.5355 Accuracy 0.4719\n",
      "Epoch 35 Batch 1250 Loss 2.5360 Accuracy 0.4719\n",
      "Epoch 35 Batch 1300 Loss 2.5361 Accuracy 0.4720\n",
      "Epoch 35 Batch 1350 Loss 2.5360 Accuracy 0.4721\n",
      "Epoch 35 Batch 1400 Loss 2.5369 Accuracy 0.4720\n",
      "Epoch 35 Batch 1450 Loss 2.5376 Accuracy 0.4719\n",
      "Epoch 35 Batch 1500 Loss 2.5375 Accuracy 0.4719\n",
      "Epoch 35 Batch 1550 Loss 2.5375 Accuracy 0.4719\n",
      "Epoch 35 Batch 1600 Loss 2.5376 Accuracy 0.4719\n",
      "Epoch 35 Batch 1650 Loss 2.5375 Accuracy 0.4719\n",
      "Epoch 35 Batch 1700 Loss 2.5371 Accuracy 0.4721\n",
      "Epoch 35 Batch 1750 Loss 2.5378 Accuracy 0.4720\n",
      "Epoch 35 Batch 1800 Loss 2.5382 Accuracy 0.4719\n",
      "Epoch 35 Batch 1850 Loss 2.5384 Accuracy 0.4718\n",
      "Epoch 35 Batch 1900 Loss 2.5377 Accuracy 0.4719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Batch 1950 Loss 2.5379 Accuracy 0.4719\n",
      "Epoch 35 Batch 2000 Loss 2.5382 Accuracy 0.4719\n",
      "Epoch 35 Batch 2050 Loss 2.5379 Accuracy 0.4720\n",
      "Epoch 35 Batch 2100 Loss 2.5379 Accuracy 0.4720\n",
      "Epoch 35 Batch 2150 Loss 2.5383 Accuracy 0.4719\n",
      "Epoch 35 Batch 2200 Loss 2.5388 Accuracy 0.4719\n",
      "Epoch 35 Batch 2250 Loss 2.5392 Accuracy 0.4719\n",
      "Epoch 35 Batch 2300 Loss 2.5394 Accuracy 0.4719\n",
      "Epoch 35 Batch 2350 Loss 2.5396 Accuracy 0.4719\n",
      "Epoch 35 Batch 2400 Loss 2.5398 Accuracy 0.4718\n",
      "Epoch 35 Batch 2450 Loss 2.5403 Accuracy 0.4718\n",
      "Epoch 35 Batch 2500 Loss 2.5403 Accuracy 0.4718\n",
      "Epoch 35 Batch 2550 Loss 2.5404 Accuracy 0.4717\n",
      "Epoch 35 Batch 2600 Loss 2.5409 Accuracy 0.4717\n",
      "Saving checkpoint for epoch 35 at ./checkpoints/train_full/ckpt-9\n",
      "Epoch 35 Loss 2.5412 Accuracy 0.4716\n",
      "Time taken for 1 epoch: 452.80837869644165 secs\n",
      "\n",
      "Epoch 36 Batch 0 Loss 2.5390 Accuracy 0.4708\n",
      "Epoch 36 Batch 50 Loss 2.4965 Accuracy 0.4770\n",
      "Epoch 36 Batch 100 Loss 2.5026 Accuracy 0.4760\n",
      "Epoch 36 Batch 150 Loss 2.5069 Accuracy 0.4754\n",
      "Epoch 36 Batch 200 Loss 2.5064 Accuracy 0.4757\n",
      "Epoch 36 Batch 250 Loss 2.5109 Accuracy 0.4748\n",
      "Epoch 36 Batch 300 Loss 2.5147 Accuracy 0.4742\n",
      "Epoch 36 Batch 350 Loss 2.5166 Accuracy 0.4741\n",
      "Epoch 36 Batch 400 Loss 2.5181 Accuracy 0.4740\n",
      "Epoch 36 Batch 450 Loss 2.5185 Accuracy 0.4739\n",
      "Epoch 36 Batch 500 Loss 2.5181 Accuracy 0.4741\n",
      "Epoch 36 Batch 550 Loss 2.5179 Accuracy 0.4742\n",
      "Epoch 36 Batch 600 Loss 2.5201 Accuracy 0.4741\n",
      "Epoch 36 Batch 650 Loss 2.5212 Accuracy 0.4739\n",
      "Epoch 36 Batch 700 Loss 2.5214 Accuracy 0.4738\n",
      "Epoch 36 Batch 750 Loss 2.5222 Accuracy 0.4736\n",
      "Epoch 36 Batch 800 Loss 2.5224 Accuracy 0.4735\n",
      "Epoch 36 Batch 850 Loss 2.5235 Accuracy 0.4735\n",
      "Epoch 36 Batch 900 Loss 2.5236 Accuracy 0.4734\n",
      "Epoch 36 Batch 950 Loss 2.5240 Accuracy 0.4734\n",
      "Epoch 36 Batch 1000 Loss 2.5243 Accuracy 0.4733\n",
      "Epoch 36 Batch 1050 Loss 2.5244 Accuracy 0.4733\n",
      "Epoch 36 Batch 1100 Loss 2.5247 Accuracy 0.4733\n",
      "Epoch 36 Batch 1150 Loss 2.5245 Accuracy 0.4733\n",
      "Epoch 36 Batch 1200 Loss 2.5248 Accuracy 0.4733\n",
      "Epoch 36 Batch 1250 Loss 2.5251 Accuracy 0.4732\n",
      "Epoch 36 Batch 1300 Loss 2.5258 Accuracy 0.4732\n",
      "Epoch 36 Batch 1350 Loss 2.5253 Accuracy 0.4733\n",
      "Epoch 36 Batch 1400 Loss 2.5261 Accuracy 0.4732\n",
      "Epoch 36 Batch 1450 Loss 2.5254 Accuracy 0.4733\n",
      "Epoch 36 Batch 1500 Loss 2.5257 Accuracy 0.4732\n",
      "Epoch 36 Batch 1550 Loss 2.5263 Accuracy 0.4732\n",
      "Epoch 36 Batch 1600 Loss 2.5265 Accuracy 0.4732\n",
      "Epoch 36 Batch 1650 Loss 2.5265 Accuracy 0.4732\n",
      "Epoch 36 Batch 1700 Loss 2.5271 Accuracy 0.4732\n",
      "Epoch 36 Batch 1750 Loss 2.5271 Accuracy 0.4732\n",
      "Epoch 36 Batch 1800 Loss 2.5271 Accuracy 0.4732\n",
      "Epoch 36 Batch 1850 Loss 2.5270 Accuracy 0.4732\n",
      "Epoch 36 Batch 1900 Loss 2.5274 Accuracy 0.4731\n",
      "Epoch 36 Batch 1950 Loss 2.5272 Accuracy 0.4732\n",
      "Epoch 36 Batch 2000 Loss 2.5272 Accuracy 0.4732\n",
      "Epoch 36 Batch 2050 Loss 2.5273 Accuracy 0.4732\n",
      "Epoch 36 Batch 2100 Loss 2.5273 Accuracy 0.4732\n",
      "Epoch 36 Batch 2150 Loss 2.5279 Accuracy 0.4731\n",
      "Epoch 36 Batch 2200 Loss 2.5279 Accuracy 0.4731\n",
      "Epoch 36 Batch 2250 Loss 2.5283 Accuracy 0.4730\n",
      "Epoch 36 Batch 2300 Loss 2.5286 Accuracy 0.4730\n",
      "Epoch 36 Batch 2350 Loss 2.5288 Accuracy 0.4730\n",
      "Epoch 36 Batch 2400 Loss 2.5290 Accuracy 0.4730\n",
      "Epoch 36 Batch 2450 Loss 2.5289 Accuracy 0.4730\n",
      "Epoch 36 Batch 2500 Loss 2.5294 Accuracy 0.4730\n",
      "Epoch 36 Batch 2550 Loss 2.5298 Accuracy 0.4729\n",
      "Epoch 36 Batch 2600 Loss 2.5306 Accuracy 0.4728\n",
      "Epoch 36 Loss 2.5308 Accuracy 0.4728\n",
      "Time taken for 1 epoch: 454.6493103504181 secs\n",
      "\n",
      "Epoch 37 Batch 0 Loss 2.6222 Accuracy 0.4573\n",
      "Epoch 37 Batch 50 Loss 2.5062 Accuracy 0.4769\n",
      "Epoch 37 Batch 100 Loss 2.5004 Accuracy 0.4760\n",
      "Epoch 37 Batch 150 Loss 2.5023 Accuracy 0.4763\n",
      "Epoch 37 Batch 200 Loss 2.5038 Accuracy 0.4764\n",
      "Epoch 37 Batch 250 Loss 2.5093 Accuracy 0.4754\n",
      "Epoch 37 Batch 300 Loss 2.5090 Accuracy 0.4754\n",
      "Epoch 37 Batch 350 Loss 2.5095 Accuracy 0.4754\n",
      "Epoch 37 Batch 400 Loss 2.5099 Accuracy 0.4753\n",
      "Epoch 37 Batch 450 Loss 2.5094 Accuracy 0.4755\n",
      "Epoch 37 Batch 500 Loss 2.5115 Accuracy 0.4749\n",
      "Epoch 37 Batch 550 Loss 2.5122 Accuracy 0.4750\n",
      "Epoch 37 Batch 600 Loss 2.5135 Accuracy 0.4748\n",
      "Epoch 37 Batch 650 Loss 2.5137 Accuracy 0.4748\n",
      "Epoch 37 Batch 700 Loss 2.5140 Accuracy 0.4748\n",
      "Epoch 37 Batch 750 Loss 2.5145 Accuracy 0.4747\n",
      "Epoch 37 Batch 800 Loss 2.5153 Accuracy 0.4746\n",
      "Epoch 37 Batch 850 Loss 2.5150 Accuracy 0.4747\n",
      "Epoch 37 Batch 900 Loss 2.5137 Accuracy 0.4748\n",
      "Epoch 37 Batch 950 Loss 2.5139 Accuracy 0.4748\n",
      "Epoch 37 Batch 1000 Loss 2.5149 Accuracy 0.4746\n",
      "Epoch 37 Batch 1050 Loss 2.5154 Accuracy 0.4745\n",
      "Epoch 37 Batch 1100 Loss 2.5158 Accuracy 0.4746\n",
      "Epoch 37 Batch 1150 Loss 2.5159 Accuracy 0.4745\n",
      "Epoch 37 Batch 1200 Loss 2.5158 Accuracy 0.4746\n",
      "Epoch 37 Batch 1250 Loss 2.5166 Accuracy 0.4745\n",
      "Epoch 37 Batch 1300 Loss 2.5168 Accuracy 0.4745\n",
      "Epoch 37 Batch 1350 Loss 2.5174 Accuracy 0.4744\n",
      "Epoch 37 Batch 1400 Loss 2.5181 Accuracy 0.4744\n",
      "Epoch 37 Batch 1450 Loss 2.5180 Accuracy 0.4744\n",
      "Epoch 37 Batch 1500 Loss 2.5174 Accuracy 0.4745\n",
      "Epoch 37 Batch 1550 Loss 2.5172 Accuracy 0.4745\n",
      "Epoch 37 Batch 1600 Loss 2.5170 Accuracy 0.4745\n",
      "Epoch 37 Batch 1650 Loss 2.5173 Accuracy 0.4745\n",
      "Epoch 37 Batch 1700 Loss 2.5174 Accuracy 0.4745\n",
      "Epoch 37 Batch 1750 Loss 2.5177 Accuracy 0.4745\n",
      "Epoch 37 Batch 1800 Loss 2.5175 Accuracy 0.4745\n",
      "Epoch 37 Batch 1850 Loss 2.5178 Accuracy 0.4745\n",
      "Epoch 37 Batch 1900 Loss 2.5184 Accuracy 0.4744\n",
      "Epoch 37 Batch 1950 Loss 2.5188 Accuracy 0.4743\n",
      "Epoch 37 Batch 2000 Loss 2.5189 Accuracy 0.4743\n",
      "Epoch 37 Batch 2050 Loss 2.5190 Accuracy 0.4743\n",
      "Epoch 37 Batch 2100 Loss 2.5197 Accuracy 0.4742\n",
      "Epoch 37 Batch 2150 Loss 2.5192 Accuracy 0.4742\n",
      "Epoch 37 Batch 2200 Loss 2.5199 Accuracy 0.4741\n",
      "Epoch 37 Batch 2250 Loss 2.5199 Accuracy 0.4741\n",
      "Epoch 37 Batch 2300 Loss 2.5202 Accuracy 0.4741\n",
      "Epoch 37 Batch 2350 Loss 2.5201 Accuracy 0.4741\n",
      "Epoch 37 Batch 2400 Loss 2.5206 Accuracy 0.4740\n",
      "Epoch 37 Batch 2450 Loss 2.5208 Accuracy 0.4740\n",
      "Epoch 37 Batch 2500 Loss 2.5212 Accuracy 0.4740\n",
      "Epoch 37 Batch 2550 Loss 2.5220 Accuracy 0.4738\n",
      "Epoch 37 Batch 2600 Loss 2.5224 Accuracy 0.4738\n",
      "Epoch 37 Loss 2.5227 Accuracy 0.4738\n",
      "Time taken for 1 epoch: 453.33966302871704 secs\n",
      "\n",
      "Epoch 38 Batch 0 Loss 2.3860 Accuracy 0.4996\n",
      "Epoch 38 Batch 50 Loss 2.4867 Accuracy 0.4778\n",
      "Epoch 38 Batch 100 Loss 2.4966 Accuracy 0.4766\n",
      "Epoch 38 Batch 150 Loss 2.4921 Accuracy 0.4769\n",
      "Epoch 38 Batch 200 Loss 2.4926 Accuracy 0.4771\n",
      "Epoch 38 Batch 250 Loss 2.4900 Accuracy 0.4774\n",
      "Epoch 38 Batch 300 Loss 2.4923 Accuracy 0.4771\n",
      "Epoch 38 Batch 350 Loss 2.4936 Accuracy 0.4770\n",
      "Epoch 38 Batch 400 Loss 2.4977 Accuracy 0.4768\n",
      "Epoch 38 Batch 450 Loss 2.5017 Accuracy 0.4761\n",
      "Epoch 38 Batch 500 Loss 2.5038 Accuracy 0.4758\n",
      "Epoch 38 Batch 550 Loss 2.5040 Accuracy 0.4758\n",
      "Epoch 38 Batch 600 Loss 2.5035 Accuracy 0.4757\n",
      "Epoch 38 Batch 650 Loss 2.5040 Accuracy 0.4757\n",
      "Epoch 38 Batch 700 Loss 2.5043 Accuracy 0.4758\n",
      "Epoch 38 Batch 750 Loss 2.5047 Accuracy 0.4759\n",
      "Epoch 38 Batch 800 Loss 2.5053 Accuracy 0.4759\n",
      "Epoch 38 Batch 850 Loss 2.5056 Accuracy 0.4759\n",
      "Epoch 38 Batch 900 Loss 2.5062 Accuracy 0.4759\n",
      "Epoch 38 Batch 950 Loss 2.5072 Accuracy 0.4757\n",
      "Epoch 38 Batch 1000 Loss 2.5073 Accuracy 0.4756\n",
      "Epoch 38 Batch 1050 Loss 2.5075 Accuracy 0.4757\n",
      "Epoch 38 Batch 1100 Loss 2.5071 Accuracy 0.4758\n",
      "Epoch 38 Batch 1150 Loss 2.5067 Accuracy 0.4758\n",
      "Epoch 38 Batch 1200 Loss 2.5065 Accuracy 0.4758\n",
      "Epoch 38 Batch 1250 Loss 2.5072 Accuracy 0.4757\n",
      "Epoch 38 Batch 1300 Loss 2.5072 Accuracy 0.4757\n",
      "Epoch 38 Batch 1350 Loss 2.5081 Accuracy 0.4754\n",
      "Epoch 38 Batch 1400 Loss 2.5083 Accuracy 0.4755\n",
      "Epoch 38 Batch 1450 Loss 2.5082 Accuracy 0.4754\n",
      "Epoch 38 Batch 1500 Loss 2.5080 Accuracy 0.4755\n",
      "Epoch 38 Batch 1550 Loss 2.5083 Accuracy 0.4755\n",
      "Epoch 38 Batch 1600 Loss 2.5085 Accuracy 0.4755\n",
      "Epoch 38 Batch 1650 Loss 2.5083 Accuracy 0.4756\n",
      "Epoch 38 Batch 1700 Loss 2.5087 Accuracy 0.4755\n",
      "Epoch 38 Batch 1750 Loss 2.5090 Accuracy 0.4755\n",
      "Epoch 38 Batch 1800 Loss 2.5094 Accuracy 0.4755\n",
      "Epoch 38 Batch 1850 Loss 2.5099 Accuracy 0.4754\n",
      "Epoch 38 Batch 1900 Loss 2.5099 Accuracy 0.4754\n",
      "Epoch 38 Batch 1950 Loss 2.5104 Accuracy 0.4753\n",
      "Epoch 38 Batch 2000 Loss 2.5106 Accuracy 0.4752\n",
      "Epoch 38 Batch 2050 Loss 2.5104 Accuracy 0.4752\n",
      "Epoch 38 Batch 2100 Loss 2.5100 Accuracy 0.4753\n",
      "Epoch 38 Batch 2150 Loss 2.5101 Accuracy 0.4753\n",
      "Epoch 38 Batch 2200 Loss 2.5103 Accuracy 0.4753\n",
      "Epoch 38 Batch 2250 Loss 2.5107 Accuracy 0.4753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Batch 2300 Loss 2.5109 Accuracy 0.4753\n",
      "Epoch 38 Batch 2350 Loss 2.5112 Accuracy 0.4752\n",
      "Epoch 38 Batch 2400 Loss 2.5116 Accuracy 0.4751\n",
      "Epoch 38 Batch 2450 Loss 2.5120 Accuracy 0.4751\n",
      "Epoch 38 Batch 2500 Loss 2.5125 Accuracy 0.4750\n",
      "Epoch 38 Batch 2550 Loss 2.5130 Accuracy 0.4750\n",
      "Epoch 38 Batch 2600 Loss 2.5134 Accuracy 0.4749\n",
      "Epoch 38 Loss 2.5136 Accuracy 0.4749\n",
      "Time taken for 1 epoch: 452.00801634788513 secs\n",
      "\n",
      "Epoch 39 Batch 0 Loss 2.2984 Accuracy 0.5035\n",
      "Epoch 39 Batch 50 Loss 2.4765 Accuracy 0.4799\n",
      "Epoch 39 Batch 100 Loss 2.4823 Accuracy 0.4787\n",
      "Epoch 39 Batch 150 Loss 2.4850 Accuracy 0.4782\n",
      "Epoch 39 Batch 200 Loss 2.4883 Accuracy 0.4779\n",
      "Epoch 39 Batch 250 Loss 2.4905 Accuracy 0.4773\n",
      "Epoch 39 Batch 300 Loss 2.4908 Accuracy 0.4775\n",
      "Epoch 39 Batch 350 Loss 2.4931 Accuracy 0.4772\n",
      "Epoch 39 Batch 400 Loss 2.4943 Accuracy 0.4772\n",
      "Epoch 39 Batch 450 Loss 2.4928 Accuracy 0.4774\n",
      "Epoch 39 Batch 500 Loss 2.4929 Accuracy 0.4775\n",
      "Epoch 39 Batch 550 Loss 2.4946 Accuracy 0.4771\n",
      "Epoch 39 Batch 600 Loss 2.4941 Accuracy 0.4770\n",
      "Epoch 39 Batch 650 Loss 2.4954 Accuracy 0.4768\n",
      "Epoch 39 Batch 700 Loss 2.4960 Accuracy 0.4768\n",
      "Epoch 39 Batch 750 Loss 2.4948 Accuracy 0.4770\n",
      "Epoch 39 Batch 800 Loss 2.4952 Accuracy 0.4771\n",
      "Epoch 39 Batch 850 Loss 2.4949 Accuracy 0.4772\n",
      "Epoch 39 Batch 900 Loss 2.4961 Accuracy 0.4770\n",
      "Epoch 39 Batch 950 Loss 2.4963 Accuracy 0.4769\n",
      "Epoch 39 Batch 1000 Loss 2.4962 Accuracy 0.4770\n",
      "Epoch 39 Batch 1050 Loss 2.4962 Accuracy 0.4769\n",
      "Epoch 39 Batch 1100 Loss 2.4959 Accuracy 0.4769\n",
      "Epoch 39 Batch 1150 Loss 2.4968 Accuracy 0.4768\n",
      "Epoch 39 Batch 1200 Loss 2.4969 Accuracy 0.4768\n",
      "Epoch 39 Batch 1250 Loss 2.4971 Accuracy 0.4767\n",
      "Epoch 39 Batch 1300 Loss 2.4974 Accuracy 0.4767\n",
      "Epoch 39 Batch 1350 Loss 2.4980 Accuracy 0.4766\n",
      "Epoch 39 Batch 1400 Loss 2.4976 Accuracy 0.4767\n",
      "Epoch 39 Batch 1450 Loss 2.4983 Accuracy 0.4766\n",
      "Epoch 39 Batch 1500 Loss 2.4985 Accuracy 0.4767\n",
      "Epoch 39 Batch 1550 Loss 2.4988 Accuracy 0.4766\n",
      "Epoch 39 Batch 1600 Loss 2.4990 Accuracy 0.4767\n",
      "Epoch 39 Batch 1650 Loss 2.4989 Accuracy 0.4768\n",
      "Epoch 39 Batch 1700 Loss 2.4991 Accuracy 0.4768\n",
      "Epoch 39 Batch 1750 Loss 2.4994 Accuracy 0.4768\n",
      "Epoch 39 Batch 1800 Loss 2.4995 Accuracy 0.4768\n",
      "Epoch 39 Batch 1850 Loss 2.5003 Accuracy 0.4767\n",
      "Epoch 39 Batch 1900 Loss 2.5001 Accuracy 0.4767\n",
      "Epoch 39 Batch 1950 Loss 2.5008 Accuracy 0.4766\n",
      "Epoch 39 Batch 2000 Loss 2.5009 Accuracy 0.4766\n",
      "Epoch 39 Batch 2050 Loss 2.5012 Accuracy 0.4765\n",
      "Epoch 39 Batch 2100 Loss 2.5012 Accuracy 0.4766\n",
      "Epoch 39 Batch 2150 Loss 2.5014 Accuracy 0.4766\n",
      "Epoch 39 Batch 2200 Loss 2.5015 Accuracy 0.4766\n",
      "Epoch 39 Batch 2250 Loss 2.5023 Accuracy 0.4765\n",
      "Epoch 39 Batch 2300 Loss 2.5026 Accuracy 0.4764\n",
      "Epoch 39 Batch 2350 Loss 2.5026 Accuracy 0.4764\n",
      "Epoch 39 Batch 2400 Loss 2.5028 Accuracy 0.4764\n",
      "Epoch 39 Batch 2450 Loss 2.5029 Accuracy 0.4763\n",
      "Epoch 39 Batch 2500 Loss 2.5031 Accuracy 0.4763\n",
      "Epoch 39 Batch 2550 Loss 2.5035 Accuracy 0.4763\n",
      "Epoch 39 Batch 2600 Loss 2.5038 Accuracy 0.4763\n",
      "Epoch 39 Loss 2.5041 Accuracy 0.4763\n",
      "Time taken for 1 epoch: 452.48439359664917 secs\n",
      "\n",
      "Epoch 40 Batch 0 Loss 2.4483 Accuracy 0.4815\n",
      "Epoch 40 Batch 50 Loss 2.4867 Accuracy 0.4786\n",
      "Epoch 40 Batch 100 Loss 2.4783 Accuracy 0.4784\n",
      "Epoch 40 Batch 150 Loss 2.4848 Accuracy 0.4774\n",
      "Epoch 40 Batch 200 Loss 2.4845 Accuracy 0.4777\n",
      "Epoch 40 Batch 250 Loss 2.4839 Accuracy 0.4780\n",
      "Epoch 40 Batch 300 Loss 2.4869 Accuracy 0.4778\n",
      "Epoch 40 Batch 350 Loss 2.4860 Accuracy 0.4782\n",
      "Epoch 40 Batch 400 Loss 2.4864 Accuracy 0.4783\n",
      "Epoch 40 Batch 450 Loss 2.4877 Accuracy 0.4784\n",
      "Epoch 40 Batch 500 Loss 2.4885 Accuracy 0.4783\n",
      "Epoch 40 Batch 550 Loss 2.4882 Accuracy 0.4783\n",
      "Epoch 40 Batch 600 Loss 2.4861 Accuracy 0.4786\n",
      "Epoch 40 Batch 650 Loss 2.4867 Accuracy 0.4785\n",
      "Epoch 40 Batch 700 Loss 2.4870 Accuracy 0.4785\n",
      "Epoch 40 Batch 750 Loss 2.4863 Accuracy 0.4785\n",
      "Epoch 40 Batch 800 Loss 2.4868 Accuracy 0.4784\n",
      "Epoch 40 Batch 850 Loss 2.4871 Accuracy 0.4783\n",
      "Epoch 40 Batch 900 Loss 2.4876 Accuracy 0.4785\n",
      "Epoch 40 Batch 950 Loss 2.4884 Accuracy 0.4785\n",
      "Epoch 40 Batch 1000 Loss 2.4875 Accuracy 0.4785\n",
      "Epoch 40 Batch 1050 Loss 2.4881 Accuracy 0.4784\n",
      "Epoch 40 Batch 1100 Loss 2.4876 Accuracy 0.4784\n",
      "Epoch 40 Batch 1150 Loss 2.4872 Accuracy 0.4783\n",
      "Epoch 40 Batch 1200 Loss 2.4872 Accuracy 0.4784\n",
      "Epoch 40 Batch 1250 Loss 2.4871 Accuracy 0.4784\n",
      "Epoch 40 Batch 1300 Loss 2.4871 Accuracy 0.4784\n",
      "Epoch 40 Batch 1350 Loss 2.4877 Accuracy 0.4784\n",
      "Epoch 40 Batch 1400 Loss 2.4885 Accuracy 0.4783\n",
      "Epoch 40 Batch 1450 Loss 2.4892 Accuracy 0.4782\n",
      "Epoch 40 Batch 1500 Loss 2.4899 Accuracy 0.4781\n",
      "Epoch 40 Batch 1550 Loss 2.4905 Accuracy 0.4781\n",
      "Epoch 40 Batch 1600 Loss 2.4908 Accuracy 0.4781\n",
      "Epoch 40 Batch 1650 Loss 2.4907 Accuracy 0.4781\n",
      "Epoch 40 Batch 1700 Loss 2.4908 Accuracy 0.4781\n",
      "Epoch 40 Batch 1750 Loss 2.4906 Accuracy 0.4782\n",
      "Epoch 40 Batch 1800 Loss 2.4912 Accuracy 0.4782\n",
      "Epoch 40 Batch 1850 Loss 2.4915 Accuracy 0.4781\n",
      "Epoch 40 Batch 1900 Loss 2.4912 Accuracy 0.4781\n",
      "Epoch 40 Batch 1950 Loss 2.4917 Accuracy 0.4780\n",
      "Epoch 40 Batch 2000 Loss 2.4926 Accuracy 0.4779\n",
      "Epoch 40 Batch 2050 Loss 2.4932 Accuracy 0.4778\n",
      "Epoch 40 Batch 2100 Loss 2.4930 Accuracy 0.4778\n",
      "Epoch 40 Batch 2150 Loss 2.4934 Accuracy 0.4778\n",
      "Epoch 40 Batch 2200 Loss 2.4935 Accuracy 0.4777\n",
      "Epoch 40 Batch 2250 Loss 2.4936 Accuracy 0.4777\n",
      "Epoch 40 Batch 2300 Loss 2.4940 Accuracy 0.4776\n",
      "Epoch 40 Batch 2350 Loss 2.4941 Accuracy 0.4776\n",
      "Epoch 40 Batch 2400 Loss 2.4943 Accuracy 0.4775\n",
      "Epoch 40 Batch 2450 Loss 2.4944 Accuracy 0.4775\n",
      "Epoch 40 Batch 2500 Loss 2.4947 Accuracy 0.4775\n",
      "Epoch 40 Batch 2550 Loss 2.4950 Accuracy 0.4775\n",
      "Epoch 40 Batch 2600 Loss 2.4954 Accuracy 0.4774\n",
      "Saving checkpoint for epoch 40 at ./checkpoints/train_full/ckpt-10\n",
      "Epoch 40 Loss 2.4956 Accuracy 0.4774\n",
      "Time taken for 1 epoch: 453.75755071640015 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "          print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "    history.append(train_loss.result())\n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.5327 Accuracy 0.4738\n",
      "Epoch 1 Batch 50 Loss 2.4934 Accuracy 0.4772\n",
      "Epoch 1 Batch 100 Loss 2.4733 Accuracy 0.4804\n",
      "Epoch 1 Batch 150 Loss 2.4672 Accuracy 0.4807\n",
      "Epoch 1 Batch 200 Loss 2.4663 Accuracy 0.4806\n",
      "Epoch 1 Batch 250 Loss 2.4689 Accuracy 0.4799\n",
      "Epoch 1 Batch 300 Loss 2.4718 Accuracy 0.4793\n",
      "Epoch 1 Batch 350 Loss 2.4727 Accuracy 0.4795\n",
      "Epoch 1 Batch 400 Loss 2.4735 Accuracy 0.4797\n",
      "Epoch 1 Batch 450 Loss 2.4766 Accuracy 0.4795\n",
      "Epoch 1 Batch 500 Loss 2.4790 Accuracy 0.4791\n",
      "Epoch 1 Batch 550 Loss 2.4774 Accuracy 0.4794\n",
      "Epoch 1 Batch 600 Loss 2.4794 Accuracy 0.4793\n",
      "Epoch 1 Batch 650 Loss 2.4782 Accuracy 0.4795\n",
      "Epoch 1 Batch 700 Loss 2.4792 Accuracy 0.4794\n",
      "Epoch 1 Batch 750 Loss 2.4797 Accuracy 0.4792\n",
      "Epoch 1 Batch 800 Loss 2.4806 Accuracy 0.4792\n",
      "Epoch 1 Batch 850 Loss 2.4811 Accuracy 0.4791\n",
      "Epoch 1 Batch 900 Loss 2.4805 Accuracy 0.4792\n",
      "Epoch 1 Batch 950 Loss 2.4801 Accuracy 0.4793\n",
      "Epoch 1 Batch 1000 Loss 2.4804 Accuracy 0.4793\n",
      "Epoch 1 Batch 1050 Loss 2.4804 Accuracy 0.4793\n",
      "Epoch 1 Batch 1100 Loss 2.4805 Accuracy 0.4793\n",
      "Epoch 1 Batch 1150 Loss 2.4807 Accuracy 0.4793\n",
      "Epoch 1 Batch 1200 Loss 2.4810 Accuracy 0.4793\n",
      "Epoch 1 Batch 1250 Loss 2.4810 Accuracy 0.4794\n",
      "Epoch 1 Batch 1300 Loss 2.4821 Accuracy 0.4794\n",
      "Epoch 1 Batch 1350 Loss 2.4824 Accuracy 0.4794\n",
      "Epoch 1 Batch 1400 Loss 2.4822 Accuracy 0.4794\n",
      "Epoch 1 Batch 1450 Loss 2.4820 Accuracy 0.4795\n",
      "Epoch 1 Batch 1500 Loss 2.4822 Accuracy 0.4794\n",
      "Epoch 1 Batch 1550 Loss 2.4819 Accuracy 0.4796\n",
      "Epoch 1 Batch 1600 Loss 2.4825 Accuracy 0.4794\n",
      "Epoch 1 Batch 1650 Loss 2.4827 Accuracy 0.4794\n",
      "Epoch 1 Batch 1700 Loss 2.4837 Accuracy 0.4793\n",
      "Epoch 1 Batch 1750 Loss 2.4836 Accuracy 0.4793\n",
      "Epoch 1 Batch 1800 Loss 2.4837 Accuracy 0.4792\n",
      "Epoch 1 Batch 1850 Loss 2.4839 Accuracy 0.4792\n",
      "Epoch 1 Batch 1900 Loss 2.4845 Accuracy 0.4792\n",
      "Epoch 1 Batch 1950 Loss 2.4842 Accuracy 0.4792\n",
      "Epoch 1 Batch 2000 Loss 2.4847 Accuracy 0.4791\n",
      "Epoch 1 Batch 2050 Loss 2.4849 Accuracy 0.4791\n",
      "Epoch 1 Batch 2100 Loss 2.4848 Accuracy 0.4791\n",
      "Epoch 1 Batch 2150 Loss 2.4849 Accuracy 0.4791\n",
      "Epoch 1 Batch 2200 Loss 2.4852 Accuracy 0.4791\n",
      "Epoch 1 Batch 2250 Loss 2.4859 Accuracy 0.4790\n",
      "Epoch 1 Batch 2300 Loss 2.4864 Accuracy 0.4789\n",
      "Epoch 1 Batch 2350 Loss 2.4865 Accuracy 0.4789\n",
      "Epoch 1 Batch 2400 Loss 2.4869 Accuracy 0.4788\n",
      "Epoch 1 Batch 2450 Loss 2.4871 Accuracy 0.4788\n",
      "Epoch 1 Batch 2500 Loss 2.4876 Accuracy 0.4787\n",
      "Epoch 1 Batch 2550 Loss 2.4876 Accuracy 0.4787\n",
      "Epoch 1 Batch 2600 Loss 2.4879 Accuracy 0.4788\n",
      "Epoch 1 Loss 2.4881 Accuracy 0.4787\n",
      "Time taken for 1 epoch: 454.46432304382324 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.4028 Accuracy 0.4828\n",
      "Epoch 2 Batch 50 Loss 2.4709 Accuracy 0.4796\n",
      "Epoch 2 Batch 100 Loss 2.4676 Accuracy 0.4802\n",
      "Epoch 2 Batch 150 Loss 2.4670 Accuracy 0.4809\n",
      "Epoch 2 Batch 200 Loss 2.4643 Accuracy 0.4814\n",
      "Epoch 2 Batch 250 Loss 2.4591 Accuracy 0.4819\n",
      "Epoch 2 Batch 300 Loss 2.4595 Accuracy 0.4818\n",
      "Epoch 2 Batch 350 Loss 2.4616 Accuracy 0.4813\n",
      "Epoch 2 Batch 400 Loss 2.4631 Accuracy 0.4813\n",
      "Epoch 2 Batch 450 Loss 2.4635 Accuracy 0.4814\n",
      "Epoch 2 Batch 500 Loss 2.4668 Accuracy 0.4810\n",
      "Epoch 2 Batch 550 Loss 2.4675 Accuracy 0.4808\n",
      "Epoch 2 Batch 600 Loss 2.4684 Accuracy 0.4807\n",
      "Epoch 2 Batch 650 Loss 2.4693 Accuracy 0.4803\n",
      "Epoch 2 Batch 700 Loss 2.4703 Accuracy 0.4802\n",
      "Epoch 2 Batch 750 Loss 2.4708 Accuracy 0.4803\n",
      "Epoch 2 Batch 800 Loss 2.4710 Accuracy 0.4804\n",
      "Epoch 2 Batch 850 Loss 2.4706 Accuracy 0.4805\n",
      "Epoch 2 Batch 900 Loss 2.4712 Accuracy 0.4805\n",
      "Epoch 2 Batch 950 Loss 2.4724 Accuracy 0.4803\n",
      "Epoch 2 Batch 1000 Loss 2.4730 Accuracy 0.4803\n",
      "Epoch 2 Batch 1050 Loss 2.4722 Accuracy 0.4804\n",
      "Epoch 2 Batch 1100 Loss 2.4728 Accuracy 0.4803\n",
      "Epoch 2 Batch 1150 Loss 2.4713 Accuracy 0.4806\n",
      "Epoch 2 Batch 1200 Loss 2.4718 Accuracy 0.4804\n",
      "Epoch 2 Batch 1250 Loss 2.4717 Accuracy 0.4804\n",
      "Epoch 2 Batch 1300 Loss 2.4717 Accuracy 0.4804\n",
      "Epoch 2 Batch 1350 Loss 2.4722 Accuracy 0.4804\n",
      "Epoch 2 Batch 1400 Loss 2.4722 Accuracy 0.4805\n",
      "Epoch 2 Batch 1450 Loss 2.4728 Accuracy 0.4805\n",
      "Epoch 2 Batch 1500 Loss 2.4735 Accuracy 0.4804\n",
      "Epoch 2 Batch 1550 Loss 2.4738 Accuracy 0.4805\n",
      "Epoch 2 Batch 1600 Loss 2.4741 Accuracy 0.4804\n",
      "Epoch 2 Batch 1650 Loss 2.4738 Accuracy 0.4805\n",
      "Epoch 2 Batch 1700 Loss 2.4743 Accuracy 0.4805\n",
      "Epoch 2 Batch 1750 Loss 2.4746 Accuracy 0.4805\n",
      "Epoch 2 Batch 1800 Loss 2.4747 Accuracy 0.4805\n",
      "Epoch 2 Batch 1850 Loss 2.4752 Accuracy 0.4804\n",
      "Epoch 2 Batch 1900 Loss 2.4756 Accuracy 0.4804\n",
      "Epoch 2 Batch 1950 Loss 2.4758 Accuracy 0.4804\n",
      "Epoch 2 Batch 2000 Loss 2.4756 Accuracy 0.4804\n",
      "Epoch 2 Batch 2050 Loss 2.4758 Accuracy 0.4803\n",
      "Epoch 2 Batch 2100 Loss 2.4760 Accuracy 0.4803\n",
      "Epoch 2 Batch 2150 Loss 2.4764 Accuracy 0.4802\n",
      "Epoch 2 Batch 2200 Loss 2.4767 Accuracy 0.4802\n",
      "Epoch 2 Batch 2250 Loss 2.4769 Accuracy 0.4802\n",
      "Epoch 2 Batch 2300 Loss 2.4769 Accuracy 0.4802\n",
      "Epoch 2 Batch 2350 Loss 2.4770 Accuracy 0.4802\n",
      "Epoch 2 Batch 2400 Loss 2.4772 Accuracy 0.4801\n",
      "Epoch 2 Batch 2450 Loss 2.4775 Accuracy 0.4801\n",
      "Epoch 2 Batch 2500 Loss 2.4781 Accuracy 0.4800\n",
      "Epoch 2 Batch 2550 Loss 2.4785 Accuracy 0.4800\n",
      "Epoch 2 Batch 2600 Loss 2.4788 Accuracy 0.4800\n",
      "Epoch 2 Loss 2.4789 Accuracy 0.4800\n",
      "Time taken for 1 epoch: 455.4414541721344 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.4731 Accuracy 0.4702\n",
      "Epoch 3 Batch 50 Loss 2.4488 Accuracy 0.4836\n",
      "Epoch 3 Batch 100 Loss 2.4425 Accuracy 0.4833\n",
      "Epoch 3 Batch 150 Loss 2.4458 Accuracy 0.4831\n",
      "Epoch 3 Batch 200 Loss 2.4517 Accuracy 0.4819\n",
      "Epoch 3 Batch 250 Loss 2.4553 Accuracy 0.4816\n",
      "Epoch 3 Batch 300 Loss 2.4558 Accuracy 0.4816\n",
      "Epoch 3 Batch 350 Loss 2.4556 Accuracy 0.4819\n",
      "Epoch 3 Batch 400 Loss 2.4572 Accuracy 0.4815\n",
      "Epoch 3 Batch 450 Loss 2.4575 Accuracy 0.4816\n",
      "Epoch 3 Batch 500 Loss 2.4584 Accuracy 0.4814\n",
      "Epoch 3 Batch 550 Loss 2.4604 Accuracy 0.4812\n",
      "Epoch 3 Batch 600 Loss 2.4631 Accuracy 0.4808\n",
      "Epoch 3 Batch 650 Loss 2.4612 Accuracy 0.4813\n",
      "Epoch 3 Batch 700 Loss 2.4614 Accuracy 0.4813\n",
      "Epoch 3 Batch 750 Loss 2.4618 Accuracy 0.4812\n",
      "Epoch 3 Batch 800 Loss 2.4622 Accuracy 0.4811\n",
      "Epoch 3 Batch 850 Loss 2.4615 Accuracy 0.4814\n",
      "Epoch 3 Batch 900 Loss 2.4615 Accuracy 0.4815\n",
      "Epoch 3 Batch 950 Loss 2.4620 Accuracy 0.4815\n",
      "Epoch 3 Batch 1000 Loss 2.4611 Accuracy 0.4817\n",
      "Epoch 3 Batch 1050 Loss 2.4606 Accuracy 0.4817\n",
      "Epoch 3 Batch 1100 Loss 2.4615 Accuracy 0.4815\n",
      "Epoch 3 Batch 1150 Loss 2.4612 Accuracy 0.4816\n",
      "Epoch 3 Batch 1200 Loss 2.4624 Accuracy 0.4814\n",
      "Epoch 3 Batch 1250 Loss 2.4635 Accuracy 0.4813\n",
      "Epoch 3 Batch 1300 Loss 2.4640 Accuracy 0.4812\n",
      "Epoch 3 Batch 1350 Loss 2.4647 Accuracy 0.4812\n",
      "Epoch 3 Batch 1400 Loss 2.4650 Accuracy 0.4812\n",
      "Epoch 3 Batch 1450 Loss 2.4650 Accuracy 0.4812\n",
      "Epoch 3 Batch 1500 Loss 2.4647 Accuracy 0.4814\n",
      "Epoch 3 Batch 1550 Loss 2.4651 Accuracy 0.4813\n",
      "Epoch 3 Batch 1600 Loss 2.4654 Accuracy 0.4813\n",
      "Epoch 3 Batch 1650 Loss 2.4657 Accuracy 0.4813\n",
      "Epoch 3 Batch 1700 Loss 2.4656 Accuracy 0.4813\n",
      "Epoch 3 Batch 1750 Loss 2.4655 Accuracy 0.4813\n",
      "Epoch 3 Batch 1800 Loss 2.4661 Accuracy 0.4812\n",
      "Epoch 3 Batch 1850 Loss 2.4656 Accuracy 0.4813\n",
      "Epoch 3 Batch 1900 Loss 2.4660 Accuracy 0.4812\n",
      "Epoch 3 Batch 1950 Loss 2.4665 Accuracy 0.4811\n",
      "Epoch 3 Batch 2000 Loss 2.4668 Accuracy 0.4811\n",
      "Epoch 3 Batch 2050 Loss 2.4671 Accuracy 0.4811\n",
      "Epoch 3 Batch 2100 Loss 2.4670 Accuracy 0.4812\n",
      "Epoch 3 Batch 2150 Loss 2.4671 Accuracy 0.4811\n",
      "Epoch 3 Batch 2200 Loss 2.4677 Accuracy 0.4810\n",
      "Epoch 3 Batch 2250 Loss 2.4685 Accuracy 0.4809\n",
      "Epoch 3 Batch 2300 Loss 2.4687 Accuracy 0.4809\n",
      "Epoch 3 Batch 2350 Loss 2.4687 Accuracy 0.4809\n",
      "Epoch 3 Batch 2400 Loss 2.4691 Accuracy 0.4808\n",
      "Epoch 3 Batch 2450 Loss 2.4691 Accuracy 0.4809\n",
      "Epoch 3 Batch 2500 Loss 2.4701 Accuracy 0.4808\n",
      "Epoch 3 Batch 2550 Loss 2.4702 Accuracy 0.4808\n",
      "Epoch 3 Batch 2600 Loss 2.4709 Accuracy 0.4807\n",
      "Epoch 3 Loss 2.4712 Accuracy 0.4807\n",
      "Time taken for 1 epoch: 455.6210653781891 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.6296 Accuracy 0.4584\n",
      "Epoch 4 Batch 50 Loss 2.4603 Accuracy 0.4800\n",
      "Epoch 4 Batch 100 Loss 2.4644 Accuracy 0.4793\n",
      "Epoch 4 Batch 150 Loss 2.4537 Accuracy 0.4813\n",
      "Epoch 4 Batch 200 Loss 2.4498 Accuracy 0.4820\n",
      "Epoch 4 Batch 250 Loss 2.4512 Accuracy 0.4820\n",
      "Epoch 4 Batch 300 Loss 2.4528 Accuracy 0.4822\n",
      "Epoch 4 Batch 350 Loss 2.4512 Accuracy 0.4825\n",
      "Epoch 4 Batch 400 Loss 2.4525 Accuracy 0.4822\n",
      "Epoch 4 Batch 450 Loss 2.4531 Accuracy 0.4821\n",
      "Epoch 4 Batch 500 Loss 2.4534 Accuracy 0.4823\n",
      "Epoch 4 Batch 550 Loss 2.4542 Accuracy 0.4822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 600 Loss 2.4545 Accuracy 0.4822\n",
      "Epoch 4 Batch 650 Loss 2.4551 Accuracy 0.4822\n",
      "Epoch 4 Batch 700 Loss 2.4568 Accuracy 0.4819\n",
      "Epoch 4 Batch 750 Loss 2.4561 Accuracy 0.4819\n",
      "Epoch 4 Batch 800 Loss 2.4565 Accuracy 0.4819\n",
      "Epoch 4 Batch 850 Loss 2.4569 Accuracy 0.4818\n",
      "Epoch 4 Batch 900 Loss 2.4569 Accuracy 0.4819\n",
      "Epoch 4 Batch 950 Loss 2.4568 Accuracy 0.4821\n",
      "Epoch 4 Batch 1000 Loss 2.4560 Accuracy 0.4822\n",
      "Epoch 4 Batch 1050 Loss 2.4567 Accuracy 0.4820\n",
      "Epoch 4 Batch 1100 Loss 2.4564 Accuracy 0.4822\n",
      "Epoch 4 Batch 1150 Loss 2.4568 Accuracy 0.4822\n",
      "Epoch 4 Batch 1200 Loss 2.4575 Accuracy 0.4821\n",
      "Epoch 4 Batch 1250 Loss 2.4578 Accuracy 0.4821\n",
      "Epoch 4 Batch 1300 Loss 2.4577 Accuracy 0.4822\n",
      "Epoch 4 Batch 1350 Loss 2.4575 Accuracy 0.4823\n",
      "Epoch 4 Batch 1400 Loss 2.4576 Accuracy 0.4822\n",
      "Epoch 4 Batch 1450 Loss 2.4582 Accuracy 0.4821\n",
      "Epoch 4 Batch 1500 Loss 2.4580 Accuracy 0.4822\n",
      "Epoch 4 Batch 1550 Loss 2.4589 Accuracy 0.4821\n",
      "Epoch 4 Batch 1600 Loss 2.4591 Accuracy 0.4821\n",
      "Epoch 4 Batch 1650 Loss 2.4589 Accuracy 0.4822\n",
      "Epoch 4 Batch 1700 Loss 2.4586 Accuracy 0.4822\n",
      "Epoch 4 Batch 1750 Loss 2.4591 Accuracy 0.4822\n",
      "Epoch 4 Batch 1800 Loss 2.4593 Accuracy 0.4822\n",
      "Epoch 4 Batch 1850 Loss 2.4594 Accuracy 0.4821\n",
      "Epoch 4 Batch 1900 Loss 2.4597 Accuracy 0.4821\n",
      "Epoch 4 Batch 1950 Loss 2.4598 Accuracy 0.4821\n",
      "Epoch 4 Batch 2000 Loss 2.4596 Accuracy 0.4821\n",
      "Epoch 4 Batch 2050 Loss 2.4600 Accuracy 0.4821\n",
      "Epoch 4 Batch 2100 Loss 2.4602 Accuracy 0.4820\n",
      "Epoch 4 Batch 2150 Loss 2.4609 Accuracy 0.4819\n",
      "Epoch 4 Batch 2200 Loss 2.4611 Accuracy 0.4819\n",
      "Epoch 4 Batch 2250 Loss 2.4614 Accuracy 0.4818\n",
      "Epoch 4 Batch 2300 Loss 2.4613 Accuracy 0.4819\n",
      "Epoch 4 Batch 2350 Loss 2.4617 Accuracy 0.4818\n",
      "Epoch 4 Batch 2400 Loss 2.4613 Accuracy 0.4819\n",
      "Epoch 4 Batch 2450 Loss 2.4616 Accuracy 0.4819\n",
      "Epoch 4 Batch 2500 Loss 2.4622 Accuracy 0.4818\n",
      "Epoch 4 Batch 2550 Loss 2.4624 Accuracy 0.4817\n",
      "Epoch 4 Batch 2600 Loss 2.4633 Accuracy 0.4816\n",
      "Epoch 4 Loss 2.4635 Accuracy 0.4816\n",
      "Time taken for 1 epoch: 455.3832046985626 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.4272 Accuracy 0.4793\n",
      "Epoch 5 Batch 50 Loss 2.4365 Accuracy 0.4851\n",
      "Epoch 5 Batch 100 Loss 2.4373 Accuracy 0.4849\n",
      "Epoch 5 Batch 150 Loss 2.4440 Accuracy 0.4842\n",
      "Epoch 5 Batch 200 Loss 2.4394 Accuracy 0.4849\n",
      "Epoch 5 Batch 250 Loss 2.4397 Accuracy 0.4847\n",
      "Epoch 5 Batch 300 Loss 2.4408 Accuracy 0.4847\n",
      "Epoch 5 Batch 350 Loss 2.4425 Accuracy 0.4843\n",
      "Epoch 5 Batch 400 Loss 2.4423 Accuracy 0.4843\n",
      "Epoch 5 Batch 450 Loss 2.4439 Accuracy 0.4840\n",
      "Epoch 5 Batch 500 Loss 2.4448 Accuracy 0.4840\n",
      "Epoch 5 Batch 550 Loss 2.4461 Accuracy 0.4837\n",
      "Epoch 5 Batch 600 Loss 2.4457 Accuracy 0.4838\n",
      "Epoch 5 Batch 650 Loss 2.4460 Accuracy 0.4838\n",
      "Epoch 5 Batch 700 Loss 2.4451 Accuracy 0.4838\n",
      "Epoch 5 Batch 750 Loss 2.4468 Accuracy 0.4836\n",
      "Epoch 5 Batch 800 Loss 2.4475 Accuracy 0.4835\n",
      "Epoch 5 Batch 850 Loss 2.4472 Accuracy 0.4834\n",
      "Epoch 5 Batch 900 Loss 2.4478 Accuracy 0.4834\n",
      "Epoch 5 Batch 950 Loss 2.4476 Accuracy 0.4835\n",
      "Epoch 5 Batch 1000 Loss 2.4479 Accuracy 0.4835\n",
      "Epoch 5 Batch 1050 Loss 2.4488 Accuracy 0.4834\n",
      "Epoch 5 Batch 1100 Loss 2.4480 Accuracy 0.4835\n",
      "Epoch 5 Batch 1150 Loss 2.4489 Accuracy 0.4834\n",
      "Epoch 5 Batch 1200 Loss 2.4496 Accuracy 0.4833\n",
      "Epoch 5 Batch 1250 Loss 2.4495 Accuracy 0.4834\n",
      "Epoch 5 Batch 1300 Loss 2.4496 Accuracy 0.4834\n",
      "Epoch 5 Batch 1350 Loss 2.4497 Accuracy 0.4835\n",
      "Epoch 5 Batch 1400 Loss 2.4497 Accuracy 0.4835\n",
      "Epoch 5 Batch 1450 Loss 2.4504 Accuracy 0.4834\n",
      "Epoch 5 Batch 1500 Loss 2.4508 Accuracy 0.4833\n",
      "Epoch 5 Batch 1550 Loss 2.4511 Accuracy 0.4833\n",
      "Epoch 5 Batch 1600 Loss 2.4511 Accuracy 0.4833\n",
      "Epoch 5 Batch 1650 Loss 2.4510 Accuracy 0.4834\n",
      "Epoch 5 Batch 1700 Loss 2.4517 Accuracy 0.4833\n",
      "Epoch 5 Batch 1750 Loss 2.4520 Accuracy 0.4833\n",
      "Epoch 5 Batch 1800 Loss 2.4523 Accuracy 0.4833\n",
      "Epoch 5 Batch 1850 Loss 2.4527 Accuracy 0.4832\n",
      "Epoch 5 Batch 1900 Loss 2.4525 Accuracy 0.4833\n",
      "Epoch 5 Batch 1950 Loss 2.4529 Accuracy 0.4832\n",
      "Epoch 5 Batch 2000 Loss 2.4529 Accuracy 0.4833\n",
      "Epoch 5 Batch 2050 Loss 2.4525 Accuracy 0.4833\n",
      "Epoch 5 Batch 2100 Loss 2.4527 Accuracy 0.4832\n",
      "Epoch 5 Batch 2150 Loss 2.4533 Accuracy 0.4832\n",
      "Epoch 5 Batch 2200 Loss 2.4534 Accuracy 0.4832\n",
      "Epoch 5 Batch 2250 Loss 2.4532 Accuracy 0.4832\n",
      "Epoch 5 Batch 2300 Loss 2.4534 Accuracy 0.4831\n",
      "Epoch 5 Batch 2350 Loss 2.4534 Accuracy 0.4831\n",
      "Epoch 5 Batch 2400 Loss 2.4539 Accuracy 0.4831\n",
      "Epoch 5 Batch 2450 Loss 2.4539 Accuracy 0.4832\n",
      "Epoch 5 Batch 2500 Loss 2.4547 Accuracy 0.4831\n",
      "Epoch 5 Batch 2550 Loss 2.4552 Accuracy 0.4830\n",
      "Epoch 5 Batch 2600 Loss 2.4555 Accuracy 0.4830\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train_full/ckpt-11\n",
      "Epoch 5 Loss 2.4556 Accuracy 0.4830\n",
      "Time taken for 1 epoch: 456.1930022239685 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.4589 Accuracy 0.4996\n",
      "Epoch 6 Batch 50 Loss 2.4494 Accuracy 0.4816\n",
      "Epoch 6 Batch 100 Loss 2.4439 Accuracy 0.4828\n",
      "Epoch 6 Batch 150 Loss 2.4384 Accuracy 0.4833\n",
      "Epoch 6 Batch 200 Loss 2.4387 Accuracy 0.4837\n",
      "Epoch 6 Batch 250 Loss 2.4399 Accuracy 0.4834\n",
      "Epoch 6 Batch 300 Loss 2.4359 Accuracy 0.4837\n",
      "Epoch 6 Batch 350 Loss 2.4357 Accuracy 0.4840\n",
      "Epoch 6 Batch 400 Loss 2.4339 Accuracy 0.4843\n",
      "Epoch 6 Batch 450 Loss 2.4341 Accuracy 0.4844\n",
      "Epoch 6 Batch 500 Loss 2.4349 Accuracy 0.4845\n",
      "Epoch 6 Batch 550 Loss 2.4369 Accuracy 0.4843\n",
      "Epoch 6 Batch 600 Loss 2.4375 Accuracy 0.4845\n",
      "Epoch 6 Batch 650 Loss 2.4388 Accuracy 0.4844\n",
      "Epoch 6 Batch 700 Loss 2.4394 Accuracy 0.4845\n",
      "Epoch 6 Batch 750 Loss 2.4397 Accuracy 0.4844\n",
      "Epoch 6 Batch 800 Loss 2.4395 Accuracy 0.4845\n",
      "Epoch 6 Batch 850 Loss 2.4382 Accuracy 0.4847\n",
      "Epoch 6 Batch 900 Loss 2.4386 Accuracy 0.4848\n",
      "Epoch 6 Batch 950 Loss 2.4400 Accuracy 0.4846\n",
      "Epoch 6 Batch 1000 Loss 2.4401 Accuracy 0.4845\n",
      "Epoch 6 Batch 1050 Loss 2.4408 Accuracy 0.4844\n",
      "Epoch 6 Batch 1100 Loss 2.4412 Accuracy 0.4843\n",
      "Epoch 6 Batch 1150 Loss 2.4409 Accuracy 0.4844\n",
      "Epoch 6 Batch 1200 Loss 2.4414 Accuracy 0.4843\n",
      "Epoch 6 Batch 1250 Loss 2.4408 Accuracy 0.4845\n",
      "Epoch 6 Batch 1300 Loss 2.4417 Accuracy 0.4844\n",
      "Epoch 6 Batch 1350 Loss 2.4413 Accuracy 0.4845\n",
      "Epoch 6 Batch 1400 Loss 2.4416 Accuracy 0.4845\n",
      "Epoch 6 Batch 1450 Loss 2.4418 Accuracy 0.4844\n",
      "Epoch 6 Batch 1500 Loss 2.4418 Accuracy 0.4844\n",
      "Epoch 6 Batch 1550 Loss 2.4418 Accuracy 0.4846\n",
      "Epoch 6 Batch 1600 Loss 2.4425 Accuracy 0.4844\n",
      "Epoch 6 Batch 1650 Loss 2.4425 Accuracy 0.4845\n",
      "Epoch 6 Batch 1700 Loss 2.4431 Accuracy 0.4843\n",
      "Epoch 6 Batch 1750 Loss 2.4435 Accuracy 0.4843\n",
      "Epoch 6 Batch 1800 Loss 2.4438 Accuracy 0.4842\n",
      "Epoch 6 Batch 1850 Loss 2.4435 Accuracy 0.4842\n",
      "Epoch 6 Batch 1900 Loss 2.4438 Accuracy 0.4842\n",
      "Epoch 6 Batch 1950 Loss 2.4443 Accuracy 0.4842\n",
      "Epoch 6 Batch 2000 Loss 2.4443 Accuracy 0.4841\n",
      "Epoch 6 Batch 2050 Loss 2.4449 Accuracy 0.4841\n",
      "Epoch 6 Batch 2100 Loss 2.4451 Accuracy 0.4841\n",
      "Epoch 6 Batch 2150 Loss 2.4454 Accuracy 0.4840\n",
      "Epoch 6 Batch 2200 Loss 2.4454 Accuracy 0.4840\n",
      "Epoch 6 Batch 2250 Loss 2.4457 Accuracy 0.4839\n",
      "Epoch 6 Batch 2300 Loss 2.4461 Accuracy 0.4839\n",
      "Epoch 6 Batch 2350 Loss 2.4460 Accuracy 0.4839\n",
      "Epoch 6 Batch 2400 Loss 2.4461 Accuracy 0.4838\n",
      "Epoch 6 Batch 2450 Loss 2.4464 Accuracy 0.4838\n",
      "Epoch 6 Batch 2500 Loss 2.4466 Accuracy 0.4838\n",
      "Epoch 6 Batch 2550 Loss 2.4469 Accuracy 0.4838\n",
      "Epoch 6 Batch 2600 Loss 2.4475 Accuracy 0.4837\n",
      "Epoch 6 Loss 2.4478 Accuracy 0.4837\n",
      "Time taken for 1 epoch: 457.7829518318176 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.4342 Accuracy 0.4825\n",
      "Epoch 7 Batch 50 Loss 2.3934 Accuracy 0.4920\n",
      "Epoch 7 Batch 100 Loss 2.4082 Accuracy 0.4899\n",
      "Epoch 7 Batch 150 Loss 2.4137 Accuracy 0.4883\n",
      "Epoch 7 Batch 200 Loss 2.4170 Accuracy 0.4878\n",
      "Epoch 7 Batch 250 Loss 2.4194 Accuracy 0.4873\n",
      "Epoch 7 Batch 300 Loss 2.4230 Accuracy 0.4867\n",
      "Epoch 7 Batch 350 Loss 2.4282 Accuracy 0.4861\n",
      "Epoch 7 Batch 400 Loss 2.4301 Accuracy 0.4858\n",
      "Epoch 7 Batch 450 Loss 2.4278 Accuracy 0.4863\n",
      "Epoch 7 Batch 500 Loss 2.4285 Accuracy 0.4861\n",
      "Epoch 7 Batch 550 Loss 2.4292 Accuracy 0.4860\n",
      "Epoch 7 Batch 600 Loss 2.4300 Accuracy 0.4858\n",
      "Epoch 7 Batch 650 Loss 2.4307 Accuracy 0.4857\n",
      "Epoch 7 Batch 700 Loss 2.4320 Accuracy 0.4855\n",
      "Epoch 7 Batch 750 Loss 2.4330 Accuracy 0.4855\n",
      "Epoch 7 Batch 800 Loss 2.4330 Accuracy 0.4856\n",
      "Epoch 7 Batch 850 Loss 2.4325 Accuracy 0.4857\n",
      "Epoch 7 Batch 900 Loss 2.4323 Accuracy 0.4858\n",
      "Epoch 7 Batch 950 Loss 2.4321 Accuracy 0.4858\n",
      "Epoch 7 Batch 1000 Loss 2.4333 Accuracy 0.4857\n",
      "Epoch 7 Batch 1050 Loss 2.4340 Accuracy 0.4856\n",
      "Epoch 7 Batch 1100 Loss 2.4344 Accuracy 0.4855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 1150 Loss 2.4344 Accuracy 0.4855\n",
      "Epoch 7 Batch 1200 Loss 2.4340 Accuracy 0.4857\n",
      "Epoch 7 Batch 1250 Loss 2.4336 Accuracy 0.4858\n",
      "Epoch 7 Batch 1300 Loss 2.4343 Accuracy 0.4857\n",
      "Epoch 7 Batch 1350 Loss 2.4339 Accuracy 0.4858\n",
      "Epoch 7 Batch 1400 Loss 2.4348 Accuracy 0.4856\n",
      "Epoch 7 Batch 1450 Loss 2.4352 Accuracy 0.4855\n",
      "Epoch 7 Batch 1500 Loss 2.4352 Accuracy 0.4855\n",
      "Epoch 7 Batch 1550 Loss 2.4353 Accuracy 0.4854\n",
      "Epoch 7 Batch 1600 Loss 2.4359 Accuracy 0.4853\n",
      "Epoch 7 Batch 1650 Loss 2.4354 Accuracy 0.4854\n",
      "Epoch 7 Batch 1700 Loss 2.4361 Accuracy 0.4854\n",
      "Epoch 7 Batch 1750 Loss 2.4366 Accuracy 0.4854\n",
      "Epoch 7 Batch 1800 Loss 2.4363 Accuracy 0.4855\n",
      "Epoch 7 Batch 1850 Loss 2.4361 Accuracy 0.4855\n",
      "Epoch 7 Batch 1900 Loss 2.4362 Accuracy 0.4855\n",
      "Epoch 7 Batch 1950 Loss 2.4362 Accuracy 0.4855\n",
      "Epoch 7 Batch 2000 Loss 2.4367 Accuracy 0.4855\n",
      "Epoch 7 Batch 2050 Loss 2.4372 Accuracy 0.4854\n",
      "Epoch 7 Batch 2100 Loss 2.4376 Accuracy 0.4854\n",
      "Epoch 7 Batch 2150 Loss 2.4383 Accuracy 0.4853\n",
      "Epoch 7 Batch 2200 Loss 2.4384 Accuracy 0.4852\n",
      "Epoch 7 Batch 2250 Loss 2.4391 Accuracy 0.4851\n",
      "Epoch 7 Batch 2300 Loss 2.4393 Accuracy 0.4850\n",
      "Epoch 7 Batch 2350 Loss 2.4396 Accuracy 0.4850\n",
      "Epoch 7 Batch 2400 Loss 2.4399 Accuracy 0.4849\n",
      "Epoch 7 Batch 2450 Loss 2.4400 Accuracy 0.4849\n",
      "Epoch 7 Batch 2500 Loss 2.4402 Accuracy 0.4849\n",
      "Epoch 7 Batch 2550 Loss 2.4405 Accuracy 0.4848\n",
      "Epoch 7 Batch 2600 Loss 2.4408 Accuracy 0.4848\n",
      "Epoch 7 Loss 2.4412 Accuracy 0.4847\n",
      "Time taken for 1 epoch: 455.5479793548584 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.5268 Accuracy 0.4608\n",
      "Epoch 8 Batch 50 Loss 2.4267 Accuracy 0.4862\n",
      "Epoch 8 Batch 100 Loss 2.4147 Accuracy 0.4881\n",
      "Epoch 8 Batch 150 Loss 2.4154 Accuracy 0.4878\n",
      "Epoch 8 Batch 200 Loss 2.4118 Accuracy 0.4881\n",
      "Epoch 8 Batch 250 Loss 2.4151 Accuracy 0.4875\n",
      "Epoch 8 Batch 300 Loss 2.4132 Accuracy 0.4880\n",
      "Epoch 8 Batch 350 Loss 2.4157 Accuracy 0.4876\n",
      "Epoch 8 Batch 400 Loss 2.4177 Accuracy 0.4874\n",
      "Epoch 8 Batch 450 Loss 2.4180 Accuracy 0.4872\n",
      "Epoch 8 Batch 500 Loss 2.4181 Accuracy 0.4874\n",
      "Epoch 8 Batch 550 Loss 2.4202 Accuracy 0.4870\n",
      "Epoch 8 Batch 600 Loss 2.4223 Accuracy 0.4868\n",
      "Epoch 8 Batch 650 Loss 2.4238 Accuracy 0.4867\n",
      "Epoch 8 Batch 700 Loss 2.4256 Accuracy 0.4864\n",
      "Epoch 8 Batch 750 Loss 2.4249 Accuracy 0.4866\n",
      "Epoch 8 Batch 800 Loss 2.4242 Accuracy 0.4866\n",
      "Epoch 8 Batch 850 Loss 2.4259 Accuracy 0.4864\n",
      "Epoch 8 Batch 900 Loss 2.4267 Accuracy 0.4863\n",
      "Epoch 8 Batch 950 Loss 2.4254 Accuracy 0.4866\n",
      "Epoch 8 Batch 1000 Loss 2.4252 Accuracy 0.4866\n",
      "Epoch 8 Batch 1050 Loss 2.4263 Accuracy 0.4864\n",
      "Epoch 8 Batch 1100 Loss 2.4259 Accuracy 0.4865\n",
      "Epoch 8 Batch 1150 Loss 2.4256 Accuracy 0.4865\n",
      "Epoch 8 Batch 1200 Loss 2.4259 Accuracy 0.4866\n",
      "Epoch 8 Batch 1250 Loss 2.4267 Accuracy 0.4865\n",
      "Epoch 8 Batch 1300 Loss 2.4273 Accuracy 0.4865\n",
      "Epoch 8 Batch 1350 Loss 2.4274 Accuracy 0.4866\n",
      "Epoch 8 Batch 1400 Loss 2.4272 Accuracy 0.4866\n",
      "Epoch 8 Batch 1450 Loss 2.4278 Accuracy 0.4865\n",
      "Epoch 8 Batch 1500 Loss 2.4281 Accuracy 0.4865\n",
      "Epoch 8 Batch 1550 Loss 2.4288 Accuracy 0.4864\n",
      "Epoch 8 Batch 1600 Loss 2.4285 Accuracy 0.4865\n",
      "Epoch 8 Batch 1650 Loss 2.4290 Accuracy 0.4865\n",
      "Epoch 8 Batch 1700 Loss 2.4289 Accuracy 0.4865\n",
      "Epoch 8 Batch 1750 Loss 2.4294 Accuracy 0.4865\n",
      "Epoch 8 Batch 1800 Loss 2.4296 Accuracy 0.4864\n",
      "Epoch 8 Batch 1850 Loss 2.4297 Accuracy 0.4863\n",
      "Epoch 8 Batch 1900 Loss 2.4302 Accuracy 0.4863\n",
      "Epoch 8 Batch 1950 Loss 2.4308 Accuracy 0.4862\n",
      "Epoch 8 Batch 2000 Loss 2.4308 Accuracy 0.4862\n",
      "Epoch 8 Batch 2050 Loss 2.4313 Accuracy 0.4861\n",
      "Epoch 8 Batch 2100 Loss 2.4311 Accuracy 0.4861\n",
      "Epoch 8 Batch 2150 Loss 2.4312 Accuracy 0.4861\n",
      "Epoch 8 Batch 2200 Loss 2.4314 Accuracy 0.4861\n",
      "Epoch 8 Batch 2250 Loss 2.4318 Accuracy 0.4860\n",
      "Epoch 8 Batch 2300 Loss 2.4317 Accuracy 0.4860\n",
      "Epoch 8 Batch 2350 Loss 2.4320 Accuracy 0.4860\n",
      "Epoch 8 Batch 2400 Loss 2.4323 Accuracy 0.4860\n",
      "Epoch 8 Batch 2450 Loss 2.4328 Accuracy 0.4859\n",
      "Epoch 8 Batch 2500 Loss 2.4326 Accuracy 0.4860\n",
      "Epoch 8 Batch 2550 Loss 2.4331 Accuracy 0.4859\n",
      "Epoch 8 Batch 2600 Loss 2.4338 Accuracy 0.4858\n",
      "Epoch 8 Loss 2.4338 Accuracy 0.4858\n",
      "Time taken for 1 epoch: 459.3503828048706 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.4291 Accuracy 0.4790\n",
      "Epoch 9 Batch 50 Loss 2.4118 Accuracy 0.4901\n",
      "Epoch 9 Batch 100 Loss 2.4076 Accuracy 0.4895\n",
      "Epoch 9 Batch 150 Loss 2.4139 Accuracy 0.4879\n",
      "Epoch 9 Batch 200 Loss 2.4185 Accuracy 0.4873\n",
      "Epoch 9 Batch 250 Loss 2.4176 Accuracy 0.4876\n",
      "Epoch 9 Batch 300 Loss 2.4162 Accuracy 0.4879\n",
      "Epoch 9 Batch 350 Loss 2.4170 Accuracy 0.4877\n",
      "Epoch 9 Batch 400 Loss 2.4188 Accuracy 0.4873\n",
      "Epoch 9 Batch 450 Loss 2.4158 Accuracy 0.4877\n",
      "Epoch 9 Batch 500 Loss 2.4153 Accuracy 0.4879\n",
      "Epoch 9 Batch 550 Loss 2.4173 Accuracy 0.4878\n",
      "Epoch 9 Batch 600 Loss 2.4174 Accuracy 0.4878\n",
      "Epoch 9 Batch 650 Loss 2.4181 Accuracy 0.4876\n",
      "Epoch 9 Batch 700 Loss 2.4194 Accuracy 0.4874\n",
      "Epoch 9 Batch 750 Loss 2.4185 Accuracy 0.4877\n",
      "Epoch 9 Batch 800 Loss 2.4176 Accuracy 0.4878\n",
      "Epoch 9 Batch 850 Loss 2.4183 Accuracy 0.4877\n",
      "Epoch 9 Batch 900 Loss 2.4177 Accuracy 0.4877\n",
      "Epoch 9 Batch 950 Loss 2.4182 Accuracy 0.4877\n",
      "Epoch 9 Batch 1000 Loss 2.4181 Accuracy 0.4878\n",
      "Epoch 9 Batch 1050 Loss 2.4185 Accuracy 0.4878\n",
      "Epoch 9 Batch 1100 Loss 2.4189 Accuracy 0.4878\n",
      "Epoch 9 Batch 1150 Loss 2.4194 Accuracy 0.4877\n",
      "Epoch 9 Batch 1200 Loss 2.4198 Accuracy 0.4878\n",
      "Epoch 9 Batch 1250 Loss 2.4203 Accuracy 0.4876\n",
      "Epoch 9 Batch 1300 Loss 2.4202 Accuracy 0.4877\n",
      "Epoch 9 Batch 1350 Loss 2.4206 Accuracy 0.4876\n",
      "Epoch 9 Batch 1400 Loss 2.4210 Accuracy 0.4877\n",
      "Epoch 9 Batch 1450 Loss 2.4212 Accuracy 0.4877\n",
      "Epoch 9 Batch 1500 Loss 2.4213 Accuracy 0.4876\n",
      "Epoch 9 Batch 1550 Loss 2.4222 Accuracy 0.4873\n",
      "Epoch 9 Batch 1600 Loss 2.4217 Accuracy 0.4875\n",
      "Epoch 9 Batch 1650 Loss 2.4223 Accuracy 0.4874\n",
      "Epoch 9 Batch 1700 Loss 2.4228 Accuracy 0.4874\n",
      "Epoch 9 Batch 1750 Loss 2.4235 Accuracy 0.4872\n",
      "Epoch 9 Batch 1800 Loss 2.4236 Accuracy 0.4872\n",
      "Epoch 9 Batch 1850 Loss 2.4238 Accuracy 0.4872\n",
      "Epoch 9 Batch 1900 Loss 2.4239 Accuracy 0.4872\n",
      "Epoch 9 Batch 1950 Loss 2.4245 Accuracy 0.4870\n",
      "Epoch 9 Batch 2000 Loss 2.4247 Accuracy 0.4870\n",
      "Epoch 9 Batch 2050 Loss 2.4251 Accuracy 0.4869\n",
      "Epoch 9 Batch 2100 Loss 2.4255 Accuracy 0.4868\n",
      "Epoch 9 Batch 2150 Loss 2.4259 Accuracy 0.4868\n",
      "Epoch 9 Batch 2200 Loss 2.4259 Accuracy 0.4868\n",
      "Epoch 9 Batch 2250 Loss 2.4260 Accuracy 0.4868\n",
      "Epoch 9 Batch 2300 Loss 2.4261 Accuracy 0.4868\n",
      "Epoch 9 Batch 2350 Loss 2.4258 Accuracy 0.4868\n",
      "Epoch 9 Batch 2400 Loss 2.4263 Accuracy 0.4867\n",
      "Epoch 9 Batch 2450 Loss 2.4266 Accuracy 0.4867\n",
      "Epoch 9 Batch 2500 Loss 2.4269 Accuracy 0.4867\n",
      "Epoch 9 Batch 2550 Loss 2.4267 Accuracy 0.4867\n",
      "Epoch 9 Batch 2600 Loss 2.4270 Accuracy 0.4867\n",
      "Epoch 9 Loss 2.4270 Accuracy 0.4867\n",
      "Time taken for 1 epoch: 455.8027777671814 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 2.4700 Accuracy 0.4724\n",
      "Epoch 10 Batch 50 Loss 2.4106 Accuracy 0.4889\n",
      "Epoch 10 Batch 100 Loss 2.4117 Accuracy 0.4883\n",
      "Epoch 10 Batch 150 Loss 2.4032 Accuracy 0.4886\n",
      "Epoch 10 Batch 200 Loss 2.4022 Accuracy 0.4891\n",
      "Epoch 10 Batch 250 Loss 2.4057 Accuracy 0.4886\n",
      "Epoch 10 Batch 300 Loss 2.4090 Accuracy 0.4883\n",
      "Epoch 10 Batch 350 Loss 2.4067 Accuracy 0.4888\n",
      "Epoch 10 Batch 400 Loss 2.4069 Accuracy 0.4889\n",
      "Epoch 10 Batch 450 Loss 2.4082 Accuracy 0.4888\n",
      "Epoch 10 Batch 500 Loss 2.4092 Accuracy 0.4887\n",
      "Epoch 10 Batch 550 Loss 2.4088 Accuracy 0.4890\n",
      "Epoch 10 Batch 600 Loss 2.4088 Accuracy 0.4890\n",
      "Epoch 10 Batch 650 Loss 2.4094 Accuracy 0.4890\n",
      "Epoch 10 Batch 700 Loss 2.4098 Accuracy 0.4888\n",
      "Epoch 10 Batch 750 Loss 2.4098 Accuracy 0.4887\n",
      "Epoch 10 Batch 800 Loss 2.4102 Accuracy 0.4886\n",
      "Epoch 10 Batch 850 Loss 2.4111 Accuracy 0.4885\n",
      "Epoch 10 Batch 900 Loss 2.4118 Accuracy 0.4884\n",
      "Epoch 10 Batch 950 Loss 2.4125 Accuracy 0.4883\n",
      "Epoch 10 Batch 1000 Loss 2.4132 Accuracy 0.4882\n",
      "Epoch 10 Batch 1050 Loss 2.4128 Accuracy 0.4883\n",
      "Epoch 10 Batch 1100 Loss 2.4131 Accuracy 0.4883\n",
      "Epoch 10 Batch 1150 Loss 2.4139 Accuracy 0.4882\n",
      "Epoch 10 Batch 1200 Loss 2.4141 Accuracy 0.4882\n",
      "Epoch 10 Batch 1250 Loss 2.4142 Accuracy 0.4882\n",
      "Epoch 10 Batch 1300 Loss 2.4147 Accuracy 0.4881\n",
      "Epoch 10 Batch 1350 Loss 2.4147 Accuracy 0.4882\n",
      "Epoch 10 Batch 1400 Loss 2.4152 Accuracy 0.4881\n",
      "Epoch 10 Batch 1450 Loss 2.4154 Accuracy 0.4881\n",
      "Epoch 10 Batch 1500 Loss 2.4149 Accuracy 0.4882\n",
      "Epoch 10 Batch 1550 Loss 2.4150 Accuracy 0.4882\n",
      "Epoch 10 Batch 1600 Loss 2.4152 Accuracy 0.4882\n",
      "Epoch 10 Batch 1650 Loss 2.4153 Accuracy 0.4882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 1700 Loss 2.4156 Accuracy 0.4882\n",
      "Epoch 10 Batch 1750 Loss 2.4157 Accuracy 0.4883\n",
      "Epoch 10 Batch 1800 Loss 2.4158 Accuracy 0.4883\n",
      "Epoch 10 Batch 1850 Loss 2.4164 Accuracy 0.4883\n",
      "Epoch 10 Batch 1900 Loss 2.4171 Accuracy 0.4882\n",
      "Epoch 10 Batch 1950 Loss 2.4172 Accuracy 0.4881\n",
      "Epoch 10 Batch 2000 Loss 2.4177 Accuracy 0.4880\n",
      "Epoch 10 Batch 2050 Loss 2.4181 Accuracy 0.4879\n",
      "Epoch 10 Batch 2100 Loss 2.4184 Accuracy 0.4879\n",
      "Epoch 10 Batch 2150 Loss 2.4183 Accuracy 0.4880\n",
      "Epoch 10 Batch 2200 Loss 2.4185 Accuracy 0.4879\n",
      "Epoch 10 Batch 2250 Loss 2.4187 Accuracy 0.4879\n",
      "Epoch 10 Batch 2300 Loss 2.4193 Accuracy 0.4878\n",
      "Epoch 10 Batch 2350 Loss 2.4194 Accuracy 0.4878\n",
      "Epoch 10 Batch 2400 Loss 2.4197 Accuracy 0.4878\n",
      "Epoch 10 Batch 2450 Loss 2.4195 Accuracy 0.4878\n",
      "Epoch 10 Batch 2500 Loss 2.4197 Accuracy 0.4878\n",
      "Epoch 10 Batch 2550 Loss 2.4204 Accuracy 0.4877\n",
      "Epoch 10 Batch 2600 Loss 2.4203 Accuracy 0.4877\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train_full/ckpt-12\n",
      "Epoch 10 Loss 2.4205 Accuracy 0.4877\n",
      "Time taken for 1 epoch: 475.0865457057953 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 2.3555 Accuracy 0.5109\n",
      "Epoch 11 Batch 50 Loss 2.4098 Accuracy 0.4859\n",
      "Epoch 11 Batch 100 Loss 2.3992 Accuracy 0.4876\n",
      "Epoch 11 Batch 150 Loss 2.3984 Accuracy 0.4885\n",
      "Epoch 11 Batch 200 Loss 2.3951 Accuracy 0.4887\n",
      "Epoch 11 Batch 250 Loss 2.3935 Accuracy 0.4897\n",
      "Epoch 11 Batch 300 Loss 2.3957 Accuracy 0.4891\n",
      "Epoch 11 Batch 350 Loss 2.3968 Accuracy 0.4891\n",
      "Epoch 11 Batch 400 Loss 2.3976 Accuracy 0.4892\n",
      "Epoch 11 Batch 450 Loss 2.3988 Accuracy 0.4894\n",
      "Epoch 11 Batch 500 Loss 2.3997 Accuracy 0.4897\n",
      "Epoch 11 Batch 550 Loss 2.3995 Accuracy 0.4896\n",
      "Epoch 11 Batch 600 Loss 2.3992 Accuracy 0.4898\n",
      "Epoch 11 Batch 650 Loss 2.4008 Accuracy 0.4895\n",
      "Epoch 11 Batch 700 Loss 2.4022 Accuracy 0.4895\n",
      "Epoch 11 Batch 750 Loss 2.4023 Accuracy 0.4894\n",
      "Epoch 11 Batch 800 Loss 2.4028 Accuracy 0.4893\n",
      "Epoch 11 Batch 850 Loss 2.4031 Accuracy 0.4893\n",
      "Epoch 11 Batch 900 Loss 2.4039 Accuracy 0.4893\n",
      "Epoch 11 Batch 950 Loss 2.4030 Accuracy 0.4895\n",
      "Epoch 11 Batch 1000 Loss 2.4040 Accuracy 0.4894\n",
      "Epoch 11 Batch 1050 Loss 2.4054 Accuracy 0.4892\n",
      "Epoch 11 Batch 1100 Loss 2.4056 Accuracy 0.4891\n",
      "Epoch 11 Batch 1150 Loss 2.4057 Accuracy 0.4892\n",
      "Epoch 11 Batch 1200 Loss 2.4062 Accuracy 0.4891\n",
      "Epoch 11 Batch 1250 Loss 2.4069 Accuracy 0.4890\n",
      "Epoch 11 Batch 1300 Loss 2.4072 Accuracy 0.4890\n",
      "Epoch 11 Batch 1350 Loss 2.4075 Accuracy 0.4890\n",
      "Epoch 11 Batch 1400 Loss 2.4078 Accuracy 0.4890\n",
      "Epoch 11 Batch 1450 Loss 2.4078 Accuracy 0.4890\n",
      "Epoch 11 Batch 1500 Loss 2.4082 Accuracy 0.4890\n",
      "Epoch 11 Batch 1550 Loss 2.4091 Accuracy 0.4888\n",
      "Epoch 11 Batch 1600 Loss 2.4086 Accuracy 0.4889\n",
      "Epoch 11 Batch 1650 Loss 2.4088 Accuracy 0.4889\n",
      "Epoch 11 Batch 1700 Loss 2.4093 Accuracy 0.4889\n",
      "Epoch 11 Batch 1750 Loss 2.4093 Accuracy 0.4890\n",
      "Epoch 11 Batch 1800 Loss 2.4095 Accuracy 0.4890\n",
      "Epoch 11 Batch 1850 Loss 2.4096 Accuracy 0.4889\n",
      "Epoch 11 Batch 1900 Loss 2.4099 Accuracy 0.4889\n",
      "Epoch 11 Batch 1950 Loss 2.4097 Accuracy 0.4889\n",
      "Epoch 11 Batch 2000 Loss 2.4099 Accuracy 0.4889\n",
      "Epoch 11 Batch 2050 Loss 2.4098 Accuracy 0.4890\n",
      "Epoch 11 Batch 2100 Loss 2.4100 Accuracy 0.4889\n",
      "Epoch 11 Batch 2150 Loss 2.4103 Accuracy 0.4889\n",
      "Epoch 11 Batch 2200 Loss 2.4109 Accuracy 0.4888\n",
      "Epoch 11 Batch 2250 Loss 2.4113 Accuracy 0.4887\n",
      "Epoch 11 Batch 2300 Loss 2.4116 Accuracy 0.4887\n",
      "Epoch 11 Batch 2350 Loss 2.4114 Accuracy 0.4887\n",
      "Epoch 11 Batch 2400 Loss 2.4117 Accuracy 0.4886\n",
      "Epoch 11 Batch 2450 Loss 2.4122 Accuracy 0.4886\n",
      "Epoch 11 Batch 2500 Loss 2.4123 Accuracy 0.4886\n",
      "Epoch 11 Batch 2550 Loss 2.4128 Accuracy 0.4885\n",
      "Epoch 11 Batch 2600 Loss 2.4130 Accuracy 0.4885\n",
      "Epoch 11 Loss 2.4133 Accuracy 0.4884\n",
      "Time taken for 1 epoch: 466.0353488922119 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 2.2823 Accuracy 0.5220\n",
      "Epoch 12 Batch 50 Loss 2.3882 Accuracy 0.4907\n",
      "Epoch 12 Batch 100 Loss 2.3861 Accuracy 0.4915\n",
      "Epoch 12 Batch 150 Loss 2.3856 Accuracy 0.4914\n",
      "Epoch 12 Batch 200 Loss 2.3900 Accuracy 0.4912\n",
      "Epoch 12 Batch 250 Loss 2.3882 Accuracy 0.4915\n",
      "Epoch 12 Batch 300 Loss 2.3882 Accuracy 0.4918\n",
      "Epoch 12 Batch 350 Loss 2.3909 Accuracy 0.4914\n",
      "Epoch 12 Batch 400 Loss 2.3921 Accuracy 0.4912\n",
      "Epoch 12 Batch 450 Loss 2.3922 Accuracy 0.4914\n",
      "Epoch 12 Batch 500 Loss 2.3926 Accuracy 0.4912\n",
      "Epoch 12 Batch 550 Loss 2.3948 Accuracy 0.4911\n",
      "Epoch 12 Batch 600 Loss 2.3959 Accuracy 0.4909\n",
      "Epoch 12 Batch 650 Loss 2.3951 Accuracy 0.4911\n",
      "Epoch 12 Batch 700 Loss 2.3966 Accuracy 0.4908\n",
      "Epoch 12 Batch 750 Loss 2.3975 Accuracy 0.4908\n",
      "Epoch 12 Batch 800 Loss 2.3983 Accuracy 0.4907\n",
      "Epoch 12 Batch 850 Loss 2.3977 Accuracy 0.4908\n",
      "Epoch 12 Batch 900 Loss 2.3993 Accuracy 0.4905\n",
      "Epoch 12 Batch 950 Loss 2.3994 Accuracy 0.4905\n",
      "Epoch 12 Batch 1000 Loss 2.3998 Accuracy 0.4905\n",
      "Epoch 12 Batch 1050 Loss 2.3996 Accuracy 0.4905\n",
      "Epoch 12 Batch 1100 Loss 2.4000 Accuracy 0.4904\n",
      "Epoch 12 Batch 1150 Loss 2.4000 Accuracy 0.4904\n",
      "Epoch 12 Batch 1200 Loss 2.4000 Accuracy 0.4904\n",
      "Epoch 12 Batch 1250 Loss 2.4005 Accuracy 0.4904\n",
      "Epoch 12 Batch 1300 Loss 2.4006 Accuracy 0.4904\n",
      "Epoch 12 Batch 1350 Loss 2.4010 Accuracy 0.4903\n",
      "Epoch 12 Batch 1400 Loss 2.4015 Accuracy 0.4903\n",
      "Epoch 12 Batch 1450 Loss 2.4018 Accuracy 0.4902\n",
      "Epoch 12 Batch 1500 Loss 2.4020 Accuracy 0.4901\n",
      "Epoch 12 Batch 1550 Loss 2.4023 Accuracy 0.4901\n",
      "Epoch 12 Batch 1600 Loss 2.4024 Accuracy 0.4902\n",
      "Epoch 12 Batch 1650 Loss 2.4027 Accuracy 0.4902\n",
      "Epoch 12 Batch 1700 Loss 2.4032 Accuracy 0.4900\n",
      "Epoch 12 Batch 1750 Loss 2.4034 Accuracy 0.4900\n",
      "Epoch 12 Batch 1800 Loss 2.4040 Accuracy 0.4900\n",
      "Epoch 12 Batch 1850 Loss 2.4041 Accuracy 0.4899\n",
      "Epoch 12 Batch 1900 Loss 2.4040 Accuracy 0.4900\n",
      "Epoch 12 Batch 1950 Loss 2.4041 Accuracy 0.4900\n",
      "Epoch 12 Batch 2000 Loss 2.4044 Accuracy 0.4899\n",
      "Epoch 12 Batch 2050 Loss 2.4046 Accuracy 0.4899\n",
      "Epoch 12 Batch 2100 Loss 2.4047 Accuracy 0.4898\n",
      "Epoch 12 Batch 2150 Loss 2.4050 Accuracy 0.4898\n",
      "Epoch 12 Batch 2200 Loss 2.4051 Accuracy 0.4898\n",
      "Epoch 12 Batch 2250 Loss 2.4053 Accuracy 0.4898\n",
      "Epoch 12 Batch 2300 Loss 2.4055 Accuracy 0.4898\n",
      "Epoch 12 Batch 2350 Loss 2.4053 Accuracy 0.4898\n",
      "Epoch 12 Batch 2400 Loss 2.4058 Accuracy 0.4897\n",
      "Epoch 12 Batch 2450 Loss 2.4061 Accuracy 0.4897\n",
      "Epoch 12 Batch 2500 Loss 2.4066 Accuracy 0.4896\n",
      "Epoch 12 Batch 2550 Loss 2.4068 Accuracy 0.4895\n",
      "Epoch 12 Batch 2600 Loss 2.4072 Accuracy 0.4895\n",
      "Epoch 12 Loss 2.4075 Accuracy 0.4895\n",
      "Time taken for 1 epoch: 485.09728622436523 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 2.2996 Accuracy 0.5187\n",
      "Epoch 13 Batch 50 Loss 2.3805 Accuracy 0.4923\n",
      "Epoch 13 Batch 100 Loss 2.3740 Accuracy 0.4933\n",
      "Epoch 13 Batch 150 Loss 2.3785 Accuracy 0.4924\n",
      "Epoch 13 Batch 200 Loss 2.3835 Accuracy 0.4916\n",
      "Epoch 13 Batch 250 Loss 2.3860 Accuracy 0.4914\n",
      "Epoch 13 Batch 300 Loss 2.3859 Accuracy 0.4915\n",
      "Epoch 13 Batch 350 Loss 2.3870 Accuracy 0.4917\n",
      "Epoch 13 Batch 400 Loss 2.3870 Accuracy 0.4918\n",
      "Epoch 13 Batch 450 Loss 2.3889 Accuracy 0.4916\n",
      "Epoch 13 Batch 500 Loss 2.3881 Accuracy 0.4920\n",
      "Epoch 13 Batch 550 Loss 2.3889 Accuracy 0.4918\n",
      "Epoch 13 Batch 600 Loss 2.3889 Accuracy 0.4919\n",
      "Epoch 13 Batch 650 Loss 2.3882 Accuracy 0.4922\n",
      "Epoch 13 Batch 700 Loss 2.3897 Accuracy 0.4918\n",
      "Epoch 13 Batch 750 Loss 2.3902 Accuracy 0.4919\n",
      "Epoch 13 Batch 800 Loss 2.3901 Accuracy 0.4920\n",
      "Epoch 13 Batch 850 Loss 2.3910 Accuracy 0.4918\n",
      "Epoch 13 Batch 900 Loss 2.3915 Accuracy 0.4916\n",
      "Epoch 13 Batch 950 Loss 2.3918 Accuracy 0.4916\n",
      "Epoch 13 Batch 1000 Loss 2.3924 Accuracy 0.4915\n",
      "Epoch 13 Batch 1050 Loss 2.3917 Accuracy 0.4916\n",
      "Epoch 13 Batch 1100 Loss 2.3916 Accuracy 0.4915\n",
      "Epoch 13 Batch 1150 Loss 2.3914 Accuracy 0.4916\n",
      "Epoch 13 Batch 1200 Loss 2.3914 Accuracy 0.4917\n",
      "Epoch 13 Batch 1250 Loss 2.3927 Accuracy 0.4915\n",
      "Epoch 13 Batch 1300 Loss 2.3931 Accuracy 0.4913\n",
      "Epoch 13 Batch 1350 Loss 2.3940 Accuracy 0.4912\n",
      "Epoch 13 Batch 1400 Loss 2.3938 Accuracy 0.4912\n",
      "Epoch 13 Batch 1450 Loss 2.3944 Accuracy 0.4911\n",
      "Epoch 13 Batch 1500 Loss 2.3944 Accuracy 0.4911\n",
      "Epoch 13 Batch 1550 Loss 2.3946 Accuracy 0.4911\n",
      "Epoch 13 Batch 1600 Loss 2.3948 Accuracy 0.4911\n",
      "Epoch 13 Batch 1650 Loss 2.3952 Accuracy 0.4911\n",
      "Epoch 13 Batch 1700 Loss 2.3957 Accuracy 0.4911\n",
      "Epoch 13 Batch 1750 Loss 2.3962 Accuracy 0.4910\n",
      "Epoch 13 Batch 1800 Loss 2.3970 Accuracy 0.4910\n",
      "Epoch 13 Batch 1850 Loss 2.3966 Accuracy 0.4909\n",
      "Epoch 13 Batch 1900 Loss 2.3968 Accuracy 0.4909\n",
      "Epoch 13 Batch 1950 Loss 2.3972 Accuracy 0.4908\n",
      "Epoch 13 Batch 2000 Loss 2.3982 Accuracy 0.4906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 2050 Loss 2.3984 Accuracy 0.4906\n",
      "Epoch 13 Batch 2100 Loss 2.3983 Accuracy 0.4906\n",
      "Epoch 13 Batch 2150 Loss 2.3982 Accuracy 0.4906\n",
      "Epoch 13 Batch 2200 Loss 2.3985 Accuracy 0.4906\n",
      "Epoch 13 Batch 2250 Loss 2.3989 Accuracy 0.4905\n",
      "Epoch 13 Batch 2300 Loss 2.3991 Accuracy 0.4905\n",
      "Epoch 13 Batch 2350 Loss 2.3992 Accuracy 0.4904\n",
      "Epoch 13 Batch 2400 Loss 2.3994 Accuracy 0.4904\n",
      "Epoch 13 Batch 2450 Loss 2.3993 Accuracy 0.4904\n",
      "Epoch 13 Batch 2500 Loss 2.3997 Accuracy 0.4904\n",
      "Epoch 13 Batch 2550 Loss 2.3996 Accuracy 0.4904\n",
      "Epoch 13 Batch 2600 Loss 2.4001 Accuracy 0.4904\n",
      "Epoch 13 Loss 2.4002 Accuracy 0.4904\n",
      "Time taken for 1 epoch: 492.9581801891327 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 2.3898 Accuracy 0.4812\n",
      "Epoch 14 Batch 50 Loss 2.3700 Accuracy 0.4949\n",
      "Epoch 14 Batch 100 Loss 2.3693 Accuracy 0.4942\n",
      "Epoch 14 Batch 150 Loss 2.3787 Accuracy 0.4923\n",
      "Epoch 14 Batch 200 Loss 2.3778 Accuracy 0.4925\n",
      "Epoch 14 Batch 250 Loss 2.3793 Accuracy 0.4923\n",
      "Epoch 14 Batch 300 Loss 2.3809 Accuracy 0.4922\n",
      "Epoch 14 Batch 350 Loss 2.3810 Accuracy 0.4923\n",
      "Epoch 14 Batch 400 Loss 2.3816 Accuracy 0.4922\n",
      "Epoch 14 Batch 450 Loss 2.3827 Accuracy 0.4920\n",
      "Epoch 14 Batch 500 Loss 2.3808 Accuracy 0.4924\n",
      "Epoch 14 Batch 550 Loss 2.3830 Accuracy 0.4925\n",
      "Epoch 14 Batch 600 Loss 2.3820 Accuracy 0.4925\n",
      "Epoch 14 Batch 650 Loss 2.3830 Accuracy 0.4924\n",
      "Epoch 14 Batch 700 Loss 2.3846 Accuracy 0.4921\n",
      "Epoch 14 Batch 750 Loss 2.3844 Accuracy 0.4922\n",
      "Epoch 14 Batch 800 Loss 2.3853 Accuracy 0.4920\n",
      "Epoch 14 Batch 850 Loss 2.3852 Accuracy 0.4919\n",
      "Epoch 14 Batch 900 Loss 2.3851 Accuracy 0.4919\n",
      "Epoch 14 Batch 950 Loss 2.3857 Accuracy 0.4919\n",
      "Epoch 14 Batch 1000 Loss 2.3853 Accuracy 0.4920\n",
      "Epoch 14 Batch 1050 Loss 2.3848 Accuracy 0.4921\n",
      "Epoch 14 Batch 1100 Loss 2.3848 Accuracy 0.4922\n",
      "Epoch 14 Batch 1150 Loss 2.3851 Accuracy 0.4922\n",
      "Epoch 14 Batch 1200 Loss 2.3858 Accuracy 0.4922\n",
      "Epoch 14 Batch 1250 Loss 2.3861 Accuracy 0.4922\n",
      "Epoch 14 Batch 1300 Loss 2.3863 Accuracy 0.4920\n",
      "Epoch 14 Batch 1350 Loss 2.3869 Accuracy 0.4920\n",
      "Epoch 14 Batch 1400 Loss 2.3873 Accuracy 0.4920\n",
      "Epoch 14 Batch 1450 Loss 2.3873 Accuracy 0.4920\n",
      "Epoch 14 Batch 1500 Loss 2.3879 Accuracy 0.4919\n",
      "Epoch 14 Batch 1550 Loss 2.3882 Accuracy 0.4919\n",
      "Epoch 14 Batch 1600 Loss 2.3881 Accuracy 0.4919\n",
      "Epoch 14 Batch 1650 Loss 2.3885 Accuracy 0.4918\n",
      "Epoch 14 Batch 1700 Loss 2.3893 Accuracy 0.4918\n",
      "Epoch 14 Batch 1750 Loss 2.3895 Accuracy 0.4918\n",
      "Epoch 14 Batch 1800 Loss 2.3899 Accuracy 0.4916\n",
      "Epoch 14 Batch 1850 Loss 2.3901 Accuracy 0.4916\n",
      "Epoch 14 Batch 1900 Loss 2.3906 Accuracy 0.4916\n",
      "Epoch 14 Batch 1950 Loss 2.3905 Accuracy 0.4916\n",
      "Epoch 14 Batch 2000 Loss 2.3907 Accuracy 0.4917\n",
      "Epoch 14 Batch 2050 Loss 2.3908 Accuracy 0.4917\n",
      "Epoch 14 Batch 2100 Loss 2.3909 Accuracy 0.4916\n",
      "Epoch 14 Batch 2150 Loss 2.3911 Accuracy 0.4916\n",
      "Epoch 14 Batch 2200 Loss 2.3915 Accuracy 0.4916\n",
      "Epoch 14 Batch 2250 Loss 2.3916 Accuracy 0.4916\n",
      "Epoch 14 Batch 2300 Loss 2.3920 Accuracy 0.4915\n",
      "Epoch 14 Batch 2350 Loss 2.3921 Accuracy 0.4915\n",
      "Epoch 14 Batch 2400 Loss 2.3920 Accuracy 0.4915\n",
      "Epoch 14 Batch 2450 Loss 2.3922 Accuracy 0.4915\n",
      "Epoch 14 Batch 2500 Loss 2.3927 Accuracy 0.4914\n",
      "Epoch 14 Batch 2550 Loss 2.3930 Accuracy 0.4913\n",
      "Epoch 14 Batch 2600 Loss 2.3935 Accuracy 0.4913\n",
      "Epoch 14 Loss 2.3938 Accuracy 0.4913\n",
      "Time taken for 1 epoch: 464.04652094841003 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 2.2618 Accuracy 0.4983\n",
      "Epoch 15 Batch 50 Loss 2.3695 Accuracy 0.4956\n",
      "Epoch 15 Batch 100 Loss 2.3618 Accuracy 0.4954\n",
      "Epoch 15 Batch 150 Loss 2.3625 Accuracy 0.4948\n",
      "Epoch 15 Batch 200 Loss 2.3704 Accuracy 0.4938\n",
      "Epoch 15 Batch 250 Loss 2.3730 Accuracy 0.4930\n",
      "Epoch 15 Batch 300 Loss 2.3716 Accuracy 0.4932\n",
      "Epoch 15 Batch 350 Loss 2.3725 Accuracy 0.4933\n",
      "Epoch 15 Batch 400 Loss 2.3733 Accuracy 0.4934\n",
      "Epoch 15 Batch 450 Loss 2.3736 Accuracy 0.4933\n",
      "Epoch 15 Batch 500 Loss 2.3763 Accuracy 0.4930\n",
      "Epoch 15 Batch 550 Loss 2.3757 Accuracy 0.4932\n",
      "Epoch 15 Batch 600 Loss 2.3762 Accuracy 0.4932\n",
      "Epoch 15 Batch 650 Loss 2.3783 Accuracy 0.4928\n",
      "Epoch 15 Batch 700 Loss 2.3771 Accuracy 0.4931\n",
      "Epoch 15 Batch 750 Loss 2.3773 Accuracy 0.4932\n",
      "Epoch 15 Batch 800 Loss 2.3768 Accuracy 0.4933\n",
      "Epoch 15 Batch 850 Loss 2.3768 Accuracy 0.4933\n",
      "Epoch 15 Batch 900 Loss 2.3770 Accuracy 0.4934\n",
      "Epoch 15 Batch 950 Loss 2.3791 Accuracy 0.4931\n",
      "Epoch 15 Batch 1000 Loss 2.3787 Accuracy 0.4932\n",
      "Epoch 15 Batch 1050 Loss 2.3786 Accuracy 0.4933\n",
      "Epoch 15 Batch 1100 Loss 2.3786 Accuracy 0.4933\n",
      "Epoch 15 Batch 1150 Loss 2.3796 Accuracy 0.4932\n",
      "Epoch 15 Batch 1200 Loss 2.3800 Accuracy 0.4931\n",
      "Epoch 15 Batch 1250 Loss 2.3801 Accuracy 0.4931\n",
      "Epoch 15 Batch 1300 Loss 2.3802 Accuracy 0.4930\n",
      "Epoch 15 Batch 1350 Loss 2.3805 Accuracy 0.4930\n",
      "Epoch 15 Batch 1400 Loss 2.3813 Accuracy 0.4929\n",
      "Epoch 15 Batch 1450 Loss 2.3814 Accuracy 0.4929\n",
      "Epoch 15 Batch 1500 Loss 2.3812 Accuracy 0.4929\n",
      "Epoch 15 Batch 1550 Loss 2.3811 Accuracy 0.4931\n",
      "Epoch 15 Batch 1600 Loss 2.3822 Accuracy 0.4928\n",
      "Epoch 15 Batch 1650 Loss 2.3828 Accuracy 0.4927\n",
      "Epoch 15 Batch 1700 Loss 2.3831 Accuracy 0.4928\n",
      "Epoch 15 Batch 1750 Loss 2.3828 Accuracy 0.4929\n",
      "Epoch 15 Batch 1800 Loss 2.3835 Accuracy 0.4927\n",
      "Epoch 15 Batch 1850 Loss 2.3834 Accuracy 0.4927\n",
      "Epoch 15 Batch 1900 Loss 2.3837 Accuracy 0.4927\n",
      "Epoch 15 Batch 1950 Loss 2.3838 Accuracy 0.4927\n",
      "Epoch 15 Batch 2000 Loss 2.3839 Accuracy 0.4927\n",
      "Epoch 15 Batch 2050 Loss 2.3838 Accuracy 0.4927\n",
      "Epoch 15 Batch 2100 Loss 2.3842 Accuracy 0.4927\n",
      "Epoch 15 Batch 2150 Loss 2.3845 Accuracy 0.4926\n",
      "Epoch 15 Batch 2200 Loss 2.3849 Accuracy 0.4926\n",
      "Epoch 15 Batch 2250 Loss 2.3853 Accuracy 0.4925\n",
      "Epoch 15 Batch 2300 Loss 2.3857 Accuracy 0.4925\n",
      "Epoch 15 Batch 2350 Loss 2.3855 Accuracy 0.4926\n",
      "Epoch 15 Batch 2400 Loss 2.3857 Accuracy 0.4925\n",
      "Epoch 15 Batch 2450 Loss 2.3862 Accuracy 0.4924\n",
      "Epoch 15 Batch 2500 Loss 2.3862 Accuracy 0.4924\n",
      "Epoch 15 Batch 2550 Loss 2.3863 Accuracy 0.4924\n",
      "Epoch 15 Batch 2600 Loss 2.3869 Accuracy 0.4923\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train_full/ckpt-13\n",
      "Epoch 15 Loss 2.3872 Accuracy 0.4923\n",
      "Time taken for 1 epoch: 457.55076241493225 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 2.2835 Accuracy 0.5113\n",
      "Epoch 16 Batch 50 Loss 2.3686 Accuracy 0.4929\n",
      "Epoch 16 Batch 100 Loss 2.3709 Accuracy 0.4944\n",
      "Epoch 16 Batch 150 Loss 2.3739 Accuracy 0.4935\n",
      "Epoch 16 Batch 200 Loss 2.3735 Accuracy 0.4932\n",
      "Epoch 16 Batch 250 Loss 2.3698 Accuracy 0.4938\n",
      "Epoch 16 Batch 300 Loss 2.3672 Accuracy 0.4944\n",
      "Epoch 16 Batch 350 Loss 2.3680 Accuracy 0.4944\n",
      "Epoch 16 Batch 400 Loss 2.3699 Accuracy 0.4942\n",
      "Epoch 16 Batch 450 Loss 2.3711 Accuracy 0.4940\n",
      "Epoch 16 Batch 500 Loss 2.3703 Accuracy 0.4941\n",
      "Epoch 16 Batch 550 Loss 2.3709 Accuracy 0.4941\n",
      "Epoch 16 Batch 600 Loss 2.3703 Accuracy 0.4943\n",
      "Epoch 16 Batch 650 Loss 2.3710 Accuracy 0.4942\n",
      "Epoch 16 Batch 700 Loss 2.3733 Accuracy 0.4938\n",
      "Epoch 16 Batch 750 Loss 2.3729 Accuracy 0.4937\n",
      "Epoch 16 Batch 800 Loss 2.3727 Accuracy 0.4938\n",
      "Epoch 16 Batch 850 Loss 2.3733 Accuracy 0.4937\n",
      "Epoch 16 Batch 900 Loss 2.3725 Accuracy 0.4938\n",
      "Epoch 16 Batch 950 Loss 2.3737 Accuracy 0.4937\n",
      "Epoch 16 Batch 1000 Loss 2.3741 Accuracy 0.4937\n",
      "Epoch 16 Batch 1050 Loss 2.3747 Accuracy 0.4936\n",
      "Epoch 16 Batch 1100 Loss 2.3751 Accuracy 0.4935\n",
      "Epoch 16 Batch 1150 Loss 2.3748 Accuracy 0.4935\n",
      "Epoch 16 Batch 1200 Loss 2.3755 Accuracy 0.4935\n",
      "Epoch 16 Batch 1250 Loss 2.3750 Accuracy 0.4936\n",
      "Epoch 16 Batch 1300 Loss 2.3754 Accuracy 0.4935\n",
      "Epoch 16 Batch 1350 Loss 2.3754 Accuracy 0.4935\n",
      "Epoch 16 Batch 1400 Loss 2.3754 Accuracy 0.4935\n",
      "Epoch 16 Batch 1450 Loss 2.3755 Accuracy 0.4935\n",
      "Epoch 16 Batch 1500 Loss 2.3758 Accuracy 0.4935\n",
      "Epoch 16 Batch 1550 Loss 2.3762 Accuracy 0.4935\n",
      "Epoch 16 Batch 1600 Loss 2.3765 Accuracy 0.4935\n",
      "Epoch 16 Batch 1650 Loss 2.3764 Accuracy 0.4936\n",
      "Epoch 16 Batch 1700 Loss 2.3767 Accuracy 0.4936\n",
      "Epoch 16 Batch 1750 Loss 2.3771 Accuracy 0.4935\n",
      "Epoch 16 Batch 1800 Loss 2.3774 Accuracy 0.4934\n",
      "Epoch 16 Batch 1850 Loss 2.3771 Accuracy 0.4935\n",
      "Epoch 16 Batch 1900 Loss 2.3774 Accuracy 0.4935\n",
      "Epoch 16 Batch 1950 Loss 2.3779 Accuracy 0.4934\n",
      "Epoch 16 Batch 2000 Loss 2.3780 Accuracy 0.4934\n",
      "Epoch 16 Batch 2050 Loss 2.3783 Accuracy 0.4934\n",
      "Epoch 16 Batch 2100 Loss 2.3784 Accuracy 0.4933\n",
      "Epoch 16 Batch 2150 Loss 2.3785 Accuracy 0.4934\n",
      "Epoch 16 Batch 2200 Loss 2.3791 Accuracy 0.4933\n",
      "Epoch 16 Batch 2250 Loss 2.3790 Accuracy 0.4933\n",
      "Epoch 16 Batch 2300 Loss 2.3788 Accuracy 0.4933\n",
      "Epoch 16 Batch 2350 Loss 2.3791 Accuracy 0.4933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Batch 2400 Loss 2.3795 Accuracy 0.4932\n",
      "Epoch 16 Batch 2450 Loss 2.3799 Accuracy 0.4932\n",
      "Epoch 16 Batch 2500 Loss 2.3807 Accuracy 0.4931\n",
      "Epoch 16 Batch 2550 Loss 2.3809 Accuracy 0.4931\n",
      "Epoch 16 Batch 2600 Loss 2.3813 Accuracy 0.4931\n",
      "Epoch 16 Loss 2.3814 Accuracy 0.4931\n",
      "Time taken for 1 epoch: 467.47423243522644 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 2.5285 Accuracy 0.4656\n",
      "Epoch 17 Batch 50 Loss 2.3558 Accuracy 0.4954\n",
      "Epoch 17 Batch 100 Loss 2.3418 Accuracy 0.4964\n",
      "Epoch 17 Batch 150 Loss 2.3489 Accuracy 0.4953\n",
      "Epoch 17 Batch 200 Loss 2.3492 Accuracy 0.4957\n",
      "Epoch 17 Batch 250 Loss 2.3498 Accuracy 0.4960\n",
      "Epoch 17 Batch 300 Loss 2.3513 Accuracy 0.4960\n",
      "Epoch 17 Batch 350 Loss 2.3523 Accuracy 0.4959\n",
      "Epoch 17 Batch 400 Loss 2.3552 Accuracy 0.4959\n",
      "Epoch 17 Batch 450 Loss 2.3578 Accuracy 0.4956\n",
      "Epoch 17 Batch 500 Loss 2.3588 Accuracy 0.4955\n",
      "Epoch 17 Batch 550 Loss 2.3611 Accuracy 0.4955\n",
      "Epoch 17 Batch 600 Loss 2.3629 Accuracy 0.4952\n",
      "Epoch 17 Batch 650 Loss 2.3613 Accuracy 0.4953\n",
      "Epoch 17 Batch 700 Loss 2.3634 Accuracy 0.4951\n",
      "Epoch 17 Batch 750 Loss 2.3648 Accuracy 0.4949\n",
      "Epoch 17 Batch 800 Loss 2.3646 Accuracy 0.4949\n",
      "Epoch 17 Batch 850 Loss 2.3650 Accuracy 0.4949\n",
      "Epoch 17 Batch 900 Loss 2.3653 Accuracy 0.4949\n",
      "Epoch 17 Batch 950 Loss 2.3663 Accuracy 0.4947\n",
      "Epoch 17 Batch 1000 Loss 2.3655 Accuracy 0.4950\n",
      "Epoch 17 Batch 1050 Loss 2.3658 Accuracy 0.4949\n",
      "Epoch 17 Batch 1100 Loss 2.3665 Accuracy 0.4947\n",
      "Epoch 17 Batch 1150 Loss 2.3671 Accuracy 0.4945\n",
      "Epoch 17 Batch 1200 Loss 2.3687 Accuracy 0.4942\n",
      "Epoch 17 Batch 1250 Loss 2.3688 Accuracy 0.4942\n",
      "Epoch 17 Batch 1300 Loss 2.3686 Accuracy 0.4943\n",
      "Epoch 17 Batch 1350 Loss 2.3689 Accuracy 0.4943\n",
      "Epoch 17 Batch 1400 Loss 2.3695 Accuracy 0.4942\n",
      "Epoch 17 Batch 1450 Loss 2.3695 Accuracy 0.4943\n",
      "Epoch 17 Batch 1500 Loss 2.3698 Accuracy 0.4943\n",
      "Epoch 17 Batch 1550 Loss 2.3699 Accuracy 0.4943\n",
      "Epoch 17 Batch 1600 Loss 2.3702 Accuracy 0.4943\n",
      "Epoch 17 Batch 1650 Loss 2.3702 Accuracy 0.4944\n",
      "Epoch 17 Batch 1700 Loss 2.3707 Accuracy 0.4943\n",
      "Epoch 17 Batch 1750 Loss 2.3711 Accuracy 0.4942\n",
      "Epoch 17 Batch 1800 Loss 2.3717 Accuracy 0.4942\n",
      "Epoch 17 Batch 1850 Loss 2.3715 Accuracy 0.4943\n",
      "Epoch 17 Batch 1900 Loss 2.3715 Accuracy 0.4944\n",
      "Epoch 17 Batch 1950 Loss 2.3717 Accuracy 0.4943\n",
      "Epoch 17 Batch 2000 Loss 2.3715 Accuracy 0.4943\n",
      "Epoch 17 Batch 2050 Loss 2.3717 Accuracy 0.4943\n",
      "Epoch 17 Batch 2100 Loss 2.3716 Accuracy 0.4943\n",
      "Epoch 17 Batch 2150 Loss 2.3719 Accuracy 0.4943\n",
      "Epoch 17 Batch 2200 Loss 2.3724 Accuracy 0.4942\n",
      "Epoch 17 Batch 2250 Loss 2.3729 Accuracy 0.4942\n",
      "Epoch 17 Batch 2300 Loss 2.3731 Accuracy 0.4942\n",
      "Epoch 17 Batch 2350 Loss 2.3735 Accuracy 0.4942\n",
      "Epoch 17 Batch 2400 Loss 2.3736 Accuracy 0.4941\n",
      "Epoch 17 Batch 2450 Loss 2.3739 Accuracy 0.4941\n",
      "Epoch 17 Batch 2500 Loss 2.3746 Accuracy 0.4940\n",
      "Epoch 17 Batch 2550 Loss 2.3747 Accuracy 0.4940\n",
      "Epoch 17 Batch 2600 Loss 2.3752 Accuracy 0.4940\n",
      "Epoch 17 Loss 2.3751 Accuracy 0.4940\n",
      "Time taken for 1 epoch: 458.74986839294434 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 2.3684 Accuracy 0.4926\n",
      "Epoch 18 Batch 50 Loss 2.3779 Accuracy 0.4905\n",
      "Epoch 18 Batch 100 Loss 2.3675 Accuracy 0.4925\n",
      "Epoch 18 Batch 150 Loss 2.3608 Accuracy 0.4936\n",
      "Epoch 18 Batch 200 Loss 2.3574 Accuracy 0.4948\n",
      "Epoch 18 Batch 250 Loss 2.3574 Accuracy 0.4948\n",
      "Epoch 18 Batch 300 Loss 2.3584 Accuracy 0.4954\n",
      "Epoch 18 Batch 350 Loss 2.3574 Accuracy 0.4955\n",
      "Epoch 18 Batch 400 Loss 2.3561 Accuracy 0.4956\n",
      "Epoch 18 Batch 450 Loss 2.3560 Accuracy 0.4957\n",
      "Epoch 18 Batch 500 Loss 2.3570 Accuracy 0.4955\n",
      "Epoch 18 Batch 550 Loss 2.3571 Accuracy 0.4958\n",
      "Epoch 18 Batch 600 Loss 2.3586 Accuracy 0.4956\n",
      "Epoch 18 Batch 650 Loss 2.3591 Accuracy 0.4955\n",
      "Epoch 18 Batch 700 Loss 2.3598 Accuracy 0.4955\n",
      "Epoch 18 Batch 750 Loss 2.3598 Accuracy 0.4956\n",
      "Epoch 18 Batch 800 Loss 2.3603 Accuracy 0.4953\n",
      "Epoch 18 Batch 850 Loss 2.3609 Accuracy 0.4954\n",
      "Epoch 18 Batch 900 Loss 2.3623 Accuracy 0.4952\n",
      "Epoch 18 Batch 950 Loss 2.3625 Accuracy 0.4952\n",
      "Epoch 18 Batch 1000 Loss 2.3625 Accuracy 0.4951\n",
      "Epoch 18 Batch 1050 Loss 2.3615 Accuracy 0.4954\n",
      "Epoch 18 Batch 1100 Loss 2.3614 Accuracy 0.4954\n",
      "Epoch 18 Batch 1150 Loss 2.3615 Accuracy 0.4954\n",
      "Epoch 18 Batch 1200 Loss 2.3620 Accuracy 0.4954\n",
      "Epoch 18 Batch 1250 Loss 2.3622 Accuracy 0.4953\n",
      "Epoch 18 Batch 1300 Loss 2.3625 Accuracy 0.4953\n",
      "Epoch 18 Batch 1350 Loss 2.3634 Accuracy 0.4953\n",
      "Epoch 18 Batch 1400 Loss 2.3632 Accuracy 0.4954\n",
      "Epoch 18 Batch 1450 Loss 2.3637 Accuracy 0.4954\n",
      "Epoch 18 Batch 1500 Loss 2.3643 Accuracy 0.4953\n",
      "Epoch 18 Batch 1550 Loss 2.3647 Accuracy 0.4952\n",
      "Epoch 18 Batch 1600 Loss 2.3653 Accuracy 0.4952\n",
      "Epoch 18 Batch 1650 Loss 2.3656 Accuracy 0.4952\n",
      "Epoch 18 Batch 1700 Loss 2.3653 Accuracy 0.4952\n",
      "Epoch 18 Batch 1750 Loss 2.3654 Accuracy 0.4952\n",
      "Epoch 18 Batch 1800 Loss 2.3658 Accuracy 0.4952\n",
      "Epoch 18 Batch 1850 Loss 2.3653 Accuracy 0.4953\n",
      "Epoch 18 Batch 1900 Loss 2.3660 Accuracy 0.4952\n",
      "Epoch 18 Batch 1950 Loss 2.3657 Accuracy 0.4953\n",
      "Epoch 18 Batch 2000 Loss 2.3655 Accuracy 0.4953\n",
      "Epoch 18 Batch 2050 Loss 2.3656 Accuracy 0.4953\n",
      "Epoch 18 Batch 2100 Loss 2.3656 Accuracy 0.4952\n",
      "Epoch 18 Batch 2150 Loss 2.3663 Accuracy 0.4951\n",
      "Epoch 18 Batch 2200 Loss 2.3666 Accuracy 0.4950\n",
      "Epoch 18 Batch 2250 Loss 2.3671 Accuracy 0.4949\n",
      "Epoch 18 Batch 2300 Loss 2.3674 Accuracy 0.4949\n",
      "Epoch 18 Batch 2350 Loss 2.3673 Accuracy 0.4949\n",
      "Epoch 18 Batch 2400 Loss 2.3679 Accuracy 0.4948\n",
      "Epoch 18 Batch 2450 Loss 2.3681 Accuracy 0.4948\n",
      "Epoch 18 Batch 2500 Loss 2.3686 Accuracy 0.4947\n",
      "Epoch 18 Batch 2550 Loss 2.3692 Accuracy 0.4947\n",
      "Epoch 18 Batch 2600 Loss 2.3695 Accuracy 0.4946\n",
      "Epoch 18 Loss 2.3696 Accuracy 0.4946\n",
      "Time taken for 1 epoch: 470.2468912601471 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 2.3096 Accuracy 0.5159\n",
      "Epoch 19 Batch 50 Loss 2.3484 Accuracy 0.4969\n",
      "Epoch 19 Batch 100 Loss 2.3469 Accuracy 0.4972\n",
      "Epoch 19 Batch 150 Loss 2.3441 Accuracy 0.4977\n",
      "Epoch 19 Batch 200 Loss 2.3425 Accuracy 0.4979\n",
      "Epoch 19 Batch 250 Loss 2.3438 Accuracy 0.4978\n",
      "Epoch 19 Batch 300 Loss 2.3426 Accuracy 0.4981\n",
      "Epoch 19 Batch 350 Loss 2.3427 Accuracy 0.4981\n",
      "Epoch 19 Batch 400 Loss 2.3435 Accuracy 0.4979\n",
      "Epoch 19 Batch 450 Loss 2.3468 Accuracy 0.4975\n",
      "Epoch 19 Batch 500 Loss 2.3487 Accuracy 0.4972\n",
      "Epoch 19 Batch 550 Loss 2.3508 Accuracy 0.4968\n",
      "Epoch 19 Batch 600 Loss 2.3500 Accuracy 0.4970\n",
      "Epoch 19 Batch 650 Loss 2.3513 Accuracy 0.4966\n",
      "Epoch 19 Batch 700 Loss 2.3533 Accuracy 0.4962\n",
      "Epoch 19 Batch 750 Loss 2.3529 Accuracy 0.4962\n",
      "Epoch 19 Batch 800 Loss 2.3536 Accuracy 0.4961\n",
      "Epoch 19 Batch 850 Loss 2.3536 Accuracy 0.4963\n",
      "Epoch 19 Batch 900 Loss 2.3533 Accuracy 0.4964\n",
      "Epoch 19 Batch 950 Loss 2.3540 Accuracy 0.4965\n",
      "Epoch 19 Batch 1000 Loss 2.3538 Accuracy 0.4965\n",
      "Epoch 19 Batch 1050 Loss 2.3542 Accuracy 0.4964\n",
      "Epoch 19 Batch 1100 Loss 2.3549 Accuracy 0.4963\n",
      "Epoch 19 Batch 1150 Loss 2.3564 Accuracy 0.4961\n",
      "Epoch 19 Batch 1200 Loss 2.3561 Accuracy 0.4962\n",
      "Epoch 19 Batch 1250 Loss 2.3560 Accuracy 0.4962\n",
      "Epoch 19 Batch 1300 Loss 2.3559 Accuracy 0.4962\n",
      "Epoch 19 Batch 1350 Loss 2.3557 Accuracy 0.4963\n",
      "Epoch 19 Batch 1400 Loss 2.3563 Accuracy 0.4962\n",
      "Epoch 19 Batch 1450 Loss 2.3567 Accuracy 0.4961\n",
      "Epoch 19 Batch 1500 Loss 2.3568 Accuracy 0.4962\n",
      "Epoch 19 Batch 1550 Loss 2.3575 Accuracy 0.4961\n",
      "Epoch 19 Batch 1600 Loss 2.3575 Accuracy 0.4961\n",
      "Epoch 19 Batch 1650 Loss 2.3577 Accuracy 0.4962\n",
      "Epoch 19 Batch 1700 Loss 2.3583 Accuracy 0.4961\n",
      "Epoch 19 Batch 1750 Loss 2.3587 Accuracy 0.4961\n",
      "Epoch 19 Batch 1800 Loss 2.3594 Accuracy 0.4960\n",
      "Epoch 19 Batch 1850 Loss 2.3597 Accuracy 0.4959\n",
      "Epoch 19 Batch 1900 Loss 2.3597 Accuracy 0.4959\n",
      "Epoch 19 Batch 1950 Loss 2.3597 Accuracy 0.4959\n",
      "Epoch 19 Batch 2000 Loss 2.3603 Accuracy 0.4958\n",
      "Epoch 19 Batch 2050 Loss 2.3609 Accuracy 0.4957\n",
      "Epoch 19 Batch 2100 Loss 2.3608 Accuracy 0.4958\n",
      "Epoch 19 Batch 2150 Loss 2.3603 Accuracy 0.4958\n",
      "Epoch 19 Batch 2200 Loss 2.3609 Accuracy 0.4957\n",
      "Epoch 19 Batch 2250 Loss 2.3612 Accuracy 0.4957\n",
      "Epoch 19 Batch 2300 Loss 2.3616 Accuracy 0.4956\n",
      "Epoch 19 Batch 2350 Loss 2.3616 Accuracy 0.4956\n",
      "Epoch 19 Batch 2400 Loss 2.3619 Accuracy 0.4956\n",
      "Epoch 19 Batch 2450 Loss 2.3618 Accuracy 0.4956\n",
      "Epoch 19 Batch 2500 Loss 2.3621 Accuracy 0.4956\n",
      "Epoch 19 Batch 2550 Loss 2.3623 Accuracy 0.4956\n",
      "Epoch 19 Batch 2600 Loss 2.3626 Accuracy 0.4956\n",
      "Epoch 19 Loss 2.3627 Accuracy 0.4956\n",
      "Time taken for 1 epoch: 456.32882285118103 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 2.4097 Accuracy 0.4920\n",
      "Epoch 20 Batch 50 Loss 2.3498 Accuracy 0.4976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 100 Loss 2.3440 Accuracy 0.4975\n",
      "Epoch 20 Batch 150 Loss 2.3438 Accuracy 0.4978\n",
      "Epoch 20 Batch 200 Loss 2.3437 Accuracy 0.4972\n",
      "Epoch 20 Batch 250 Loss 2.3399 Accuracy 0.4977\n",
      "Epoch 20 Batch 300 Loss 2.3426 Accuracy 0.4974\n",
      "Epoch 20 Batch 350 Loss 2.3401 Accuracy 0.4981\n",
      "Epoch 20 Batch 400 Loss 2.3416 Accuracy 0.4980\n",
      "Epoch 20 Batch 450 Loss 2.3413 Accuracy 0.4984\n",
      "Epoch 20 Batch 500 Loss 2.3415 Accuracy 0.4984\n",
      "Epoch 20 Batch 550 Loss 2.3425 Accuracy 0.4984\n",
      "Epoch 20 Batch 600 Loss 2.3433 Accuracy 0.4982\n",
      "Epoch 20 Batch 650 Loss 2.3451 Accuracy 0.4978\n",
      "Epoch 20 Batch 700 Loss 2.3468 Accuracy 0.4974\n",
      "Epoch 20 Batch 750 Loss 2.3475 Accuracy 0.4974\n",
      "Epoch 20 Batch 800 Loss 2.3485 Accuracy 0.4974\n",
      "Epoch 20 Batch 850 Loss 2.3496 Accuracy 0.4972\n",
      "Epoch 20 Batch 900 Loss 2.3502 Accuracy 0.4970\n",
      "Epoch 20 Batch 950 Loss 2.3498 Accuracy 0.4972\n",
      "Epoch 20 Batch 1000 Loss 2.3495 Accuracy 0.4973\n",
      "Epoch 20 Batch 1050 Loss 2.3497 Accuracy 0.4973\n",
      "Epoch 20 Batch 1100 Loss 2.3498 Accuracy 0.4973\n",
      "Epoch 20 Batch 1150 Loss 2.3499 Accuracy 0.4972\n",
      "Epoch 20 Batch 1200 Loss 2.3497 Accuracy 0.4972\n",
      "Epoch 20 Batch 1250 Loss 2.3501 Accuracy 0.4972\n",
      "Epoch 20 Batch 1300 Loss 2.3499 Accuracy 0.4972\n",
      "Epoch 20 Batch 1350 Loss 2.3497 Accuracy 0.4972\n",
      "Epoch 20 Batch 1400 Loss 2.3499 Accuracy 0.4972\n",
      "Epoch 20 Batch 1450 Loss 2.3506 Accuracy 0.4971\n",
      "Epoch 20 Batch 1500 Loss 2.3509 Accuracy 0.4971\n",
      "Epoch 20 Batch 1550 Loss 2.3510 Accuracy 0.4970\n",
      "Epoch 20 Batch 1600 Loss 2.3515 Accuracy 0.4970\n",
      "Epoch 20 Batch 1650 Loss 2.3519 Accuracy 0.4970\n",
      "Epoch 20 Batch 1700 Loss 2.3515 Accuracy 0.4971\n",
      "Epoch 20 Batch 1750 Loss 2.3514 Accuracy 0.4972\n",
      "Epoch 20 Batch 1800 Loss 2.3521 Accuracy 0.4971\n",
      "Epoch 20 Batch 1850 Loss 2.3527 Accuracy 0.4970\n",
      "Epoch 20 Batch 1900 Loss 2.3527 Accuracy 0.4970\n",
      "Epoch 20 Batch 1950 Loss 2.3529 Accuracy 0.4971\n",
      "Epoch 20 Batch 2000 Loss 2.3534 Accuracy 0.4970\n",
      "Epoch 20 Batch 2050 Loss 2.3537 Accuracy 0.4969\n",
      "Epoch 20 Batch 2100 Loss 2.3537 Accuracy 0.4970\n",
      "Epoch 20 Batch 2150 Loss 2.3540 Accuracy 0.4969\n",
      "Epoch 20 Batch 2200 Loss 2.3542 Accuracy 0.4969\n",
      "Epoch 20 Batch 2250 Loss 2.3546 Accuracy 0.4968\n",
      "Epoch 20 Batch 2300 Loss 2.3546 Accuracy 0.4968\n",
      "Epoch 20 Batch 2350 Loss 2.3547 Accuracy 0.4968\n",
      "Epoch 20 Batch 2400 Loss 2.3549 Accuracy 0.4968\n",
      "Epoch 20 Batch 2450 Loss 2.3556 Accuracy 0.4967\n",
      "Epoch 20 Batch 2500 Loss 2.3557 Accuracy 0.4966\n",
      "Epoch 20 Batch 2550 Loss 2.3561 Accuracy 0.4966\n",
      "Epoch 20 Batch 2600 Loss 2.3565 Accuracy 0.4965\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train_full/ckpt-14\n",
      "Epoch 20 Loss 2.3567 Accuracy 0.4965\n",
      "Time taken for 1 epoch: 455.32422161102295 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 2.3313 Accuracy 0.4862\n",
      "Epoch 21 Batch 50 Loss 2.3404 Accuracy 0.4963\n",
      "Epoch 21 Batch 100 Loss 2.3349 Accuracy 0.4978\n",
      "Epoch 21 Batch 150 Loss 2.3329 Accuracy 0.4985\n",
      "Epoch 21 Batch 200 Loss 2.3328 Accuracy 0.4987\n",
      "Epoch 21 Batch 250 Loss 2.3345 Accuracy 0.4992\n",
      "Epoch 21 Batch 300 Loss 2.3362 Accuracy 0.4989\n",
      "Epoch 21 Batch 350 Loss 2.3351 Accuracy 0.4992\n",
      "Epoch 21 Batch 400 Loss 2.3356 Accuracy 0.4988\n",
      "Epoch 21 Batch 450 Loss 2.3353 Accuracy 0.4987\n",
      "Epoch 21 Batch 500 Loss 2.3368 Accuracy 0.4989\n",
      "Epoch 21 Batch 550 Loss 2.3377 Accuracy 0.4987\n",
      "Epoch 21 Batch 600 Loss 2.3384 Accuracy 0.4985\n",
      "Epoch 21 Batch 650 Loss 2.3380 Accuracy 0.4988\n",
      "Epoch 21 Batch 700 Loss 2.3387 Accuracy 0.4988\n",
      "Epoch 21 Batch 750 Loss 2.3390 Accuracy 0.4986\n",
      "Epoch 21 Batch 800 Loss 2.3392 Accuracy 0.4987\n",
      "Epoch 21 Batch 850 Loss 2.3398 Accuracy 0.4986\n",
      "Epoch 21 Batch 900 Loss 2.3396 Accuracy 0.4986\n",
      "Epoch 21 Batch 950 Loss 2.3398 Accuracy 0.4986\n",
      "Epoch 21 Batch 1000 Loss 2.3397 Accuracy 0.4986\n",
      "Epoch 21 Batch 1050 Loss 2.3396 Accuracy 0.4986\n",
      "Epoch 21 Batch 1100 Loss 2.3411 Accuracy 0.4984\n",
      "Epoch 21 Batch 1150 Loss 2.3411 Accuracy 0.4984\n",
      "Epoch 21 Batch 1200 Loss 2.3410 Accuracy 0.4984\n",
      "Epoch 21 Batch 1250 Loss 2.3411 Accuracy 0.4984\n",
      "Epoch 21 Batch 1300 Loss 2.3414 Accuracy 0.4984\n",
      "Epoch 21 Batch 1350 Loss 2.3409 Accuracy 0.4985\n",
      "Epoch 21 Batch 1400 Loss 2.3417 Accuracy 0.4984\n",
      "Epoch 21 Batch 1450 Loss 2.3420 Accuracy 0.4983\n",
      "Epoch 21 Batch 1500 Loss 2.3423 Accuracy 0.4984\n",
      "Epoch 21 Batch 1550 Loss 2.3423 Accuracy 0.4984\n",
      "Epoch 21 Batch 1600 Loss 2.3431 Accuracy 0.4983\n",
      "Epoch 21 Batch 1650 Loss 2.3429 Accuracy 0.4984\n",
      "Epoch 21 Batch 1700 Loss 2.3435 Accuracy 0.4983\n",
      "Epoch 21 Batch 1750 Loss 2.3439 Accuracy 0.4984\n",
      "Epoch 21 Batch 1800 Loss 2.3442 Accuracy 0.4983\n",
      "Epoch 21 Batch 1850 Loss 2.3452 Accuracy 0.4982\n",
      "Epoch 21 Batch 1900 Loss 2.3460 Accuracy 0.4981\n",
      "Epoch 21 Batch 1950 Loss 2.3469 Accuracy 0.4980\n",
      "Epoch 21 Batch 2000 Loss 2.3464 Accuracy 0.4981\n",
      "Epoch 21 Batch 2050 Loss 2.3463 Accuracy 0.4980\n",
      "Epoch 21 Batch 2100 Loss 2.3463 Accuracy 0.4981\n",
      "Epoch 21 Batch 2150 Loss 2.3465 Accuracy 0.4980\n",
      "Epoch 21 Batch 2200 Loss 2.3467 Accuracy 0.4980\n",
      "Epoch 21 Batch 2250 Loss 2.3466 Accuracy 0.4980\n",
      "Epoch 21 Batch 2300 Loss 2.3466 Accuracy 0.4980\n",
      "Epoch 21 Batch 2350 Loss 2.3470 Accuracy 0.4979\n",
      "Epoch 21 Batch 2400 Loss 2.3473 Accuracy 0.4979\n",
      "Epoch 21 Batch 2450 Loss 2.3479 Accuracy 0.4978\n",
      "Epoch 21 Batch 2500 Loss 2.3483 Accuracy 0.4977\n",
      "Epoch 21 Batch 2550 Loss 2.3488 Accuracy 0.4977\n",
      "Epoch 21 Batch 2600 Loss 2.3490 Accuracy 0.4976\n",
      "Epoch 21 Loss 2.3491 Accuracy 0.4976\n",
      "Time taken for 1 epoch: 455.3312976360321 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss 2.3571 Accuracy 0.4916\n",
      "Epoch 22 Batch 50 Loss 2.3164 Accuracy 0.5008\n",
      "Epoch 22 Batch 100 Loss 2.3247 Accuracy 0.5000\n",
      "Epoch 22 Batch 150 Loss 2.3241 Accuracy 0.5006\n",
      "Epoch 22 Batch 200 Loss 2.3232 Accuracy 0.5011\n",
      "Epoch 22 Batch 250 Loss 2.3244 Accuracy 0.5006\n",
      "Epoch 22 Batch 300 Loss 2.3265 Accuracy 0.5003\n",
      "Epoch 22 Batch 350 Loss 2.3277 Accuracy 0.5004\n",
      "Epoch 22 Batch 400 Loss 2.3289 Accuracy 0.5000\n",
      "Epoch 22 Batch 450 Loss 2.3302 Accuracy 0.4999\n",
      "Epoch 22 Batch 500 Loss 2.3287 Accuracy 0.5003\n",
      "Epoch 22 Batch 550 Loss 2.3283 Accuracy 0.5004\n",
      "Epoch 22 Batch 600 Loss 2.3292 Accuracy 0.5004\n",
      "Epoch 22 Batch 650 Loss 2.3300 Accuracy 0.5003\n",
      "Epoch 22 Batch 700 Loss 2.3319 Accuracy 0.5000\n",
      "Epoch 22 Batch 750 Loss 2.3326 Accuracy 0.4998\n",
      "Epoch 22 Batch 800 Loss 2.3327 Accuracy 0.4997\n",
      "Epoch 22 Batch 850 Loss 2.3329 Accuracy 0.4998\n",
      "Epoch 22 Batch 900 Loss 2.3332 Accuracy 0.4998\n",
      "Epoch 22 Batch 950 Loss 2.3329 Accuracy 0.4998\n",
      "Epoch 22 Batch 1000 Loss 2.3332 Accuracy 0.4997\n",
      "Epoch 22 Batch 1050 Loss 2.3339 Accuracy 0.4996\n",
      "Epoch 22 Batch 1100 Loss 2.3346 Accuracy 0.4995\n",
      "Epoch 22 Batch 1150 Loss 2.3347 Accuracy 0.4994\n",
      "Epoch 22 Batch 1200 Loss 2.3354 Accuracy 0.4993\n",
      "Epoch 22 Batch 1250 Loss 2.3356 Accuracy 0.4992\n",
      "Epoch 22 Batch 1300 Loss 2.3352 Accuracy 0.4993\n",
      "Epoch 22 Batch 1350 Loss 2.3357 Accuracy 0.4992\n",
      "Epoch 22 Batch 1400 Loss 2.3357 Accuracy 0.4993\n",
      "Epoch 22 Batch 1450 Loss 2.3364 Accuracy 0.4991\n",
      "Epoch 22 Batch 1500 Loss 2.3365 Accuracy 0.4991\n",
      "Epoch 22 Batch 1550 Loss 2.3366 Accuracy 0.4991\n",
      "Epoch 22 Batch 1600 Loss 2.3370 Accuracy 0.4991\n",
      "Epoch 22 Batch 1650 Loss 2.3371 Accuracy 0.4992\n",
      "Epoch 22 Batch 1700 Loss 2.3372 Accuracy 0.4992\n",
      "Epoch 22 Batch 1750 Loss 2.3374 Accuracy 0.4992\n",
      "Epoch 22 Batch 1800 Loss 2.3378 Accuracy 0.4992\n",
      "Epoch 22 Batch 1850 Loss 2.3381 Accuracy 0.4991\n",
      "Epoch 22 Batch 1900 Loss 2.3384 Accuracy 0.4991\n",
      "Epoch 22 Batch 1950 Loss 2.3389 Accuracy 0.4989\n",
      "Epoch 22 Batch 2000 Loss 2.3391 Accuracy 0.4989\n",
      "Epoch 22 Batch 2050 Loss 2.3396 Accuracy 0.4988\n",
      "Epoch 22 Batch 2100 Loss 2.3395 Accuracy 0.4988\n",
      "Epoch 22 Batch 2150 Loss 2.3397 Accuracy 0.4989\n",
      "Epoch 22 Batch 2200 Loss 2.3400 Accuracy 0.4988\n",
      "Epoch 22 Batch 2250 Loss 2.3402 Accuracy 0.4988\n",
      "Epoch 22 Batch 2300 Loss 2.3403 Accuracy 0.4988\n",
      "Epoch 22 Batch 2350 Loss 2.3408 Accuracy 0.4987\n",
      "Epoch 22 Batch 2400 Loss 2.3408 Accuracy 0.4987\n",
      "Epoch 22 Batch 2450 Loss 2.3411 Accuracy 0.4987\n",
      "Epoch 22 Batch 2500 Loss 2.3415 Accuracy 0.4987\n",
      "Epoch 22 Batch 2550 Loss 2.3422 Accuracy 0.4986\n",
      "Epoch 22 Batch 2600 Loss 2.3426 Accuracy 0.4986\n",
      "Epoch 22 Loss 2.3429 Accuracy 0.4985\n",
      "Time taken for 1 epoch: 472.37154722213745 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss 2.3659 Accuracy 0.4943\n",
      "Epoch 23 Batch 50 Loss 2.3123 Accuracy 0.5021\n",
      "Epoch 23 Batch 100 Loss 2.3162 Accuracy 0.5031\n",
      "Epoch 23 Batch 150 Loss 2.3172 Accuracy 0.5028\n",
      "Epoch 23 Batch 200 Loss 2.3217 Accuracy 0.5013\n",
      "Epoch 23 Batch 250 Loss 2.3233 Accuracy 0.5009\n",
      "Epoch 23 Batch 300 Loss 2.3254 Accuracy 0.5005\n",
      "Epoch 23 Batch 350 Loss 2.3273 Accuracy 0.5003\n",
      "Epoch 23 Batch 400 Loss 2.3275 Accuracy 0.5004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Batch 450 Loss 2.3275 Accuracy 0.5004\n",
      "Epoch 23 Batch 500 Loss 2.3287 Accuracy 0.5001\n",
      "Epoch 23 Batch 550 Loss 2.3299 Accuracy 0.5000\n",
      "Epoch 23 Batch 600 Loss 2.3295 Accuracy 0.5001\n",
      "Epoch 23 Batch 650 Loss 2.3310 Accuracy 0.5000\n",
      "Epoch 23 Batch 700 Loss 2.3316 Accuracy 0.4999\n",
      "Epoch 23 Batch 750 Loss 2.3313 Accuracy 0.4998\n",
      "Epoch 23 Batch 800 Loss 2.3301 Accuracy 0.5001\n",
      "Epoch 23 Batch 850 Loss 2.3302 Accuracy 0.5001\n",
      "Epoch 23 Batch 900 Loss 2.3297 Accuracy 0.5001\n",
      "Epoch 23 Batch 950 Loss 2.3302 Accuracy 0.5000\n",
      "Epoch 23 Batch 1000 Loss 2.3304 Accuracy 0.5000\n",
      "Epoch 23 Batch 1050 Loss 2.3301 Accuracy 0.5000\n",
      "Epoch 23 Batch 1100 Loss 2.3305 Accuracy 0.5000\n",
      "Epoch 23 Batch 1150 Loss 2.3308 Accuracy 0.4999\n",
      "Epoch 23 Batch 1200 Loss 2.3309 Accuracy 0.4999\n",
      "Epoch 23 Batch 1250 Loss 2.3308 Accuracy 0.4998\n",
      "Epoch 23 Batch 1300 Loss 2.3317 Accuracy 0.4998\n",
      "Epoch 23 Batch 1350 Loss 2.3330 Accuracy 0.4996\n",
      "Epoch 23 Batch 1400 Loss 2.3333 Accuracy 0.4996\n",
      "Epoch 23 Batch 1450 Loss 2.3336 Accuracy 0.4997\n",
      "Epoch 23 Batch 1500 Loss 2.3332 Accuracy 0.4998\n",
      "Epoch 23 Batch 1550 Loss 2.3335 Accuracy 0.4998\n",
      "Epoch 23 Batch 1600 Loss 2.3334 Accuracy 0.4997\n",
      "Epoch 23 Batch 1650 Loss 2.3338 Accuracy 0.4997\n",
      "Epoch 23 Batch 1700 Loss 2.3340 Accuracy 0.4997\n",
      "Epoch 23 Batch 1750 Loss 2.3340 Accuracy 0.4997\n",
      "Epoch 23 Batch 1800 Loss 2.3345 Accuracy 0.4997\n",
      "Epoch 23 Batch 1850 Loss 2.3348 Accuracy 0.4996\n",
      "Epoch 23 Batch 1900 Loss 2.3348 Accuracy 0.4996\n",
      "Epoch 23 Batch 1950 Loss 2.3349 Accuracy 0.4996\n",
      "Epoch 23 Batch 2000 Loss 2.3345 Accuracy 0.4997\n",
      "Epoch 23 Batch 2050 Loss 2.3348 Accuracy 0.4996\n",
      "Epoch 23 Batch 2100 Loss 2.3351 Accuracy 0.4996\n",
      "Epoch 23 Batch 2150 Loss 2.3352 Accuracy 0.4996\n",
      "Epoch 23 Batch 2200 Loss 2.3352 Accuracy 0.4996\n",
      "Epoch 23 Batch 2250 Loss 2.3355 Accuracy 0.4995\n",
      "Epoch 23 Batch 2300 Loss 2.3361 Accuracy 0.4994\n",
      "Epoch 23 Batch 2350 Loss 2.3366 Accuracy 0.4993\n",
      "Epoch 23 Batch 2400 Loss 2.3367 Accuracy 0.4993\n",
      "Epoch 23 Batch 2450 Loss 2.3368 Accuracy 0.4993\n",
      "Epoch 23 Batch 2500 Loss 2.3368 Accuracy 0.4993\n",
      "Epoch 23 Batch 2550 Loss 2.3372 Accuracy 0.4992\n",
      "Epoch 23 Batch 2600 Loss 2.3378 Accuracy 0.4991\n",
      "Epoch 23 Loss 2.3385 Accuracy 0.4991\n",
      "Time taken for 1 epoch: 482.8161199092865 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss 2.3721 Accuracy 0.4843\n",
      "Epoch 24 Batch 50 Loss 2.3106 Accuracy 0.5043\n",
      "Epoch 24 Batch 100 Loss 2.3070 Accuracy 0.5035\n",
      "Epoch 24 Batch 150 Loss 2.3063 Accuracy 0.5032\n",
      "Epoch 24 Batch 200 Loss 2.3105 Accuracy 0.5024\n",
      "Epoch 24 Batch 250 Loss 2.3120 Accuracy 0.5023\n",
      "Epoch 24 Batch 300 Loss 2.3128 Accuracy 0.5021\n",
      "Epoch 24 Batch 350 Loss 2.3145 Accuracy 0.5020\n",
      "Epoch 24 Batch 400 Loss 2.3157 Accuracy 0.5016\n",
      "Epoch 24 Batch 450 Loss 2.3171 Accuracy 0.5013\n",
      "Epoch 24 Batch 500 Loss 2.3198 Accuracy 0.5010\n",
      "Epoch 24 Batch 550 Loss 2.3213 Accuracy 0.5009\n",
      "Epoch 24 Batch 600 Loss 2.3216 Accuracy 0.5010\n",
      "Epoch 24 Batch 650 Loss 2.3219 Accuracy 0.5007\n",
      "Epoch 24 Batch 700 Loss 2.3225 Accuracy 0.5007\n",
      "Epoch 24 Batch 750 Loss 2.3220 Accuracy 0.5008\n",
      "Epoch 24 Batch 800 Loss 2.3227 Accuracy 0.5006\n",
      "Epoch 24 Batch 850 Loss 2.3232 Accuracy 0.5006\n",
      "Epoch 24 Batch 900 Loss 2.3231 Accuracy 0.5008\n",
      "Epoch 24 Batch 950 Loss 2.3230 Accuracy 0.5008\n",
      "Epoch 24 Batch 1000 Loss 2.3230 Accuracy 0.5009\n",
      "Epoch 24 Batch 1050 Loss 2.3233 Accuracy 0.5010\n",
      "Epoch 24 Batch 1100 Loss 2.3232 Accuracy 0.5010\n",
      "Epoch 24 Batch 1150 Loss 2.3237 Accuracy 0.5009\n",
      "Epoch 24 Batch 1200 Loss 2.3240 Accuracy 0.5010\n",
      "Epoch 24 Batch 1250 Loss 2.3238 Accuracy 0.5010\n",
      "Epoch 24 Batch 1300 Loss 2.3241 Accuracy 0.5009\n",
      "Epoch 24 Batch 1350 Loss 2.3248 Accuracy 0.5008\n",
      "Epoch 24 Batch 1400 Loss 2.3257 Accuracy 0.5008\n",
      "Epoch 24 Batch 1450 Loss 2.3261 Accuracy 0.5008\n",
      "Epoch 24 Batch 1500 Loss 2.3263 Accuracy 0.5008\n",
      "Epoch 24 Batch 1550 Loss 2.3266 Accuracy 0.5007\n",
      "Epoch 24 Batch 1600 Loss 2.3273 Accuracy 0.5006\n",
      "Epoch 24 Batch 1650 Loss 2.3270 Accuracy 0.5005\n",
      "Epoch 24 Batch 1700 Loss 2.3270 Accuracy 0.5006\n",
      "Epoch 24 Batch 1750 Loss 2.3271 Accuracy 0.5006\n",
      "Epoch 24 Batch 1800 Loss 2.3276 Accuracy 0.5005\n",
      "Epoch 24 Batch 1850 Loss 2.3278 Accuracy 0.5004\n",
      "Epoch 24 Batch 1900 Loss 2.3282 Accuracy 0.5004\n",
      "Epoch 24 Batch 1950 Loss 2.3283 Accuracy 0.5004\n",
      "Epoch 24 Batch 2000 Loss 2.3286 Accuracy 0.5003\n",
      "Epoch 24 Batch 2050 Loss 2.3291 Accuracy 0.5002\n",
      "Epoch 24 Batch 2100 Loss 2.3295 Accuracy 0.5002\n",
      "Epoch 24 Batch 2150 Loss 2.3295 Accuracy 0.5003\n",
      "Epoch 24 Batch 2200 Loss 2.3298 Accuracy 0.5002\n",
      "Epoch 24 Batch 2250 Loss 2.3300 Accuracy 0.5002\n",
      "Epoch 24 Batch 2300 Loss 2.3302 Accuracy 0.5002\n",
      "Epoch 24 Batch 2350 Loss 2.3298 Accuracy 0.5002\n",
      "Epoch 24 Batch 2400 Loss 2.3300 Accuracy 0.5002\n",
      "Epoch 24 Batch 2450 Loss 2.3305 Accuracy 0.5001\n",
      "Epoch 24 Batch 2500 Loss 2.3311 Accuracy 0.5001\n",
      "Epoch 24 Batch 2550 Loss 2.3317 Accuracy 0.5000\n",
      "Epoch 24 Batch 2600 Loss 2.3321 Accuracy 0.4999\n",
      "Epoch 24 Loss 2.3323 Accuracy 0.4999\n",
      "Time taken for 1 epoch: 483.6716272830963 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss 2.3123 Accuracy 0.5110\n",
      "Epoch 25 Batch 50 Loss 2.2904 Accuracy 0.5053\n",
      "Epoch 25 Batch 100 Loss 2.2965 Accuracy 0.5044\n",
      "Epoch 25 Batch 150 Loss 2.2991 Accuracy 0.5038\n",
      "Epoch 25 Batch 200 Loss 2.2976 Accuracy 0.5040\n",
      "Epoch 25 Batch 250 Loss 2.3018 Accuracy 0.5035\n",
      "Epoch 25 Batch 300 Loss 2.3033 Accuracy 0.5036\n",
      "Epoch 25 Batch 350 Loss 2.3088 Accuracy 0.5029\n",
      "Epoch 25 Batch 400 Loss 2.3114 Accuracy 0.5028\n",
      "Epoch 25 Batch 450 Loss 2.3128 Accuracy 0.5026\n",
      "Epoch 25 Batch 500 Loss 2.3142 Accuracy 0.5021\n",
      "Epoch 25 Batch 550 Loss 2.3145 Accuracy 0.5022\n",
      "Epoch 25 Batch 600 Loss 2.3149 Accuracy 0.5022\n",
      "Epoch 25 Batch 650 Loss 2.3161 Accuracy 0.5018\n",
      "Epoch 25 Batch 700 Loss 2.3163 Accuracy 0.5018\n",
      "Epoch 25 Batch 750 Loss 2.3167 Accuracy 0.5017\n",
      "Epoch 25 Batch 800 Loss 2.3182 Accuracy 0.5015\n",
      "Epoch 25 Batch 850 Loss 2.3189 Accuracy 0.5013\n",
      "Epoch 25 Batch 900 Loss 2.3188 Accuracy 0.5014\n",
      "Epoch 25 Batch 950 Loss 2.3189 Accuracy 0.5014\n",
      "Epoch 25 Batch 1000 Loss 2.3187 Accuracy 0.5015\n",
      "Epoch 25 Batch 1050 Loss 2.3189 Accuracy 0.5015\n",
      "Epoch 25 Batch 1100 Loss 2.3189 Accuracy 0.5015\n",
      "Epoch 25 Batch 1150 Loss 2.3191 Accuracy 0.5016\n",
      "Epoch 25 Batch 1200 Loss 2.3194 Accuracy 0.5015\n",
      "Epoch 25 Batch 1250 Loss 2.3202 Accuracy 0.5013\n",
      "Epoch 25 Batch 1300 Loss 2.3204 Accuracy 0.5013\n",
      "Epoch 25 Batch 1350 Loss 2.3212 Accuracy 0.5012\n",
      "Epoch 25 Batch 1400 Loss 2.3214 Accuracy 0.5012\n",
      "Epoch 25 Batch 1450 Loss 2.3214 Accuracy 0.5012\n",
      "Epoch 25 Batch 1500 Loss 2.3218 Accuracy 0.5012\n",
      "Epoch 25 Batch 1550 Loss 2.3225 Accuracy 0.5011\n",
      "Epoch 25 Batch 1600 Loss 2.3221 Accuracy 0.5012\n",
      "Epoch 25 Batch 1650 Loss 2.3219 Accuracy 0.5012\n",
      "Epoch 25 Batch 1700 Loss 2.3218 Accuracy 0.5012\n",
      "Epoch 25 Batch 1750 Loss 2.3222 Accuracy 0.5011\n",
      "Epoch 25 Batch 1800 Loss 2.3225 Accuracy 0.5011\n",
      "Epoch 25 Batch 1850 Loss 2.3231 Accuracy 0.5010\n",
      "Epoch 25 Batch 1900 Loss 2.3235 Accuracy 0.5009\n",
      "Epoch 25 Batch 1950 Loss 2.3239 Accuracy 0.5009\n",
      "Epoch 25 Batch 2000 Loss 2.3240 Accuracy 0.5009\n",
      "Epoch 25 Batch 2050 Loss 2.3242 Accuracy 0.5009\n",
      "Epoch 25 Batch 2100 Loss 2.3242 Accuracy 0.5009\n",
      "Epoch 25 Batch 2150 Loss 2.3243 Accuracy 0.5009\n",
      "Epoch 25 Batch 2200 Loss 2.3247 Accuracy 0.5008\n",
      "Epoch 25 Batch 2250 Loss 2.3249 Accuracy 0.5008\n",
      "Epoch 25 Batch 2300 Loss 2.3250 Accuracy 0.5008\n",
      "Epoch 25 Batch 2350 Loss 2.3247 Accuracy 0.5008\n",
      "Epoch 25 Batch 2400 Loss 2.3248 Accuracy 0.5008\n",
      "Epoch 25 Batch 2450 Loss 2.3250 Accuracy 0.5008\n",
      "Epoch 25 Batch 2500 Loss 2.3254 Accuracy 0.5007\n",
      "Epoch 25 Batch 2550 Loss 2.3261 Accuracy 0.5007\n",
      "Epoch 25 Batch 2600 Loss 2.3264 Accuracy 0.5007\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/train_full/ckpt-15\n",
      "Epoch 25 Loss 2.3267 Accuracy 0.5006\n",
      "Time taken for 1 epoch: 460.030202627182 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss 2.1616 Accuracy 0.5448\n",
      "Epoch 26 Batch 50 Loss 2.3088 Accuracy 0.5026\n",
      "Epoch 26 Batch 100 Loss 2.3042 Accuracy 0.5030\n",
      "Epoch 26 Batch 150 Loss 2.3023 Accuracy 0.5036\n",
      "Epoch 26 Batch 200 Loss 2.3030 Accuracy 0.5034\n",
      "Epoch 26 Batch 250 Loss 2.3022 Accuracy 0.5033\n",
      "Epoch 26 Batch 300 Loss 2.3026 Accuracy 0.5034\n",
      "Epoch 26 Batch 350 Loss 2.3039 Accuracy 0.5035\n",
      "Epoch 26 Batch 400 Loss 2.3046 Accuracy 0.5034\n",
      "Epoch 26 Batch 450 Loss 2.3062 Accuracy 0.5033\n",
      "Epoch 26 Batch 500 Loss 2.3075 Accuracy 0.5031\n",
      "Epoch 26 Batch 550 Loss 2.3105 Accuracy 0.5027\n",
      "Epoch 26 Batch 600 Loss 2.3116 Accuracy 0.5024\n",
      "Epoch 26 Batch 650 Loss 2.3106 Accuracy 0.5027\n",
      "Epoch 26 Batch 700 Loss 2.3106 Accuracy 0.5027\n",
      "Epoch 26 Batch 750 Loss 2.3101 Accuracy 0.5030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Batch 800 Loss 2.3098 Accuracy 0.5031\n",
      "Epoch 26 Batch 850 Loss 2.3099 Accuracy 0.5030\n",
      "Epoch 26 Batch 900 Loss 2.3095 Accuracy 0.5032\n",
      "Epoch 26 Batch 950 Loss 2.3094 Accuracy 0.5030\n",
      "Epoch 26 Batch 1000 Loss 2.3108 Accuracy 0.5028\n",
      "Epoch 26 Batch 1050 Loss 2.3111 Accuracy 0.5028\n",
      "Epoch 26 Batch 1100 Loss 2.3111 Accuracy 0.5028\n",
      "Epoch 26 Batch 1150 Loss 2.3114 Accuracy 0.5027\n",
      "Epoch 26 Batch 1200 Loss 2.3118 Accuracy 0.5026\n",
      "Epoch 26 Batch 1250 Loss 2.3116 Accuracy 0.5027\n",
      "Epoch 26 Batch 1300 Loss 2.3126 Accuracy 0.5025\n",
      "Epoch 26 Batch 1350 Loss 2.3127 Accuracy 0.5025\n",
      "Epoch 26 Batch 1400 Loss 2.3130 Accuracy 0.5025\n",
      "Epoch 26 Batch 1450 Loss 2.3135 Accuracy 0.5024\n",
      "Epoch 26 Batch 1500 Loss 2.3136 Accuracy 0.5025\n",
      "Epoch 26 Batch 1550 Loss 2.3139 Accuracy 0.5025\n",
      "Epoch 26 Batch 1600 Loss 2.3137 Accuracy 0.5025\n",
      "Epoch 26 Batch 1650 Loss 2.3142 Accuracy 0.5025\n",
      "Epoch 26 Batch 1700 Loss 2.3144 Accuracy 0.5025\n",
      "Epoch 26 Batch 1750 Loss 2.3148 Accuracy 0.5025\n",
      "Epoch 26 Batch 1800 Loss 2.3150 Accuracy 0.5024\n",
      "Epoch 26 Batch 1850 Loss 2.3150 Accuracy 0.5024\n",
      "Epoch 26 Batch 1900 Loss 2.3157 Accuracy 0.5023\n",
      "Epoch 26 Batch 1950 Loss 2.3160 Accuracy 0.5023\n",
      "Epoch 26 Batch 2000 Loss 2.3161 Accuracy 0.5022\n",
      "Epoch 26 Batch 2050 Loss 2.3156 Accuracy 0.5023\n",
      "Epoch 26 Batch 2100 Loss 2.3162 Accuracy 0.5021\n",
      "Epoch 26 Batch 2150 Loss 2.3170 Accuracy 0.5020\n",
      "Epoch 26 Batch 2200 Loss 2.3174 Accuracy 0.5019\n",
      "Epoch 26 Batch 2250 Loss 2.3177 Accuracy 0.5020\n",
      "Epoch 26 Batch 2300 Loss 2.3181 Accuracy 0.5019\n",
      "Epoch 26 Batch 2350 Loss 2.3182 Accuracy 0.5018\n",
      "Epoch 26 Batch 2400 Loss 2.3186 Accuracy 0.5018\n",
      "Epoch 26 Batch 2450 Loss 2.3188 Accuracy 0.5018\n",
      "Epoch 26 Batch 2500 Loss 2.3190 Accuracy 0.5018\n",
      "Epoch 26 Batch 2550 Loss 2.3198 Accuracy 0.5017\n",
      "Epoch 26 Batch 2600 Loss 2.3204 Accuracy 0.5016\n",
      "Epoch 26 Loss 2.3205 Accuracy 0.5016\n",
      "Time taken for 1 epoch: 457.91052293777466 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss 2.2311 Accuracy 0.5164\n",
      "Epoch 27 Batch 50 Loss 2.2964 Accuracy 0.5042\n",
      "Epoch 27 Batch 100 Loss 2.2907 Accuracy 0.5051\n",
      "Epoch 27 Batch 150 Loss 2.2910 Accuracy 0.5049\n",
      "Epoch 27 Batch 200 Loss 2.2912 Accuracy 0.5049\n",
      "Epoch 27 Batch 250 Loss 2.2935 Accuracy 0.5047\n",
      "Epoch 27 Batch 300 Loss 2.2929 Accuracy 0.5048\n",
      "Epoch 27 Batch 350 Loss 2.2956 Accuracy 0.5045\n",
      "Epoch 27 Batch 400 Loss 2.2986 Accuracy 0.5040\n",
      "Epoch 27 Batch 450 Loss 2.2986 Accuracy 0.5041\n",
      "Epoch 27 Batch 500 Loss 2.3007 Accuracy 0.5036\n",
      "Epoch 27 Batch 550 Loss 2.3013 Accuracy 0.5036\n",
      "Epoch 27 Batch 600 Loss 2.3011 Accuracy 0.5037\n",
      "Epoch 27 Batch 650 Loss 2.3010 Accuracy 0.5039\n",
      "Epoch 27 Batch 700 Loss 2.3026 Accuracy 0.5036\n",
      "Epoch 27 Batch 750 Loss 2.3019 Accuracy 0.5038\n",
      "Epoch 27 Batch 800 Loss 2.3034 Accuracy 0.5035\n",
      "Epoch 27 Batch 850 Loss 2.3052 Accuracy 0.5033\n",
      "Epoch 27 Batch 900 Loss 2.3053 Accuracy 0.5034\n",
      "Epoch 27 Batch 950 Loss 2.3055 Accuracy 0.5033\n",
      "Epoch 27 Batch 1000 Loss 2.3057 Accuracy 0.5034\n",
      "Epoch 27 Batch 1050 Loss 2.3064 Accuracy 0.5034\n",
      "Epoch 27 Batch 1100 Loss 2.3064 Accuracy 0.5033\n",
      "Epoch 27 Batch 1150 Loss 2.3061 Accuracy 0.5032\n",
      "Epoch 27 Batch 1200 Loss 2.3062 Accuracy 0.5033\n",
      "Epoch 27 Batch 1250 Loss 2.3068 Accuracy 0.5033\n",
      "Epoch 27 Batch 1300 Loss 2.3073 Accuracy 0.5032\n",
      "Epoch 27 Batch 1350 Loss 2.3078 Accuracy 0.5031\n",
      "Epoch 27 Batch 1400 Loss 2.3078 Accuracy 0.5031\n",
      "Epoch 27 Batch 1450 Loss 2.3083 Accuracy 0.5030\n",
      "Epoch 27 Batch 1500 Loss 2.3087 Accuracy 0.5029\n",
      "Epoch 27 Batch 1550 Loss 2.3088 Accuracy 0.5029\n",
      "Epoch 27 Batch 1600 Loss 2.3090 Accuracy 0.5030\n",
      "Epoch 27 Batch 1650 Loss 2.3093 Accuracy 0.5030\n",
      "Epoch 27 Batch 1700 Loss 2.3094 Accuracy 0.5030\n",
      "Epoch 27 Batch 1750 Loss 2.3098 Accuracy 0.5030\n",
      "Epoch 27 Batch 1800 Loss 2.3106 Accuracy 0.5029\n",
      "Epoch 27 Batch 1850 Loss 2.3109 Accuracy 0.5028\n",
      "Epoch 27 Batch 1900 Loss 2.3111 Accuracy 0.5028\n",
      "Epoch 27 Batch 1950 Loss 2.3111 Accuracy 0.5027\n",
      "Epoch 27 Batch 2000 Loss 2.3111 Accuracy 0.5027\n",
      "Epoch 27 Batch 2050 Loss 2.3113 Accuracy 0.5027\n",
      "Epoch 27 Batch 2100 Loss 2.3116 Accuracy 0.5027\n",
      "Epoch 27 Batch 2150 Loss 2.3120 Accuracy 0.5026\n",
      "Epoch 27 Batch 2200 Loss 2.3123 Accuracy 0.5026\n",
      "Epoch 27 Batch 2250 Loss 2.3128 Accuracy 0.5026\n",
      "Epoch 27 Batch 2300 Loss 2.3132 Accuracy 0.5025\n",
      "Epoch 27 Batch 2350 Loss 2.3130 Accuracy 0.5025\n",
      "Epoch 27 Batch 2400 Loss 2.3129 Accuracy 0.5026\n",
      "Epoch 27 Batch 2450 Loss 2.3129 Accuracy 0.5026\n",
      "Epoch 27 Batch 2500 Loss 2.3133 Accuracy 0.5025\n",
      "Epoch 27 Batch 2550 Loss 2.3138 Accuracy 0.5025\n",
      "Epoch 27 Batch 2600 Loss 2.3145 Accuracy 0.5024\n",
      "Epoch 27 Loss 2.3148 Accuracy 0.5023\n",
      "Time taken for 1 epoch: 455.80443596839905 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss 2.1448 Accuracy 0.5446\n",
      "Epoch 28 Batch 50 Loss 2.2870 Accuracy 0.5066\n",
      "Epoch 28 Batch 100 Loss 2.2909 Accuracy 0.5042\n",
      "Epoch 28 Batch 150 Loss 2.2961 Accuracy 0.5037\n",
      "Epoch 28 Batch 200 Loss 2.2964 Accuracy 0.5042\n",
      "Epoch 28 Batch 250 Loss 2.2984 Accuracy 0.5039\n",
      "Epoch 28 Batch 300 Loss 2.2967 Accuracy 0.5043\n",
      "Epoch 28 Batch 350 Loss 2.2939 Accuracy 0.5046\n",
      "Epoch 28 Batch 400 Loss 2.2977 Accuracy 0.5042\n",
      "Epoch 28 Batch 450 Loss 2.2977 Accuracy 0.5040\n",
      "Epoch 28 Batch 500 Loss 2.2964 Accuracy 0.5042\n",
      "Epoch 28 Batch 550 Loss 2.2996 Accuracy 0.5037\n",
      "Epoch 28 Batch 600 Loss 2.2999 Accuracy 0.5036\n",
      "Epoch 28 Batch 650 Loss 2.2998 Accuracy 0.5039\n",
      "Epoch 28 Batch 700 Loss 2.2994 Accuracy 0.5040\n",
      "Epoch 28 Batch 750 Loss 2.2989 Accuracy 0.5041\n",
      "Epoch 28 Batch 800 Loss 2.2987 Accuracy 0.5042\n",
      "Epoch 28 Batch 850 Loss 2.2996 Accuracy 0.5040\n",
      "Epoch 28 Batch 900 Loss 2.2996 Accuracy 0.5039\n",
      "Epoch 28 Batch 950 Loss 2.3000 Accuracy 0.5039\n",
      "Epoch 28 Batch 1000 Loss 2.3004 Accuracy 0.5039\n",
      "Epoch 28 Batch 1050 Loss 2.3008 Accuracy 0.5038\n",
      "Epoch 28 Batch 1100 Loss 2.3015 Accuracy 0.5037\n",
      "Epoch 28 Batch 1150 Loss 2.3021 Accuracy 0.5036\n",
      "Epoch 28 Batch 1200 Loss 2.3021 Accuracy 0.5036\n",
      "Epoch 28 Batch 1250 Loss 2.3019 Accuracy 0.5036\n",
      "Epoch 28 Batch 1300 Loss 2.3029 Accuracy 0.5035\n",
      "Epoch 28 Batch 1350 Loss 2.3034 Accuracy 0.5034\n",
      "Epoch 28 Batch 1400 Loss 2.3038 Accuracy 0.5033\n",
      "Epoch 28 Batch 1450 Loss 2.3041 Accuracy 0.5033\n",
      "Epoch 28 Batch 1500 Loss 2.3037 Accuracy 0.5035\n",
      "Epoch 28 Batch 1550 Loss 2.3035 Accuracy 0.5035\n",
      "Epoch 28 Batch 1600 Loss 2.3034 Accuracy 0.5037\n",
      "Epoch 28 Batch 1650 Loss 2.3039 Accuracy 0.5036\n",
      "Epoch 28 Batch 1700 Loss 2.3043 Accuracy 0.5035\n",
      "Epoch 28 Batch 1750 Loss 2.3045 Accuracy 0.5035\n",
      "Epoch 28 Batch 1800 Loss 2.3048 Accuracy 0.5035\n",
      "Epoch 28 Batch 1850 Loss 2.3048 Accuracy 0.5035\n",
      "Epoch 28 Batch 1900 Loss 2.3051 Accuracy 0.5034\n",
      "Epoch 28 Batch 1950 Loss 2.3056 Accuracy 0.5033\n",
      "Epoch 28 Batch 2000 Loss 2.3059 Accuracy 0.5033\n",
      "Epoch 28 Batch 2050 Loss 2.3060 Accuracy 0.5033\n",
      "Epoch 28 Batch 2100 Loss 2.3058 Accuracy 0.5033\n",
      "Epoch 28 Batch 2150 Loss 2.3059 Accuracy 0.5033\n",
      "Epoch 28 Batch 2200 Loss 2.3067 Accuracy 0.5032\n",
      "Epoch 28 Batch 2250 Loss 2.3067 Accuracy 0.5032\n",
      "Epoch 28 Batch 2300 Loss 2.3069 Accuracy 0.5033\n",
      "Epoch 28 Batch 2350 Loss 2.3070 Accuracy 0.5032\n",
      "Epoch 28 Batch 2400 Loss 2.3071 Accuracy 0.5033\n",
      "Epoch 28 Batch 2450 Loss 2.3079 Accuracy 0.5031\n",
      "Epoch 28 Batch 2500 Loss 2.3081 Accuracy 0.5031\n",
      "Epoch 28 Batch 2550 Loss 2.3087 Accuracy 0.5030\n",
      "Epoch 28 Batch 2600 Loss 2.3089 Accuracy 0.5030\n",
      "Epoch 28 Loss 2.3092 Accuracy 0.5029\n",
      "Time taken for 1 epoch: 455.89950609207153 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss 2.2733 Accuracy 0.5174\n",
      "Epoch 29 Batch 50 Loss 2.2996 Accuracy 0.5031\n",
      "Epoch 29 Batch 100 Loss 2.2854 Accuracy 0.5057\n",
      "Epoch 29 Batch 150 Loss 2.2855 Accuracy 0.5063\n",
      "Epoch 29 Batch 200 Loss 2.2884 Accuracy 0.5055\n",
      "Epoch 29 Batch 250 Loss 2.2866 Accuracy 0.5054\n",
      "Epoch 29 Batch 300 Loss 2.2884 Accuracy 0.5054\n",
      "Epoch 29 Batch 350 Loss 2.2892 Accuracy 0.5054\n",
      "Epoch 29 Batch 400 Loss 2.2894 Accuracy 0.5057\n",
      "Epoch 29 Batch 450 Loss 2.2884 Accuracy 0.5059\n",
      "Epoch 29 Batch 500 Loss 2.2890 Accuracy 0.5059\n",
      "Epoch 29 Batch 550 Loss 2.2896 Accuracy 0.5058\n",
      "Epoch 29 Batch 600 Loss 2.2914 Accuracy 0.5057\n",
      "Epoch 29 Batch 650 Loss 2.2915 Accuracy 0.5056\n",
      "Epoch 29 Batch 700 Loss 2.2915 Accuracy 0.5056\n",
      "Epoch 29 Batch 750 Loss 2.2926 Accuracy 0.5054\n",
      "Epoch 29 Batch 800 Loss 2.2930 Accuracy 0.5054\n",
      "Epoch 29 Batch 850 Loss 2.2936 Accuracy 0.5053\n",
      "Epoch 29 Batch 900 Loss 2.2938 Accuracy 0.5053\n",
      "Epoch 29 Batch 950 Loss 2.2938 Accuracy 0.5054\n",
      "Epoch 29 Batch 1000 Loss 2.2934 Accuracy 0.5056\n",
      "Epoch 29 Batch 1050 Loss 2.2937 Accuracy 0.5055\n",
      "Epoch 29 Batch 1100 Loss 2.2936 Accuracy 0.5055\n",
      "Epoch 29 Batch 1150 Loss 2.2944 Accuracy 0.5054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Batch 1200 Loss 2.2959 Accuracy 0.5051\n",
      "Epoch 29 Batch 1250 Loss 2.2956 Accuracy 0.5052\n",
      "Epoch 29 Batch 1300 Loss 2.2960 Accuracy 0.5051\n",
      "Epoch 29 Batch 1350 Loss 2.2970 Accuracy 0.5050\n",
      "Epoch 29 Batch 1400 Loss 2.2977 Accuracy 0.5049\n",
      "Epoch 29 Batch 1450 Loss 2.2982 Accuracy 0.5049\n",
      "Epoch 29 Batch 1500 Loss 2.2990 Accuracy 0.5048\n",
      "Epoch 29 Batch 1550 Loss 2.2993 Accuracy 0.5048\n",
      "Epoch 29 Batch 1600 Loss 2.2989 Accuracy 0.5049\n",
      "Epoch 29 Batch 1650 Loss 2.2995 Accuracy 0.5049\n",
      "Epoch 29 Batch 1700 Loss 2.3000 Accuracy 0.5047\n",
      "Epoch 29 Batch 1750 Loss 2.2999 Accuracy 0.5047\n",
      "Epoch 29 Batch 1800 Loss 2.3002 Accuracy 0.5046\n",
      "Epoch 29 Batch 1850 Loss 2.3004 Accuracy 0.5045\n",
      "Epoch 29 Batch 1900 Loss 2.3007 Accuracy 0.5045\n",
      "Epoch 29 Batch 1950 Loss 2.3009 Accuracy 0.5045\n",
      "Epoch 29 Batch 2000 Loss 2.3012 Accuracy 0.5045\n",
      "Epoch 29 Batch 2050 Loss 2.3015 Accuracy 0.5044\n",
      "Epoch 29 Batch 2100 Loss 2.3013 Accuracy 0.5045\n",
      "Epoch 29 Batch 2150 Loss 2.3015 Accuracy 0.5044\n",
      "Epoch 29 Batch 2200 Loss 2.3016 Accuracy 0.5043\n",
      "Epoch 29 Batch 2250 Loss 2.3014 Accuracy 0.5044\n",
      "Epoch 29 Batch 2300 Loss 2.3018 Accuracy 0.5043\n",
      "Epoch 29 Batch 2350 Loss 2.3020 Accuracy 0.5043\n",
      "Epoch 29 Batch 2400 Loss 2.3019 Accuracy 0.5043\n",
      "Epoch 29 Batch 2450 Loss 2.3024 Accuracy 0.5042\n",
      "Epoch 29 Batch 2500 Loss 2.3030 Accuracy 0.5041\n",
      "Epoch 29 Batch 2550 Loss 2.3038 Accuracy 0.5040\n",
      "Epoch 29 Batch 2600 Loss 2.3038 Accuracy 0.5040\n",
      "Epoch 29 Loss 2.3041 Accuracy 0.5040\n",
      "Time taken for 1 epoch: 455.4238760471344 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss 2.2366 Accuracy 0.5200\n",
      "Epoch 30 Batch 50 Loss 2.2851 Accuracy 0.5036\n",
      "Epoch 30 Batch 100 Loss 2.2785 Accuracy 0.5063\n",
      "Epoch 30 Batch 150 Loss 2.2811 Accuracy 0.5068\n",
      "Epoch 30 Batch 200 Loss 2.2850 Accuracy 0.5055\n",
      "Epoch 30 Batch 250 Loss 2.2830 Accuracy 0.5058\n",
      "Epoch 30 Batch 300 Loss 2.2842 Accuracy 0.5059\n",
      "Epoch 30 Batch 350 Loss 2.2822 Accuracy 0.5065\n",
      "Epoch 30 Batch 400 Loss 2.2854 Accuracy 0.5062\n",
      "Epoch 30 Batch 450 Loss 2.2854 Accuracy 0.5062\n",
      "Epoch 30 Batch 500 Loss 2.2840 Accuracy 0.5062\n",
      "Epoch 30 Batch 550 Loss 2.2838 Accuracy 0.5063\n",
      "Epoch 30 Batch 600 Loss 2.2857 Accuracy 0.5059\n",
      "Epoch 30 Batch 650 Loss 2.2862 Accuracy 0.5058\n",
      "Epoch 30 Batch 700 Loss 2.2866 Accuracy 0.5057\n",
      "Epoch 30 Batch 750 Loss 2.2875 Accuracy 0.5057\n",
      "Epoch 30 Batch 800 Loss 2.2887 Accuracy 0.5056\n",
      "Epoch 30 Batch 850 Loss 2.2889 Accuracy 0.5055\n",
      "Epoch 30 Batch 900 Loss 2.2897 Accuracy 0.5055\n",
      "Epoch 30 Batch 950 Loss 2.2904 Accuracy 0.5053\n",
      "Epoch 30 Batch 1000 Loss 2.2908 Accuracy 0.5053\n",
      "Epoch 30 Batch 1050 Loss 2.2906 Accuracy 0.5053\n",
      "Epoch 30 Batch 1100 Loss 2.2910 Accuracy 0.5052\n",
      "Epoch 30 Batch 1150 Loss 2.2913 Accuracy 0.5051\n",
      "Epoch 30 Batch 1200 Loss 2.2914 Accuracy 0.5052\n",
      "Epoch 30 Batch 1250 Loss 2.2920 Accuracy 0.5051\n",
      "Epoch 30 Batch 1300 Loss 2.2919 Accuracy 0.5052\n",
      "Epoch 30 Batch 1350 Loss 2.2918 Accuracy 0.5051\n",
      "Epoch 30 Batch 1400 Loss 2.2921 Accuracy 0.5052\n",
      "Epoch 30 Batch 1450 Loss 2.2923 Accuracy 0.5051\n",
      "Epoch 30 Batch 1500 Loss 2.2925 Accuracy 0.5052\n",
      "Epoch 30 Batch 1550 Loss 2.2932 Accuracy 0.5051\n",
      "Epoch 30 Batch 1600 Loss 2.2941 Accuracy 0.5050\n",
      "Epoch 30 Batch 1650 Loss 2.2943 Accuracy 0.5050\n",
      "Epoch 30 Batch 1700 Loss 2.2946 Accuracy 0.5050\n",
      "Epoch 30 Batch 1750 Loss 2.2949 Accuracy 0.5049\n",
      "Epoch 30 Batch 1800 Loss 2.2949 Accuracy 0.5049\n",
      "Epoch 30 Batch 1850 Loss 2.2953 Accuracy 0.5048\n",
      "Epoch 30 Batch 1900 Loss 2.2960 Accuracy 0.5047\n",
      "Epoch 30 Batch 1950 Loss 2.2966 Accuracy 0.5047\n",
      "Epoch 30 Batch 2000 Loss 2.2964 Accuracy 0.5047\n",
      "Epoch 30 Batch 2050 Loss 2.2965 Accuracy 0.5047\n",
      "Epoch 30 Batch 2100 Loss 2.2963 Accuracy 0.5048\n",
      "Epoch 30 Batch 2150 Loss 2.2971 Accuracy 0.5047\n",
      "Epoch 30 Batch 2200 Loss 2.2974 Accuracy 0.5046\n",
      "Epoch 30 Batch 2250 Loss 2.2974 Accuracy 0.5046\n",
      "Epoch 30 Batch 2300 Loss 2.2976 Accuracy 0.5046\n",
      "Epoch 30 Batch 2350 Loss 2.2978 Accuracy 0.5046\n",
      "Epoch 30 Batch 2400 Loss 2.2982 Accuracy 0.5045\n",
      "Epoch 30 Batch 2450 Loss 2.2986 Accuracy 0.5044\n",
      "Epoch 30 Batch 2500 Loss 2.2989 Accuracy 0.5044\n",
      "Epoch 30 Batch 2550 Loss 2.2990 Accuracy 0.5045\n",
      "Epoch 30 Batch 2600 Loss 2.2994 Accuracy 0.5044\n",
      "Saving checkpoint for epoch 30 at ./checkpoints/train_full/ckpt-16\n",
      "Epoch 30 Loss 2.2996 Accuracy 0.5044\n",
      "Time taken for 1 epoch: 454.0968635082245 secs\n",
      "\n",
      "Epoch 31 Batch 0 Loss 2.1860 Accuracy 0.5291\n",
      "Epoch 31 Batch 50 Loss 2.2828 Accuracy 0.5035\n",
      "Epoch 31 Batch 100 Loss 2.2706 Accuracy 0.5073\n",
      "Epoch 31 Batch 150 Loss 2.2747 Accuracy 0.5073\n",
      "Epoch 31 Batch 200 Loss 2.2769 Accuracy 0.5070\n",
      "Epoch 31 Batch 250 Loss 2.2777 Accuracy 0.5069\n",
      "Epoch 31 Batch 300 Loss 2.2793 Accuracy 0.5070\n",
      "Epoch 31 Batch 350 Loss 2.2781 Accuracy 0.5069\n",
      "Epoch 31 Batch 400 Loss 2.2784 Accuracy 0.5069\n",
      "Epoch 31 Batch 450 Loss 2.2788 Accuracy 0.5069\n",
      "Epoch 31 Batch 500 Loss 2.2798 Accuracy 0.5068\n",
      "Epoch 31 Batch 550 Loss 2.2809 Accuracy 0.5066\n",
      "Epoch 31 Batch 600 Loss 2.2826 Accuracy 0.5064\n",
      "Epoch 31 Batch 650 Loss 2.2848 Accuracy 0.5062\n",
      "Epoch 31 Batch 700 Loss 2.2848 Accuracy 0.5061\n",
      "Epoch 31 Batch 750 Loss 2.2846 Accuracy 0.5062\n",
      "Epoch 31 Batch 800 Loss 2.2848 Accuracy 0.5063\n",
      "Epoch 31 Batch 850 Loss 2.2846 Accuracy 0.5063\n",
      "Epoch 31 Batch 900 Loss 2.2842 Accuracy 0.5065\n",
      "Epoch 31 Batch 950 Loss 2.2841 Accuracy 0.5065\n",
      "Epoch 31 Batch 1000 Loss 2.2834 Accuracy 0.5068\n",
      "Epoch 31 Batch 1050 Loss 2.2841 Accuracy 0.5065\n",
      "Epoch 31 Batch 1100 Loss 2.2846 Accuracy 0.5064\n",
      "Epoch 31 Batch 1150 Loss 2.2844 Accuracy 0.5066\n",
      "Epoch 31 Batch 1200 Loss 2.2839 Accuracy 0.5066\n",
      "Epoch 31 Batch 1250 Loss 2.2844 Accuracy 0.5066\n",
      "Epoch 31 Batch 1300 Loss 2.2850 Accuracy 0.5065\n",
      "Epoch 31 Batch 1350 Loss 2.2859 Accuracy 0.5063\n",
      "Epoch 31 Batch 1400 Loss 2.2861 Accuracy 0.5063\n",
      "Epoch 31 Batch 1450 Loss 2.2861 Accuracy 0.5064\n",
      "Epoch 31 Batch 1500 Loss 2.2858 Accuracy 0.5064\n",
      "Epoch 31 Batch 1550 Loss 2.2863 Accuracy 0.5064\n",
      "Epoch 31 Batch 1600 Loss 2.2865 Accuracy 0.5065\n",
      "Epoch 31 Batch 1650 Loss 2.2868 Accuracy 0.5065\n",
      "Epoch 31 Batch 1700 Loss 2.2871 Accuracy 0.5065\n",
      "Epoch 31 Batch 1750 Loss 2.2876 Accuracy 0.5063\n",
      "Epoch 31 Batch 1800 Loss 2.2878 Accuracy 0.5063\n",
      "Epoch 31 Batch 1850 Loss 2.2881 Accuracy 0.5063\n",
      "Epoch 31 Batch 1900 Loss 2.2881 Accuracy 0.5063\n",
      "Epoch 31 Batch 1950 Loss 2.2885 Accuracy 0.5062\n",
      "Epoch 31 Batch 2000 Loss 2.2891 Accuracy 0.5062\n",
      "Epoch 31 Batch 2050 Loss 2.2894 Accuracy 0.5061\n",
      "Epoch 31 Batch 2100 Loss 2.2899 Accuracy 0.5060\n",
      "Epoch 31 Batch 2150 Loss 2.2902 Accuracy 0.5060\n",
      "Epoch 31 Batch 2200 Loss 2.2901 Accuracy 0.5060\n",
      "Epoch 31 Batch 2250 Loss 2.2902 Accuracy 0.5060\n",
      "Epoch 31 Batch 2300 Loss 2.2907 Accuracy 0.5059\n",
      "Epoch 31 Batch 2350 Loss 2.2909 Accuracy 0.5058\n",
      "Epoch 31 Batch 2400 Loss 2.2911 Accuracy 0.5058\n",
      "Epoch 31 Batch 2450 Loss 2.2913 Accuracy 0.5058\n",
      "Epoch 31 Batch 2500 Loss 2.2919 Accuracy 0.5056\n",
      "Epoch 31 Batch 2550 Loss 2.2924 Accuracy 0.5056\n",
      "Epoch 31 Batch 2600 Loss 2.2928 Accuracy 0.5055\n",
      "Epoch 31 Loss 2.2931 Accuracy 0.5054\n",
      "Time taken for 1 epoch: 459.58431005477905 secs\n",
      "\n",
      "Epoch 32 Batch 0 Loss 2.4266 Accuracy 0.4805\n",
      "Epoch 32 Batch 50 Loss 2.2635 Accuracy 0.5102\n",
      "Epoch 32 Batch 100 Loss 2.2659 Accuracy 0.5088\n",
      "Epoch 32 Batch 150 Loss 2.2571 Accuracy 0.5106\n",
      "Epoch 32 Batch 200 Loss 2.2624 Accuracy 0.5098\n",
      "Epoch 32 Batch 250 Loss 2.2645 Accuracy 0.5089\n",
      "Epoch 32 Batch 300 Loss 2.2639 Accuracy 0.5090\n",
      "Epoch 32 Batch 350 Loss 2.2681 Accuracy 0.5082\n",
      "Epoch 32 Batch 400 Loss 2.2709 Accuracy 0.5080\n",
      "Epoch 32 Batch 450 Loss 2.2701 Accuracy 0.5081\n",
      "Epoch 32 Batch 500 Loss 2.2722 Accuracy 0.5079\n",
      "Epoch 32 Batch 550 Loss 2.2737 Accuracy 0.5077\n",
      "Epoch 32 Batch 600 Loss 2.2746 Accuracy 0.5075\n",
      "Epoch 32 Batch 650 Loss 2.2752 Accuracy 0.5075\n",
      "Epoch 32 Batch 700 Loss 2.2749 Accuracy 0.5075\n",
      "Epoch 32 Batch 750 Loss 2.2748 Accuracy 0.5077\n",
      "Epoch 32 Batch 800 Loss 2.2743 Accuracy 0.5078\n",
      "Epoch 32 Batch 850 Loss 2.2754 Accuracy 0.5077\n",
      "Epoch 32 Batch 900 Loss 2.2760 Accuracy 0.5075\n",
      "Epoch 32 Batch 950 Loss 2.2765 Accuracy 0.5076\n",
      "Epoch 32 Batch 1000 Loss 2.2762 Accuracy 0.5078\n",
      "Epoch 32 Batch 1050 Loss 2.2772 Accuracy 0.5077\n",
      "Epoch 32 Batch 1100 Loss 2.2771 Accuracy 0.5076\n",
      "Epoch 32 Batch 1150 Loss 2.2776 Accuracy 0.5075\n",
      "Epoch 32 Batch 1200 Loss 2.2783 Accuracy 0.5075\n",
      "Epoch 32 Batch 1250 Loss 2.2785 Accuracy 0.5075\n",
      "Epoch 32 Batch 1300 Loss 2.2788 Accuracy 0.5074\n",
      "Epoch 32 Batch 1350 Loss 2.2787 Accuracy 0.5075\n",
      "Epoch 32 Batch 1400 Loss 2.2785 Accuracy 0.5075\n",
      "Epoch 32 Batch 1450 Loss 2.2798 Accuracy 0.5074\n",
      "Epoch 32 Batch 1500 Loss 2.2807 Accuracy 0.5073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Batch 1550 Loss 2.2807 Accuracy 0.5073\n",
      "Epoch 32 Batch 1600 Loss 2.2812 Accuracy 0.5072\n",
      "Epoch 32 Batch 1650 Loss 2.2817 Accuracy 0.5072\n",
      "Epoch 32 Batch 1700 Loss 2.2823 Accuracy 0.5071\n",
      "Epoch 32 Batch 1750 Loss 2.2826 Accuracy 0.5071\n",
      "Epoch 32 Batch 1800 Loss 2.2833 Accuracy 0.5070\n",
      "Epoch 32 Batch 1850 Loss 2.2832 Accuracy 0.5071\n",
      "Epoch 32 Batch 1900 Loss 2.2839 Accuracy 0.5070\n",
      "Epoch 32 Batch 1950 Loss 2.2842 Accuracy 0.5069\n",
      "Epoch 32 Batch 2000 Loss 2.2840 Accuracy 0.5069\n",
      "Epoch 32 Batch 2050 Loss 2.2843 Accuracy 0.5069\n",
      "Epoch 32 Batch 2100 Loss 2.2846 Accuracy 0.5068\n",
      "Epoch 32 Batch 2150 Loss 2.2849 Accuracy 0.5067\n",
      "Epoch 32 Batch 2200 Loss 2.2853 Accuracy 0.5066\n",
      "Epoch 32 Batch 2250 Loss 2.2856 Accuracy 0.5065\n",
      "Epoch 32 Batch 2300 Loss 2.2860 Accuracy 0.5065\n",
      "Epoch 32 Batch 2350 Loss 2.2861 Accuracy 0.5065\n",
      "Epoch 32 Batch 2400 Loss 2.2861 Accuracy 0.5065\n",
      "Epoch 32 Batch 2450 Loss 2.2866 Accuracy 0.5065\n",
      "Epoch 32 Batch 2500 Loss 2.2867 Accuracy 0.5064\n",
      "Epoch 32 Batch 2550 Loss 2.2873 Accuracy 0.5064\n",
      "Epoch 32 Batch 2600 Loss 2.2873 Accuracy 0.5064\n",
      "Epoch 32 Loss 2.2873 Accuracy 0.5064\n",
      "Time taken for 1 epoch: 462.2957992553711 secs\n",
      "\n",
      "Epoch 33 Batch 0 Loss 2.2626 Accuracy 0.4972\n",
      "Epoch 33 Batch 50 Loss 2.2412 Accuracy 0.5112\n",
      "Epoch 33 Batch 100 Loss 2.2579 Accuracy 0.5090\n",
      "Epoch 33 Batch 150 Loss 2.2625 Accuracy 0.5084\n",
      "Epoch 33 Batch 200 Loss 2.2608 Accuracy 0.5091\n",
      "Epoch 33 Batch 250 Loss 2.2623 Accuracy 0.5087\n",
      "Epoch 33 Batch 300 Loss 2.2610 Accuracy 0.5090\n",
      "Epoch 33 Batch 350 Loss 2.2628 Accuracy 0.5089\n",
      "Epoch 33 Batch 400 Loss 2.2644 Accuracy 0.5090\n",
      "Epoch 33 Batch 450 Loss 2.2657 Accuracy 0.5088\n",
      "Epoch 33 Batch 500 Loss 2.2656 Accuracy 0.5091\n",
      "Epoch 33 Batch 550 Loss 2.2645 Accuracy 0.5094\n",
      "Epoch 33 Batch 600 Loss 2.2664 Accuracy 0.5089\n",
      "Epoch 33 Batch 650 Loss 2.2676 Accuracy 0.5086\n",
      "Epoch 33 Batch 700 Loss 2.2684 Accuracy 0.5086\n",
      "Epoch 33 Batch 750 Loss 2.2700 Accuracy 0.5083\n",
      "Epoch 33 Batch 800 Loss 2.2695 Accuracy 0.5085\n",
      "Epoch 33 Batch 850 Loss 2.2709 Accuracy 0.5083\n",
      "Epoch 33 Batch 900 Loss 2.2703 Accuracy 0.5084\n",
      "Epoch 33 Batch 950 Loss 2.2697 Accuracy 0.5085\n",
      "Epoch 33 Batch 1000 Loss 2.2708 Accuracy 0.5084\n",
      "Epoch 33 Batch 1050 Loss 2.2722 Accuracy 0.5083\n",
      "Epoch 33 Batch 1100 Loss 2.2726 Accuracy 0.5081\n",
      "Epoch 33 Batch 1150 Loss 2.2725 Accuracy 0.5081\n",
      "Epoch 33 Batch 1200 Loss 2.2736 Accuracy 0.5079\n",
      "Epoch 33 Batch 1250 Loss 2.2740 Accuracy 0.5078\n",
      "Epoch 33 Batch 1300 Loss 2.2745 Accuracy 0.5079\n",
      "Epoch 33 Batch 1350 Loss 2.2751 Accuracy 0.5078\n",
      "Epoch 33 Batch 1400 Loss 2.2751 Accuracy 0.5079\n",
      "Epoch 33 Batch 1450 Loss 2.2756 Accuracy 0.5078\n",
      "Epoch 33 Batch 1500 Loss 2.2762 Accuracy 0.5077\n",
      "Epoch 33 Batch 1550 Loss 2.2760 Accuracy 0.5077\n",
      "Epoch 33 Batch 1600 Loss 2.2761 Accuracy 0.5078\n",
      "Epoch 33 Batch 1650 Loss 2.2767 Accuracy 0.5076\n",
      "Epoch 33 Batch 1700 Loss 2.2767 Accuracy 0.5076\n",
      "Epoch 33 Batch 1750 Loss 2.2769 Accuracy 0.5075\n",
      "Epoch 33 Batch 1800 Loss 2.2776 Accuracy 0.5075\n",
      "Epoch 33 Batch 1850 Loss 2.2778 Accuracy 0.5074\n",
      "Epoch 33 Batch 1900 Loss 2.2784 Accuracy 0.5073\n",
      "Epoch 33 Batch 1950 Loss 2.2781 Accuracy 0.5074\n",
      "Epoch 33 Batch 2000 Loss 2.2780 Accuracy 0.5074\n",
      "Epoch 33 Batch 2050 Loss 2.2785 Accuracy 0.5074\n",
      "Epoch 33 Batch 2100 Loss 2.2790 Accuracy 0.5073\n",
      "Epoch 33 Batch 2150 Loss 2.2790 Accuracy 0.5073\n",
      "Epoch 33 Batch 2200 Loss 2.2790 Accuracy 0.5073\n",
      "Epoch 33 Batch 2250 Loss 2.2794 Accuracy 0.5072\n",
      "Epoch 33 Batch 2300 Loss 2.2798 Accuracy 0.5071\n",
      "Epoch 33 Batch 2350 Loss 2.2797 Accuracy 0.5071\n",
      "Epoch 33 Batch 2400 Loss 2.2800 Accuracy 0.5071\n",
      "Epoch 33 Batch 2450 Loss 2.2801 Accuracy 0.5071\n",
      "Epoch 33 Batch 2500 Loss 2.2805 Accuracy 0.5070\n",
      "Epoch 33 Batch 2550 Loss 2.2810 Accuracy 0.5070\n",
      "Epoch 33 Batch 2600 Loss 2.2812 Accuracy 0.5070\n",
      "Epoch 33 Loss 2.2815 Accuracy 0.5070\n",
      "Time taken for 1 epoch: 462.01769399642944 secs\n",
      "\n",
      "Epoch 34 Batch 0 Loss 2.4431 Accuracy 0.4813\n",
      "Epoch 34 Batch 50 Loss 2.2530 Accuracy 0.5103\n",
      "Epoch 34 Batch 100 Loss 2.2582 Accuracy 0.5098\n",
      "Epoch 34 Batch 150 Loss 2.2596 Accuracy 0.5094\n",
      "Epoch 34 Batch 200 Loss 2.2626 Accuracy 0.5092\n",
      "Epoch 34 Batch 250 Loss 2.2606 Accuracy 0.5096\n",
      "Epoch 34 Batch 300 Loss 2.2570 Accuracy 0.5102\n",
      "Epoch 34 Batch 350 Loss 2.2580 Accuracy 0.5098\n",
      "Epoch 34 Batch 400 Loss 2.2599 Accuracy 0.5093\n",
      "Epoch 34 Batch 450 Loss 2.2618 Accuracy 0.5090\n",
      "Epoch 34 Batch 500 Loss 2.2621 Accuracy 0.5089\n",
      "Epoch 34 Batch 550 Loss 2.2624 Accuracy 0.5089\n",
      "Epoch 34 Batch 600 Loss 2.2641 Accuracy 0.5087\n",
      "Epoch 34 Batch 650 Loss 2.2649 Accuracy 0.5085\n",
      "Epoch 34 Batch 700 Loss 2.2671 Accuracy 0.5083\n",
      "Epoch 34 Batch 750 Loss 2.2674 Accuracy 0.5083\n",
      "Epoch 34 Batch 800 Loss 2.2666 Accuracy 0.5085\n",
      "Epoch 34 Batch 850 Loss 2.2662 Accuracy 0.5085\n",
      "Epoch 34 Batch 900 Loss 2.2672 Accuracy 0.5085\n",
      "Epoch 34 Batch 950 Loss 2.2679 Accuracy 0.5085\n",
      "Epoch 34 Batch 1000 Loss 2.2689 Accuracy 0.5082\n",
      "Epoch 34 Batch 1050 Loss 2.2687 Accuracy 0.5084\n",
      "Epoch 34 Batch 1100 Loss 2.2688 Accuracy 0.5084\n",
      "Epoch 34 Batch 1150 Loss 2.2687 Accuracy 0.5084\n",
      "Epoch 34 Batch 1200 Loss 2.2689 Accuracy 0.5085\n",
      "Epoch 34 Batch 1250 Loss 2.2684 Accuracy 0.5086\n",
      "Epoch 34 Batch 1300 Loss 2.2688 Accuracy 0.5085\n",
      "Epoch 34 Batch 1350 Loss 2.2688 Accuracy 0.5086\n",
      "Epoch 34 Batch 1400 Loss 2.2688 Accuracy 0.5087\n",
      "Epoch 34 Batch 1450 Loss 2.2698 Accuracy 0.5086\n",
      "Epoch 34 Batch 1500 Loss 2.2704 Accuracy 0.5086\n",
      "Epoch 34 Batch 1550 Loss 2.2703 Accuracy 0.5086\n",
      "Epoch 34 Batch 1600 Loss 2.2708 Accuracy 0.5085\n",
      "Epoch 34 Batch 1650 Loss 2.2708 Accuracy 0.5085\n",
      "Epoch 34 Batch 1700 Loss 2.2715 Accuracy 0.5084\n",
      "Epoch 34 Batch 1750 Loss 2.2720 Accuracy 0.5083\n",
      "Epoch 34 Batch 1800 Loss 2.2722 Accuracy 0.5083\n",
      "Epoch 34 Batch 1850 Loss 2.2725 Accuracy 0.5083\n",
      "Epoch 34 Batch 1900 Loss 2.2729 Accuracy 0.5083\n",
      "Epoch 34 Batch 1950 Loss 2.2731 Accuracy 0.5082\n",
      "Epoch 34 Batch 2000 Loss 2.2730 Accuracy 0.5083\n",
      "Epoch 34 Batch 2050 Loss 2.2732 Accuracy 0.5082\n",
      "Epoch 34 Batch 2100 Loss 2.2731 Accuracy 0.5082\n",
      "Epoch 34 Batch 2150 Loss 2.2739 Accuracy 0.5081\n",
      "Epoch 34 Batch 2200 Loss 2.2742 Accuracy 0.5081\n",
      "Epoch 34 Batch 2250 Loss 2.2743 Accuracy 0.5081\n",
      "Epoch 34 Batch 2300 Loss 2.2745 Accuracy 0.5080\n",
      "Epoch 34 Batch 2350 Loss 2.2744 Accuracy 0.5080\n",
      "Epoch 34 Batch 2400 Loss 2.2746 Accuracy 0.5080\n",
      "Epoch 34 Batch 2450 Loss 2.2749 Accuracy 0.5079\n",
      "Epoch 34 Batch 2500 Loss 2.2751 Accuracy 0.5079\n",
      "Epoch 34 Batch 2550 Loss 2.2755 Accuracy 0.5079\n",
      "Epoch 34 Batch 2600 Loss 2.2756 Accuracy 0.5079\n",
      "Epoch 34 Loss 2.2760 Accuracy 0.5078\n",
      "Time taken for 1 epoch: 456.2549822330475 secs\n",
      "\n",
      "Epoch 35 Batch 0 Loss 2.2272 Accuracy 0.5158\n",
      "Epoch 35 Batch 50 Loss 2.2648 Accuracy 0.5074\n",
      "Epoch 35 Batch 100 Loss 2.2604 Accuracy 0.5085\n",
      "Epoch 35 Batch 150 Loss 2.2527 Accuracy 0.5102\n",
      "Epoch 35 Batch 200 Loss 2.2523 Accuracy 0.5105\n",
      "Epoch 35 Batch 250 Loss 2.2511 Accuracy 0.5107\n",
      "Epoch 35 Batch 300 Loss 2.2514 Accuracy 0.5109\n",
      "Epoch 35 Batch 350 Loss 2.2524 Accuracy 0.5105\n",
      "Epoch 35 Batch 400 Loss 2.2564 Accuracy 0.5098\n",
      "Epoch 35 Batch 450 Loss 2.2583 Accuracy 0.5095\n",
      "Epoch 35 Batch 500 Loss 2.2582 Accuracy 0.5096\n",
      "Epoch 35 Batch 550 Loss 2.2573 Accuracy 0.5097\n",
      "Epoch 35 Batch 600 Loss 2.2574 Accuracy 0.5096\n",
      "Epoch 35 Batch 650 Loss 2.2580 Accuracy 0.5095\n",
      "Epoch 35 Batch 700 Loss 2.2602 Accuracy 0.5095\n",
      "Epoch 35 Batch 750 Loss 2.2606 Accuracy 0.5093\n",
      "Epoch 35 Batch 800 Loss 2.2622 Accuracy 0.5092\n",
      "Epoch 35 Batch 850 Loss 2.2619 Accuracy 0.5093\n",
      "Epoch 35 Batch 900 Loss 2.2624 Accuracy 0.5093\n",
      "Epoch 35 Batch 950 Loss 2.2629 Accuracy 0.5092\n",
      "Epoch 35 Batch 1000 Loss 2.2626 Accuracy 0.5094\n",
      "Epoch 35 Batch 1050 Loss 2.2629 Accuracy 0.5093\n",
      "Epoch 35 Batch 1100 Loss 2.2637 Accuracy 0.5091\n",
      "Epoch 35 Batch 1150 Loss 2.2644 Accuracy 0.5090\n",
      "Epoch 35 Batch 1200 Loss 2.2637 Accuracy 0.5092\n",
      "Epoch 35 Batch 1250 Loss 2.2636 Accuracy 0.5091\n",
      "Epoch 35 Batch 1300 Loss 2.2640 Accuracy 0.5092\n",
      "Epoch 35 Batch 1350 Loss 2.2638 Accuracy 0.5092\n",
      "Epoch 35 Batch 1400 Loss 2.2639 Accuracy 0.5091\n",
      "Epoch 35 Batch 1450 Loss 2.2640 Accuracy 0.5092\n",
      "Epoch 35 Batch 1500 Loss 2.2643 Accuracy 0.5092\n",
      "Epoch 35 Batch 1550 Loss 2.2643 Accuracy 0.5092\n",
      "Epoch 35 Batch 1600 Loss 2.2646 Accuracy 0.5092\n",
      "Epoch 35 Batch 1650 Loss 2.2647 Accuracy 0.5093\n",
      "Epoch 35 Batch 1700 Loss 2.2658 Accuracy 0.5091\n",
      "Epoch 35 Batch 1750 Loss 2.2659 Accuracy 0.5091\n",
      "Epoch 35 Batch 1800 Loss 2.2659 Accuracy 0.5091\n",
      "Epoch 35 Batch 1850 Loss 2.2665 Accuracy 0.5091\n",
      "Epoch 35 Batch 1900 Loss 2.2671 Accuracy 0.5090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Batch 1950 Loss 2.2672 Accuracy 0.5090\n",
      "Epoch 35 Batch 2000 Loss 2.2672 Accuracy 0.5090\n",
      "Epoch 35 Batch 2050 Loss 2.2670 Accuracy 0.5090\n",
      "Epoch 35 Batch 2100 Loss 2.2677 Accuracy 0.5089\n",
      "Epoch 35 Batch 2150 Loss 2.2680 Accuracy 0.5089\n",
      "Epoch 35 Batch 2200 Loss 2.2687 Accuracy 0.5087\n",
      "Epoch 35 Batch 2250 Loss 2.2689 Accuracy 0.5087\n",
      "Epoch 35 Batch 2300 Loss 2.2688 Accuracy 0.5087\n",
      "Epoch 35 Batch 2350 Loss 2.2688 Accuracy 0.5087\n",
      "Epoch 35 Batch 2400 Loss 2.2691 Accuracy 0.5087\n",
      "Epoch 35 Batch 2450 Loss 2.2692 Accuracy 0.5087\n",
      "Epoch 35 Batch 2500 Loss 2.2696 Accuracy 0.5087\n",
      "Epoch 35 Batch 2550 Loss 2.2702 Accuracy 0.5086\n",
      "Epoch 35 Batch 2600 Loss 2.2707 Accuracy 0.5085\n",
      "Saving checkpoint for epoch 35 at ./checkpoints/train_full/ckpt-17\n",
      "Epoch 35 Loss 2.2710 Accuracy 0.5085\n",
      "Time taken for 1 epoch: 455.84616136550903 secs\n",
      "\n",
      "Epoch 36 Batch 0 Loss 2.0926 Accuracy 0.5395\n",
      "Epoch 36 Batch 50 Loss 2.2341 Accuracy 0.5113\n",
      "Epoch 36 Batch 100 Loss 2.2360 Accuracy 0.5110\n",
      "Epoch 36 Batch 150 Loss 2.2367 Accuracy 0.5120\n",
      "Epoch 36 Batch 200 Loss 2.2440 Accuracy 0.5108\n",
      "Epoch 36 Batch 250 Loss 2.2464 Accuracy 0.5108\n",
      "Epoch 36 Batch 300 Loss 2.2473 Accuracy 0.5107\n",
      "Epoch 36 Batch 350 Loss 2.2504 Accuracy 0.5104\n",
      "Epoch 36 Batch 400 Loss 2.2538 Accuracy 0.5099\n",
      "Epoch 36 Batch 450 Loss 2.2542 Accuracy 0.5099\n",
      "Epoch 36 Batch 500 Loss 2.2551 Accuracy 0.5099\n",
      "Epoch 36 Batch 550 Loss 2.2541 Accuracy 0.5103\n",
      "Epoch 36 Batch 600 Loss 2.2543 Accuracy 0.5103\n",
      "Epoch 36 Batch 650 Loss 2.2535 Accuracy 0.5104\n",
      "Epoch 36 Batch 700 Loss 2.2550 Accuracy 0.5102\n",
      "Epoch 36 Batch 750 Loss 2.2559 Accuracy 0.5101\n",
      "Epoch 36 Batch 800 Loss 2.2581 Accuracy 0.5098\n",
      "Epoch 36 Batch 850 Loss 2.2586 Accuracy 0.5097\n",
      "Epoch 36 Batch 900 Loss 2.2588 Accuracy 0.5097\n",
      "Epoch 36 Batch 950 Loss 2.2591 Accuracy 0.5097\n",
      "Epoch 36 Batch 1000 Loss 2.2592 Accuracy 0.5096\n",
      "Epoch 36 Batch 1050 Loss 2.2598 Accuracy 0.5096\n",
      "Epoch 36 Batch 1100 Loss 2.2588 Accuracy 0.5098\n",
      "Epoch 36 Batch 1150 Loss 2.2598 Accuracy 0.5097\n",
      "Epoch 36 Batch 1200 Loss 2.2592 Accuracy 0.5098\n",
      "Epoch 36 Batch 1250 Loss 2.2589 Accuracy 0.5099\n",
      "Epoch 36 Batch 1300 Loss 2.2598 Accuracy 0.5098\n",
      "Epoch 36 Batch 1350 Loss 2.2596 Accuracy 0.5099\n",
      "Epoch 36 Batch 1400 Loss 2.2600 Accuracy 0.5098\n",
      "Epoch 36 Batch 1450 Loss 2.2602 Accuracy 0.5098\n",
      "Epoch 36 Batch 1500 Loss 2.2603 Accuracy 0.5098\n",
      "Epoch 36 Batch 1550 Loss 2.2607 Accuracy 0.5097\n",
      "Epoch 36 Batch 1600 Loss 2.2605 Accuracy 0.5098\n",
      "Epoch 36 Batch 1650 Loss 2.2607 Accuracy 0.5098\n",
      "Epoch 36 Batch 1700 Loss 2.2611 Accuracy 0.5097\n",
      "Epoch 36 Batch 1750 Loss 2.2614 Accuracy 0.5096\n",
      "Epoch 36 Batch 1800 Loss 2.2614 Accuracy 0.5097\n",
      "Epoch 36 Batch 1850 Loss 2.2616 Accuracy 0.5096\n",
      "Epoch 36 Batch 1900 Loss 2.2620 Accuracy 0.5096\n",
      "Epoch 36 Batch 1950 Loss 2.2623 Accuracy 0.5096\n",
      "Epoch 36 Batch 2000 Loss 2.2625 Accuracy 0.5095\n",
      "Epoch 36 Batch 2050 Loss 2.2627 Accuracy 0.5095\n",
      "Epoch 36 Batch 2100 Loss 2.2629 Accuracy 0.5095\n",
      "Epoch 36 Batch 2150 Loss 2.2632 Accuracy 0.5095\n",
      "Epoch 36 Batch 2200 Loss 2.2634 Accuracy 0.5094\n",
      "Epoch 36 Batch 2250 Loss 2.2637 Accuracy 0.5094\n",
      "Epoch 36 Batch 2300 Loss 2.2639 Accuracy 0.5093\n",
      "Epoch 36 Batch 2350 Loss 2.2641 Accuracy 0.5093\n",
      "Epoch 36 Batch 2400 Loss 2.2642 Accuracy 0.5093\n",
      "Epoch 36 Batch 2450 Loss 2.2642 Accuracy 0.5094\n",
      "Epoch 36 Batch 2500 Loss 2.2644 Accuracy 0.5094\n",
      "Epoch 36 Batch 2550 Loss 2.2647 Accuracy 0.5094\n",
      "Epoch 36 Batch 2600 Loss 2.2651 Accuracy 0.5093\n",
      "Epoch 36 Loss 2.2657 Accuracy 0.5092\n",
      "Time taken for 1 epoch: 454.82975602149963 secs\n",
      "\n",
      "Epoch 37 Batch 0 Loss 2.2748 Accuracy 0.5243\n",
      "Epoch 37 Batch 50 Loss 2.2400 Accuracy 0.5143\n",
      "Epoch 37 Batch 100 Loss 2.2370 Accuracy 0.5151\n",
      "Epoch 37 Batch 150 Loss 2.2422 Accuracy 0.5134\n",
      "Epoch 37 Batch 200 Loss 2.2431 Accuracy 0.5127\n",
      "Epoch 37 Batch 250 Loss 2.2439 Accuracy 0.5125\n",
      "Epoch 37 Batch 300 Loss 2.2457 Accuracy 0.5118\n",
      "Epoch 37 Batch 350 Loss 2.2443 Accuracy 0.5120\n",
      "Epoch 37 Batch 400 Loss 2.2463 Accuracy 0.5116\n",
      "Epoch 37 Batch 450 Loss 2.2459 Accuracy 0.5117\n",
      "Epoch 37 Batch 500 Loss 2.2459 Accuracy 0.5120\n",
      "Epoch 37 Batch 550 Loss 2.2492 Accuracy 0.5114\n",
      "Epoch 37 Batch 600 Loss 2.2493 Accuracy 0.5115\n",
      "Epoch 37 Batch 650 Loss 2.2497 Accuracy 0.5113\n",
      "Epoch 37 Batch 700 Loss 2.2505 Accuracy 0.5112\n",
      "Epoch 37 Batch 750 Loss 2.2509 Accuracy 0.5112\n",
      "Epoch 37 Batch 800 Loss 2.2506 Accuracy 0.5112\n",
      "Epoch 37 Batch 850 Loss 2.2508 Accuracy 0.5112\n",
      "Epoch 37 Batch 900 Loss 2.2511 Accuracy 0.5111\n",
      "Epoch 37 Batch 950 Loss 2.2523 Accuracy 0.5109\n",
      "Epoch 37 Batch 1000 Loss 2.2529 Accuracy 0.5108\n",
      "Epoch 37 Batch 1050 Loss 2.2533 Accuracy 0.5108\n",
      "Epoch 37 Batch 1100 Loss 2.2540 Accuracy 0.5106\n",
      "Epoch 37 Batch 1150 Loss 2.2539 Accuracy 0.5106\n",
      "Epoch 37 Batch 1200 Loss 2.2535 Accuracy 0.5108\n",
      "Epoch 37 Batch 1250 Loss 2.2538 Accuracy 0.5108\n",
      "Epoch 37 Batch 1300 Loss 2.2549 Accuracy 0.5106\n",
      "Epoch 37 Batch 1350 Loss 2.2543 Accuracy 0.5108\n",
      "Epoch 37 Batch 1400 Loss 2.2547 Accuracy 0.5107\n",
      "Epoch 37 Batch 1450 Loss 2.2553 Accuracy 0.5107\n",
      "Epoch 37 Batch 1500 Loss 2.2555 Accuracy 0.5107\n",
      "Epoch 37 Batch 1550 Loss 2.2559 Accuracy 0.5106\n",
      "Epoch 37 Batch 1600 Loss 2.2559 Accuracy 0.5106\n",
      "Epoch 37 Batch 1650 Loss 2.2562 Accuracy 0.5106\n",
      "Epoch 37 Batch 1700 Loss 2.2559 Accuracy 0.5107\n",
      "Epoch 37 Batch 1750 Loss 2.2564 Accuracy 0.5106\n",
      "Epoch 37 Batch 1800 Loss 2.2565 Accuracy 0.5106\n",
      "Epoch 37 Batch 1850 Loss 2.2565 Accuracy 0.5105\n",
      "Epoch 37 Batch 1900 Loss 2.2561 Accuracy 0.5106\n",
      "Epoch 37 Batch 1950 Loss 2.2565 Accuracy 0.5106\n",
      "Epoch 37 Batch 2000 Loss 2.2569 Accuracy 0.5104\n",
      "Epoch 37 Batch 2050 Loss 2.2570 Accuracy 0.5104\n",
      "Epoch 37 Batch 2100 Loss 2.2570 Accuracy 0.5104\n",
      "Epoch 37 Batch 2150 Loss 2.2574 Accuracy 0.5104\n",
      "Epoch 37 Batch 2200 Loss 2.2576 Accuracy 0.5103\n",
      "Epoch 37 Batch 2250 Loss 2.2578 Accuracy 0.5103\n",
      "Epoch 37 Batch 2300 Loss 2.2580 Accuracy 0.5103\n",
      "Epoch 37 Batch 2350 Loss 2.2581 Accuracy 0.5103\n",
      "Epoch 37 Batch 2400 Loss 2.2582 Accuracy 0.5103\n",
      "Epoch 37 Batch 2450 Loss 2.2584 Accuracy 0.5103\n",
      "Epoch 37 Batch 2500 Loss 2.2592 Accuracy 0.5102\n",
      "Epoch 37 Batch 2550 Loss 2.2596 Accuracy 0.5101\n",
      "Epoch 37 Batch 2600 Loss 2.2600 Accuracy 0.5101\n",
      "Epoch 37 Loss 2.2602 Accuracy 0.5101\n",
      "Time taken for 1 epoch: 455.02581429481506 secs\n",
      "\n",
      "Epoch 38 Batch 0 Loss 2.2338 Accuracy 0.5176\n",
      "Epoch 38 Batch 50 Loss 2.2661 Accuracy 0.5087\n",
      "Epoch 38 Batch 100 Loss 2.2513 Accuracy 0.5103\n",
      "Epoch 38 Batch 150 Loss 2.2437 Accuracy 0.5111\n",
      "Epoch 38 Batch 200 Loss 2.2399 Accuracy 0.5120\n",
      "Epoch 38 Batch 250 Loss 2.2368 Accuracy 0.5131\n",
      "Epoch 38 Batch 300 Loss 2.2360 Accuracy 0.5132\n",
      "Epoch 38 Batch 350 Loss 2.2373 Accuracy 0.5132\n",
      "Epoch 38 Batch 400 Loss 2.2402 Accuracy 0.5126\n",
      "Epoch 38 Batch 450 Loss 2.2409 Accuracy 0.5125\n",
      "Epoch 38 Batch 500 Loss 2.2410 Accuracy 0.5125\n",
      "Epoch 38 Batch 550 Loss 2.2410 Accuracy 0.5126\n",
      "Epoch 38 Batch 600 Loss 2.2422 Accuracy 0.5125\n",
      "Epoch 38 Batch 650 Loss 2.2424 Accuracy 0.5124\n",
      "Epoch 38 Batch 700 Loss 2.2434 Accuracy 0.5124\n",
      "Epoch 38 Batch 750 Loss 2.2447 Accuracy 0.5121\n",
      "Epoch 38 Batch 800 Loss 2.2458 Accuracy 0.5119\n",
      "Epoch 38 Batch 850 Loss 2.2457 Accuracy 0.5120\n",
      "Epoch 38 Batch 900 Loss 2.2456 Accuracy 0.5121\n",
      "Epoch 38 Batch 950 Loss 2.2456 Accuracy 0.5120\n",
      "Epoch 38 Batch 1000 Loss 2.2454 Accuracy 0.5121\n",
      "Epoch 38 Batch 1050 Loss 2.2454 Accuracy 0.5122\n",
      "Epoch 38 Batch 1100 Loss 2.2460 Accuracy 0.5121\n",
      "Epoch 38 Batch 1150 Loss 2.2463 Accuracy 0.5120\n",
      "Epoch 38 Batch 1200 Loss 2.2464 Accuracy 0.5120\n",
      "Epoch 38 Batch 1250 Loss 2.2470 Accuracy 0.5119\n",
      "Epoch 38 Batch 1300 Loss 2.2471 Accuracy 0.5120\n",
      "Epoch 38 Batch 1350 Loss 2.2475 Accuracy 0.5119\n",
      "Epoch 38 Batch 1400 Loss 2.2477 Accuracy 0.5118\n",
      "Epoch 38 Batch 1450 Loss 2.2484 Accuracy 0.5117\n",
      "Epoch 38 Batch 1500 Loss 2.2484 Accuracy 0.5117\n",
      "Epoch 38 Batch 1550 Loss 2.2492 Accuracy 0.5117\n",
      "Epoch 38 Batch 1600 Loss 2.2498 Accuracy 0.5115\n",
      "Epoch 38 Batch 1650 Loss 2.2496 Accuracy 0.5115\n",
      "Epoch 38 Batch 1700 Loss 2.2496 Accuracy 0.5116\n",
      "Epoch 38 Batch 1750 Loss 2.2502 Accuracy 0.5115\n",
      "Epoch 38 Batch 1800 Loss 2.2506 Accuracy 0.5114\n",
      "Epoch 38 Batch 1850 Loss 2.2508 Accuracy 0.5114\n",
      "Epoch 38 Batch 1900 Loss 2.2516 Accuracy 0.5112\n",
      "Epoch 38 Batch 1950 Loss 2.2518 Accuracy 0.5112\n",
      "Epoch 38 Batch 2000 Loss 2.2520 Accuracy 0.5111\n",
      "Epoch 38 Batch 2050 Loss 2.2525 Accuracy 0.5111\n",
      "Epoch 38 Batch 2100 Loss 2.2521 Accuracy 0.5112\n",
      "Epoch 38 Batch 2150 Loss 2.2521 Accuracy 0.5112\n",
      "Epoch 38 Batch 2200 Loss 2.2523 Accuracy 0.5111\n",
      "Epoch 38 Batch 2250 Loss 2.2526 Accuracy 0.5111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Batch 2300 Loss 2.2531 Accuracy 0.5111\n",
      "Epoch 38 Batch 2350 Loss 2.2530 Accuracy 0.5111\n",
      "Epoch 38 Batch 2400 Loss 2.2536 Accuracy 0.5110\n",
      "Epoch 38 Batch 2450 Loss 2.2541 Accuracy 0.5110\n",
      "Epoch 38 Batch 2500 Loss 2.2545 Accuracy 0.5109\n",
      "Epoch 38 Batch 2550 Loss 2.2547 Accuracy 0.5109\n",
      "Epoch 38 Batch 2600 Loss 2.2551 Accuracy 0.5109\n",
      "Epoch 38 Loss 2.2553 Accuracy 0.5108\n",
      "Time taken for 1 epoch: 455.543404340744 secs\n",
      "\n",
      "Epoch 39 Batch 0 Loss 2.3539 Accuracy 0.5013\n",
      "Epoch 39 Batch 50 Loss 2.2489 Accuracy 0.5133\n",
      "Epoch 39 Batch 100 Loss 2.2379 Accuracy 0.5136\n",
      "Epoch 39 Batch 150 Loss 2.2338 Accuracy 0.5135\n",
      "Epoch 39 Batch 200 Loss 2.2309 Accuracy 0.5139\n",
      "Epoch 39 Batch 250 Loss 2.2304 Accuracy 0.5138\n",
      "Epoch 39 Batch 300 Loss 2.2302 Accuracy 0.5142\n",
      "Epoch 39 Batch 350 Loss 2.2324 Accuracy 0.5138\n",
      "Epoch 39 Batch 400 Loss 2.2349 Accuracy 0.5136\n",
      "Epoch 39 Batch 450 Loss 2.2360 Accuracy 0.5135\n",
      "Epoch 39 Batch 500 Loss 2.2381 Accuracy 0.5130\n",
      "Epoch 39 Batch 550 Loss 2.2386 Accuracy 0.5131\n",
      "Epoch 39 Batch 600 Loss 2.2380 Accuracy 0.5134\n",
      "Epoch 39 Batch 650 Loss 2.2392 Accuracy 0.5132\n",
      "Epoch 39 Batch 700 Loss 2.2397 Accuracy 0.5133\n",
      "Epoch 39 Batch 750 Loss 2.2404 Accuracy 0.5131\n",
      "Epoch 39 Batch 800 Loss 2.2408 Accuracy 0.5129\n",
      "Epoch 39 Batch 850 Loss 2.2420 Accuracy 0.5126\n",
      "Epoch 39 Batch 900 Loss 2.2422 Accuracy 0.5128\n",
      "Epoch 39 Batch 950 Loss 2.2419 Accuracy 0.5128\n",
      "Epoch 39 Batch 1000 Loss 2.2430 Accuracy 0.5127\n",
      "Epoch 39 Batch 1050 Loss 2.2433 Accuracy 0.5127\n",
      "Epoch 39 Batch 1100 Loss 2.2438 Accuracy 0.5126\n",
      "Epoch 39 Batch 1150 Loss 2.2433 Accuracy 0.5126\n",
      "Epoch 39 Batch 1200 Loss 2.2433 Accuracy 0.5127\n",
      "Epoch 39 Batch 1250 Loss 2.2433 Accuracy 0.5127\n",
      "Epoch 39 Batch 1300 Loss 2.2432 Accuracy 0.5128\n",
      "Epoch 39 Batch 1350 Loss 2.2433 Accuracy 0.5128\n",
      "Epoch 39 Batch 1400 Loss 2.2439 Accuracy 0.5127\n",
      "Epoch 39 Batch 1450 Loss 2.2433 Accuracy 0.5128\n",
      "Epoch 39 Batch 1500 Loss 2.2444 Accuracy 0.5126\n",
      "Epoch 39 Batch 1550 Loss 2.2443 Accuracy 0.5126\n",
      "Epoch 39 Batch 1600 Loss 2.2440 Accuracy 0.5126\n",
      "Epoch 39 Batch 1650 Loss 2.2437 Accuracy 0.5127\n",
      "Epoch 39 Batch 1700 Loss 2.2441 Accuracy 0.5126\n",
      "Epoch 39 Batch 1750 Loss 2.2444 Accuracy 0.5125\n",
      "Epoch 39 Batch 1800 Loss 2.2450 Accuracy 0.5125\n",
      "Epoch 39 Batch 1850 Loss 2.2452 Accuracy 0.5125\n",
      "Epoch 39 Batch 1900 Loss 2.2456 Accuracy 0.5123\n",
      "Epoch 39 Batch 1950 Loss 2.2464 Accuracy 0.5122\n",
      "Epoch 39 Batch 2000 Loss 2.2467 Accuracy 0.5122\n",
      "Epoch 39 Batch 2050 Loss 2.2471 Accuracy 0.5122\n",
      "Epoch 39 Batch 2100 Loss 2.2471 Accuracy 0.5122\n",
      "Epoch 39 Batch 2150 Loss 2.2471 Accuracy 0.5122\n",
      "Epoch 39 Batch 2200 Loss 2.2472 Accuracy 0.5122\n",
      "Epoch 39 Batch 2250 Loss 2.2477 Accuracy 0.5121\n",
      "Epoch 39 Batch 2300 Loss 2.2476 Accuracy 0.5122\n",
      "Epoch 39 Batch 2350 Loss 2.2479 Accuracy 0.5121\n",
      "Epoch 39 Batch 2400 Loss 2.2485 Accuracy 0.5120\n",
      "Epoch 39 Batch 2450 Loss 2.2487 Accuracy 0.5121\n",
      "Epoch 39 Batch 2500 Loss 2.2491 Accuracy 0.5120\n",
      "Epoch 39 Batch 2550 Loss 2.2496 Accuracy 0.5118\n",
      "Epoch 39 Batch 2600 Loss 2.2500 Accuracy 0.5118\n",
      "Epoch 39 Loss 2.2501 Accuracy 0.5118\n",
      "Time taken for 1 epoch: 453.39966773986816 secs\n",
      "\n",
      "Epoch 40 Batch 0 Loss 2.2850 Accuracy 0.5137\n",
      "Epoch 40 Batch 50 Loss 2.2282 Accuracy 0.5154\n",
      "Epoch 40 Batch 100 Loss 2.2269 Accuracy 0.5147\n",
      "Epoch 40 Batch 150 Loss 2.2295 Accuracy 0.5144\n",
      "Epoch 40 Batch 200 Loss 2.2280 Accuracy 0.5144\n",
      "Epoch 40 Batch 250 Loss 2.2270 Accuracy 0.5150\n",
      "Epoch 40 Batch 300 Loss 2.2286 Accuracy 0.5144\n",
      "Epoch 40 Batch 350 Loss 2.2317 Accuracy 0.5142\n",
      "Epoch 40 Batch 400 Loss 2.2327 Accuracy 0.5141\n",
      "Epoch 40 Batch 450 Loss 2.2329 Accuracy 0.5140\n",
      "Epoch 40 Batch 500 Loss 2.2316 Accuracy 0.5141\n",
      "Epoch 40 Batch 550 Loss 2.2321 Accuracy 0.5139\n",
      "Epoch 40 Batch 600 Loss 2.2329 Accuracy 0.5137\n",
      "Epoch 40 Batch 650 Loss 2.2329 Accuracy 0.5138\n",
      "Epoch 40 Batch 700 Loss 2.2338 Accuracy 0.5136\n",
      "Epoch 40 Batch 750 Loss 2.2348 Accuracy 0.5135\n",
      "Epoch 40 Batch 800 Loss 2.2355 Accuracy 0.5135\n",
      "Epoch 40 Batch 850 Loss 2.2359 Accuracy 0.5135\n",
      "Epoch 40 Batch 900 Loss 2.2367 Accuracy 0.5133\n",
      "Epoch 40 Batch 950 Loss 2.2369 Accuracy 0.5132\n",
      "Epoch 40 Batch 1000 Loss 2.2367 Accuracy 0.5132\n",
      "Epoch 40 Batch 1050 Loss 2.2371 Accuracy 0.5131\n",
      "Epoch 40 Batch 1100 Loss 2.2382 Accuracy 0.5129\n",
      "Epoch 40 Batch 1150 Loss 2.2379 Accuracy 0.5130\n",
      "Epoch 40 Batch 1200 Loss 2.2382 Accuracy 0.5129\n",
      "Epoch 40 Batch 1250 Loss 2.2385 Accuracy 0.5129\n",
      "Epoch 40 Batch 1300 Loss 2.2387 Accuracy 0.5129\n",
      "Epoch 40 Batch 1350 Loss 2.2387 Accuracy 0.5130\n",
      "Epoch 40 Batch 1400 Loss 2.2389 Accuracy 0.5130\n",
      "Epoch 40 Batch 1450 Loss 2.2400 Accuracy 0.5129\n",
      "Epoch 40 Batch 1500 Loss 2.2401 Accuracy 0.5129\n",
      "Epoch 40 Batch 1550 Loss 2.2404 Accuracy 0.5128\n",
      "Epoch 40 Batch 1600 Loss 2.2408 Accuracy 0.5129\n",
      "Epoch 40 Batch 1650 Loss 2.2408 Accuracy 0.5130\n",
      "Epoch 40 Batch 1700 Loss 2.2416 Accuracy 0.5129\n",
      "Epoch 40 Batch 1750 Loss 2.2424 Accuracy 0.5127\n",
      "Epoch 40 Batch 1800 Loss 2.2421 Accuracy 0.5128\n",
      "Epoch 40 Batch 1850 Loss 2.2426 Accuracy 0.5128\n",
      "Epoch 40 Batch 1900 Loss 2.2428 Accuracy 0.5127\n",
      "Epoch 40 Batch 1950 Loss 2.2432 Accuracy 0.5126\n",
      "Epoch 40 Batch 2000 Loss 2.2432 Accuracy 0.5126\n",
      "Epoch 40 Batch 2050 Loss 2.2434 Accuracy 0.5126\n",
      "Epoch 40 Batch 2100 Loss 2.2437 Accuracy 0.5125\n",
      "Epoch 40 Batch 2150 Loss 2.2437 Accuracy 0.5125\n",
      "Epoch 40 Batch 2200 Loss 2.2439 Accuracy 0.5125\n",
      "Epoch 40 Batch 2250 Loss 2.2443 Accuracy 0.5124\n",
      "Epoch 40 Batch 2300 Loss 2.2445 Accuracy 0.5124\n",
      "Epoch 40 Batch 2350 Loss 2.2439 Accuracy 0.5125\n",
      "Epoch 40 Batch 2400 Loss 2.2441 Accuracy 0.5125\n",
      "Epoch 40 Batch 2450 Loss 2.2444 Accuracy 0.5125\n",
      "Epoch 40 Batch 2500 Loss 2.2449 Accuracy 0.5124\n",
      "Epoch 40 Batch 2550 Loss 2.2451 Accuracy 0.5124\n",
      "Epoch 40 Batch 2600 Loss 2.2456 Accuracy 0.5123\n",
      "Saving checkpoint for epoch 40 at ./checkpoints/train_full/ckpt-18\n",
      "Epoch 40 Loss 2.2459 Accuracy 0.5123\n",
      "Time taken for 1 epoch: 454.2269172668457 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "          print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "    history.append(train_loss.result())\n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.2032 Accuracy 0.5107\n",
      "Epoch 1 Batch 50 Loss 2.2361 Accuracy 0.5128\n",
      "Epoch 1 Batch 100 Loss 2.2264 Accuracy 0.5137\n",
      "Epoch 1 Batch 150 Loss 2.2211 Accuracy 0.5141\n",
      "Epoch 1 Batch 200 Loss 2.2285 Accuracy 0.5130\n",
      "Epoch 1 Batch 250 Loss 2.2293 Accuracy 0.5134\n",
      "Epoch 1 Batch 300 Loss 2.2282 Accuracy 0.5136\n",
      "Epoch 1 Batch 350 Loss 2.2289 Accuracy 0.5137\n",
      "Epoch 1 Batch 400 Loss 2.2280 Accuracy 0.5140\n",
      "Epoch 1 Batch 450 Loss 2.2276 Accuracy 0.5141\n",
      "Epoch 1 Batch 500 Loss 2.2285 Accuracy 0.5139\n",
      "Epoch 1 Batch 550 Loss 2.2287 Accuracy 0.5138\n",
      "Epoch 1 Batch 600 Loss 2.2292 Accuracy 0.5140\n",
      "Epoch 1 Batch 650 Loss 2.2297 Accuracy 0.5141\n",
      "Epoch 1 Batch 700 Loss 2.2315 Accuracy 0.5137\n",
      "Epoch 1 Batch 750 Loss 2.2322 Accuracy 0.5135\n",
      "Epoch 1 Batch 800 Loss 2.2313 Accuracy 0.5137\n",
      "Epoch 1 Batch 850 Loss 2.2331 Accuracy 0.5135\n",
      "Epoch 1 Batch 900 Loss 2.2332 Accuracy 0.5135\n",
      "Epoch 1 Batch 950 Loss 2.2334 Accuracy 0.5135\n",
      "Epoch 1 Batch 1000 Loss 2.2335 Accuracy 0.5136\n",
      "Epoch 1 Batch 1050 Loss 2.2337 Accuracy 0.5136\n",
      "Epoch 1 Batch 1100 Loss 2.2339 Accuracy 0.5135\n",
      "Epoch 1 Batch 1150 Loss 2.2339 Accuracy 0.5134\n",
      "Epoch 1 Batch 1200 Loss 2.2330 Accuracy 0.5136\n",
      "Epoch 1 Batch 1250 Loss 2.2332 Accuracy 0.5136\n",
      "Epoch 1 Batch 1300 Loss 2.2340 Accuracy 0.5135\n",
      "Epoch 1 Batch 1350 Loss 2.2341 Accuracy 0.5136\n",
      "Epoch 1 Batch 1400 Loss 2.2343 Accuracy 0.5136\n",
      "Epoch 1 Batch 1450 Loss 2.2347 Accuracy 0.5136\n",
      "Epoch 1 Batch 1500 Loss 2.2350 Accuracy 0.5136\n",
      "Epoch 1 Batch 1550 Loss 2.2354 Accuracy 0.5135\n",
      "Epoch 1 Batch 1600 Loss 2.2356 Accuracy 0.5135\n",
      "Epoch 1 Batch 1650 Loss 2.2357 Accuracy 0.5135\n",
      "Epoch 1 Batch 1700 Loss 2.2364 Accuracy 0.5135\n",
      "Epoch 1 Batch 1750 Loss 2.2365 Accuracy 0.5134\n",
      "Epoch 1 Batch 1800 Loss 2.2367 Accuracy 0.5135\n",
      "Epoch 1 Batch 1850 Loss 2.2368 Accuracy 0.5135\n",
      "Epoch 1 Batch 1900 Loss 2.2373 Accuracy 0.5134\n",
      "Epoch 1 Batch 1950 Loss 2.2376 Accuracy 0.5133\n",
      "Epoch 1 Batch 2000 Loss 2.2382 Accuracy 0.5132\n",
      "Epoch 1 Batch 2050 Loss 2.2382 Accuracy 0.5132\n",
      "Epoch 1 Batch 2100 Loss 2.2387 Accuracy 0.5131\n",
      "Epoch 1 Batch 2150 Loss 2.2387 Accuracy 0.5131\n",
      "Epoch 1 Batch 2200 Loss 2.2390 Accuracy 0.5131\n",
      "Epoch 1 Batch 2250 Loss 2.2392 Accuracy 0.5130\n",
      "Epoch 1 Batch 2300 Loss 2.2392 Accuracy 0.5130\n",
      "Epoch 1 Batch 2350 Loss 2.2393 Accuracy 0.5130\n",
      "Epoch 1 Batch 2400 Loss 2.2394 Accuracy 0.5130\n",
      "Epoch 1 Batch 2450 Loss 2.2397 Accuracy 0.5130\n",
      "Epoch 1 Batch 2500 Loss 2.2399 Accuracy 0.5129\n",
      "Epoch 1 Batch 2550 Loss 2.2404 Accuracy 0.5128\n",
      "Epoch 1 Batch 2600 Loss 2.2408 Accuracy 0.5127\n",
      "Epoch 1 Loss 2.2411 Accuracy 0.5126\n",
      "Time taken for 1 epoch: 455.93300437927246 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.2693 Accuracy 0.5245\n",
      "Epoch 2 Batch 50 Loss 2.2381 Accuracy 0.5136\n",
      "Epoch 2 Batch 100 Loss 2.2206 Accuracy 0.5153\n",
      "Epoch 2 Batch 150 Loss 2.2185 Accuracy 0.5153\n",
      "Epoch 2 Batch 200 Loss 2.2175 Accuracy 0.5153\n",
      "Epoch 2 Batch 250 Loss 2.2166 Accuracy 0.5159\n",
      "Epoch 2 Batch 300 Loss 2.2173 Accuracy 0.5156\n",
      "Epoch 2 Batch 350 Loss 2.2177 Accuracy 0.5157\n",
      "Epoch 2 Batch 400 Loss 2.2193 Accuracy 0.5153\n",
      "Epoch 2 Batch 450 Loss 2.2226 Accuracy 0.5149\n",
      "Epoch 2 Batch 500 Loss 2.2208 Accuracy 0.5153\n",
      "Epoch 2 Batch 550 Loss 2.2224 Accuracy 0.5150\n",
      "Epoch 2 Batch 600 Loss 2.2235 Accuracy 0.5149\n",
      "Epoch 2 Batch 650 Loss 2.2245 Accuracy 0.5147\n",
      "Epoch 2 Batch 700 Loss 2.2260 Accuracy 0.5144\n",
      "Epoch 2 Batch 750 Loss 2.2261 Accuracy 0.5144\n",
      "Epoch 2 Batch 800 Loss 2.2271 Accuracy 0.5143\n",
      "Epoch 2 Batch 850 Loss 2.2283 Accuracy 0.5143\n",
      "Epoch 2 Batch 900 Loss 2.2291 Accuracy 0.5143\n",
      "Epoch 2 Batch 950 Loss 2.2294 Accuracy 0.5143\n",
      "Epoch 2 Batch 1000 Loss 2.2298 Accuracy 0.5144\n",
      "Epoch 2 Batch 1050 Loss 2.2296 Accuracy 0.5144\n",
      "Epoch 2 Batch 1100 Loss 2.2301 Accuracy 0.5143\n",
      "Epoch 2 Batch 1150 Loss 2.2303 Accuracy 0.5143\n",
      "Epoch 2 Batch 1200 Loss 2.2300 Accuracy 0.5143\n",
      "Epoch 2 Batch 1250 Loss 2.2306 Accuracy 0.5142\n",
      "Epoch 2 Batch 1300 Loss 2.2311 Accuracy 0.5141\n",
      "Epoch 2 Batch 1350 Loss 2.2316 Accuracy 0.5141\n",
      "Epoch 2 Batch 1400 Loss 2.2326 Accuracy 0.5139\n",
      "Epoch 2 Batch 1450 Loss 2.2326 Accuracy 0.5140\n",
      "Epoch 2 Batch 1500 Loss 2.2329 Accuracy 0.5140\n",
      "Epoch 2 Batch 1550 Loss 2.2333 Accuracy 0.5140\n",
      "Epoch 2 Batch 1600 Loss 2.2333 Accuracy 0.5139\n",
      "Epoch 2 Batch 1650 Loss 2.2332 Accuracy 0.5140\n",
      "Epoch 2 Batch 1700 Loss 2.2336 Accuracy 0.5140\n",
      "Epoch 2 Batch 1750 Loss 2.2333 Accuracy 0.5140\n",
      "Epoch 2 Batch 1800 Loss 2.2334 Accuracy 0.5139\n",
      "Epoch 2 Batch 1850 Loss 2.2338 Accuracy 0.5139\n",
      "Epoch 2 Batch 1900 Loss 2.2340 Accuracy 0.5138\n",
      "Epoch 2 Batch 1950 Loss 2.2346 Accuracy 0.5137\n",
      "Epoch 2 Batch 2000 Loss 2.2349 Accuracy 0.5136\n",
      "Epoch 2 Batch 2050 Loss 2.2348 Accuracy 0.5136\n",
      "Epoch 2 Batch 2100 Loss 2.2346 Accuracy 0.5136\n",
      "Epoch 2 Batch 2150 Loss 2.2343 Accuracy 0.5137\n",
      "Epoch 2 Batch 2200 Loss 2.2345 Accuracy 0.5137\n",
      "Epoch 2 Batch 2250 Loss 2.2349 Accuracy 0.5136\n",
      "Epoch 2 Batch 2300 Loss 2.2350 Accuracy 0.5136\n",
      "Epoch 2 Batch 2350 Loss 2.2348 Accuracy 0.5136\n",
      "Epoch 2 Batch 2400 Loss 2.2351 Accuracy 0.5136\n",
      "Epoch 2 Batch 2450 Loss 2.2351 Accuracy 0.5136\n",
      "Epoch 2 Batch 2500 Loss 2.2356 Accuracy 0.5136\n",
      "Epoch 2 Batch 2550 Loss 2.2361 Accuracy 0.5134\n",
      "Epoch 2 Batch 2600 Loss 2.2366 Accuracy 0.5133\n",
      "Epoch 2 Loss 2.2368 Accuracy 0.5133\n",
      "Time taken for 1 epoch: 454.447829246521 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.2259 Accuracy 0.5240\n",
      "Epoch 3 Batch 50 Loss 2.2077 Accuracy 0.5185\n",
      "Epoch 3 Batch 100 Loss 2.2041 Accuracy 0.5186\n",
      "Epoch 3 Batch 150 Loss 2.2061 Accuracy 0.5181\n",
      "Epoch 3 Batch 200 Loss 2.2099 Accuracy 0.5171\n",
      "Epoch 3 Batch 250 Loss 2.2092 Accuracy 0.5178\n",
      "Epoch 3 Batch 300 Loss 2.2123 Accuracy 0.5172\n",
      "Epoch 3 Batch 350 Loss 2.2145 Accuracy 0.5169\n",
      "Epoch 3 Batch 400 Loss 2.2147 Accuracy 0.5166\n",
      "Epoch 3 Batch 450 Loss 2.2149 Accuracy 0.5165\n",
      "Epoch 3 Batch 500 Loss 2.2146 Accuracy 0.5166\n",
      "Epoch 3 Batch 550 Loss 2.2151 Accuracy 0.5164\n",
      "Epoch 3 Batch 600 Loss 2.2160 Accuracy 0.5162\n",
      "Epoch 3 Batch 650 Loss 2.2169 Accuracy 0.5161\n",
      "Epoch 3 Batch 700 Loss 2.2183 Accuracy 0.5159\n",
      "Epoch 3 Batch 750 Loss 2.2197 Accuracy 0.5159\n",
      "Epoch 3 Batch 800 Loss 2.2207 Accuracy 0.5157\n",
      "Epoch 3 Batch 850 Loss 2.2201 Accuracy 0.5158\n",
      "Epoch 3 Batch 900 Loss 2.2201 Accuracy 0.5158\n",
      "Epoch 3 Batch 950 Loss 2.2206 Accuracy 0.5158\n",
      "Epoch 3 Batch 1000 Loss 2.2205 Accuracy 0.5158\n",
      "Epoch 3 Batch 1050 Loss 2.2215 Accuracy 0.5157\n",
      "Epoch 3 Batch 1100 Loss 2.2219 Accuracy 0.5157\n",
      "Epoch 3 Batch 1150 Loss 2.2228 Accuracy 0.5155\n",
      "Epoch 3 Batch 1200 Loss 2.2227 Accuracy 0.5155\n",
      "Epoch 3 Batch 1250 Loss 2.2225 Accuracy 0.5155\n",
      "Epoch 3 Batch 1300 Loss 2.2225 Accuracy 0.5155\n",
      "Epoch 3 Batch 1350 Loss 2.2235 Accuracy 0.5153\n",
      "Epoch 3 Batch 1400 Loss 2.2241 Accuracy 0.5153\n",
      "Epoch 3 Batch 1450 Loss 2.2247 Accuracy 0.5152\n",
      "Epoch 3 Batch 1500 Loss 2.2247 Accuracy 0.5152\n",
      "Epoch 3 Batch 1550 Loss 2.2257 Accuracy 0.5151\n",
      "Epoch 3 Batch 1600 Loss 2.2258 Accuracy 0.5151\n",
      "Epoch 3 Batch 1650 Loss 2.2258 Accuracy 0.5151\n",
      "Epoch 3 Batch 1700 Loss 2.2267 Accuracy 0.5150\n",
      "Epoch 3 Batch 1750 Loss 2.2268 Accuracy 0.5150\n",
      "Epoch 3 Batch 1800 Loss 2.2272 Accuracy 0.5150\n",
      "Epoch 3 Batch 1850 Loss 2.2281 Accuracy 0.5148\n",
      "Epoch 3 Batch 1900 Loss 2.2284 Accuracy 0.5147\n",
      "Epoch 3 Batch 1950 Loss 2.2286 Accuracy 0.5147\n",
      "Epoch 3 Batch 2000 Loss 2.2285 Accuracy 0.5147\n",
      "Epoch 3 Batch 2050 Loss 2.2288 Accuracy 0.5147\n",
      "Epoch 3 Batch 2100 Loss 2.2292 Accuracy 0.5146\n",
      "Epoch 3 Batch 2150 Loss 2.2293 Accuracy 0.5146\n",
      "Epoch 3 Batch 2200 Loss 2.2295 Accuracy 0.5146\n",
      "Epoch 3 Batch 2250 Loss 2.2295 Accuracy 0.5146\n",
      "Epoch 3 Batch 2300 Loss 2.2297 Accuracy 0.5146\n",
      "Epoch 3 Batch 2350 Loss 2.2301 Accuracy 0.5145\n",
      "Epoch 3 Batch 2400 Loss 2.2302 Accuracy 0.5145\n",
      "Epoch 3 Batch 2450 Loss 2.2307 Accuracy 0.5144\n",
      "Epoch 3 Batch 2500 Loss 2.2308 Accuracy 0.5144\n",
      "Epoch 3 Batch 2550 Loss 2.2308 Accuracy 0.5144\n",
      "Epoch 3 Batch 2600 Loss 2.2313 Accuracy 0.5144\n",
      "Epoch 3 Loss 2.2315 Accuracy 0.5143\n",
      "Time taken for 1 epoch: 453.56561827659607 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.3663 Accuracy 0.4749\n",
      "Epoch 4 Batch 50 Loss 2.2145 Accuracy 0.5121\n",
      "Epoch 4 Batch 100 Loss 2.2132 Accuracy 0.5138\n",
      "Epoch 4 Batch 150 Loss 2.2125 Accuracy 0.5148\n",
      "Epoch 4 Batch 200 Loss 2.2134 Accuracy 0.5147\n",
      "Epoch 4 Batch 250 Loss 2.2129 Accuracy 0.5155\n",
      "Epoch 4 Batch 300 Loss 2.2107 Accuracy 0.5163\n",
      "Epoch 4 Batch 350 Loss 2.2119 Accuracy 0.5165\n",
      "Epoch 4 Batch 400 Loss 2.2141 Accuracy 0.5162\n",
      "Epoch 4 Batch 450 Loss 2.2139 Accuracy 0.5161\n",
      "Epoch 4 Batch 500 Loss 2.2142 Accuracy 0.5162\n",
      "Epoch 4 Batch 550 Loss 2.2151 Accuracy 0.5161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 600 Loss 2.2146 Accuracy 0.5165\n",
      "Epoch 4 Batch 650 Loss 2.2168 Accuracy 0.5161\n",
      "Epoch 4 Batch 700 Loss 2.2165 Accuracy 0.5160\n",
      "Epoch 4 Batch 750 Loss 2.2176 Accuracy 0.5160\n",
      "Epoch 4 Batch 800 Loss 2.2185 Accuracy 0.5158\n",
      "Epoch 4 Batch 850 Loss 2.2185 Accuracy 0.5158\n",
      "Epoch 4 Batch 900 Loss 2.2184 Accuracy 0.5159\n",
      "Epoch 4 Batch 950 Loss 2.2183 Accuracy 0.5158\n",
      "Epoch 4 Batch 1000 Loss 2.2189 Accuracy 0.5158\n",
      "Epoch 4 Batch 1050 Loss 2.2190 Accuracy 0.5159\n",
      "Epoch 4 Batch 1100 Loss 2.2194 Accuracy 0.5157\n",
      "Epoch 4 Batch 1150 Loss 2.2191 Accuracy 0.5158\n",
      "Epoch 4 Batch 1200 Loss 2.2194 Accuracy 0.5158\n",
      "Epoch 4 Batch 1250 Loss 2.2192 Accuracy 0.5159\n",
      "Epoch 4 Batch 1300 Loss 2.2193 Accuracy 0.5158\n",
      "Epoch 4 Batch 1350 Loss 2.2196 Accuracy 0.5158\n",
      "Epoch 4 Batch 1400 Loss 2.2201 Accuracy 0.5157\n",
      "Epoch 4 Batch 1450 Loss 2.2207 Accuracy 0.5157\n",
      "Epoch 4 Batch 1500 Loss 2.2210 Accuracy 0.5157\n",
      "Epoch 4 Batch 1550 Loss 2.2214 Accuracy 0.5157\n",
      "Epoch 4 Batch 1600 Loss 2.2214 Accuracy 0.5157\n",
      "Epoch 4 Batch 1650 Loss 2.2212 Accuracy 0.5157\n",
      "Epoch 4 Batch 1700 Loss 2.2215 Accuracy 0.5157\n",
      "Epoch 4 Batch 1750 Loss 2.2217 Accuracy 0.5156\n",
      "Epoch 4 Batch 1800 Loss 2.2228 Accuracy 0.5154\n",
      "Epoch 4 Batch 1850 Loss 2.2231 Accuracy 0.5154\n",
      "Epoch 4 Batch 1900 Loss 2.2238 Accuracy 0.5152\n",
      "Epoch 4 Batch 1950 Loss 2.2241 Accuracy 0.5153\n",
      "Epoch 4 Batch 2000 Loss 2.2242 Accuracy 0.5153\n",
      "Epoch 4 Batch 2050 Loss 2.2243 Accuracy 0.5153\n",
      "Epoch 4 Batch 2100 Loss 2.2246 Accuracy 0.5153\n",
      "Epoch 4 Batch 2150 Loss 2.2247 Accuracy 0.5153\n",
      "Epoch 4 Batch 2200 Loss 2.2246 Accuracy 0.5153\n",
      "Epoch 4 Batch 2250 Loss 2.2248 Accuracy 0.5153\n",
      "Epoch 4 Batch 2300 Loss 2.2252 Accuracy 0.5152\n",
      "Epoch 4 Batch 2350 Loss 2.2255 Accuracy 0.5152\n",
      "Epoch 4 Batch 2400 Loss 2.2256 Accuracy 0.5152\n",
      "Epoch 4 Batch 2450 Loss 2.2260 Accuracy 0.5151\n",
      "Epoch 4 Batch 2500 Loss 2.2262 Accuracy 0.5151\n",
      "Epoch 4 Batch 2550 Loss 2.2264 Accuracy 0.5151\n",
      "Epoch 4 Batch 2600 Loss 2.2270 Accuracy 0.5150\n",
      "Epoch 4 Loss 2.2270 Accuracy 0.5151\n",
      "Time taken for 1 epoch: 453.4662284851074 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.1506 Accuracy 0.5226\n",
      "Epoch 5 Batch 50 Loss 2.1952 Accuracy 0.5181\n",
      "Epoch 5 Batch 100 Loss 2.1953 Accuracy 0.5191\n",
      "Epoch 5 Batch 150 Loss 2.1962 Accuracy 0.5186\n",
      "Epoch 5 Batch 200 Loss 2.1944 Accuracy 0.5195\n",
      "Epoch 5 Batch 250 Loss 2.2004 Accuracy 0.5185\n",
      "Epoch 5 Batch 300 Loss 2.2027 Accuracy 0.5181\n",
      "Epoch 5 Batch 350 Loss 2.2051 Accuracy 0.5177\n",
      "Epoch 5 Batch 400 Loss 2.2051 Accuracy 0.5179\n",
      "Epoch 5 Batch 450 Loss 2.2060 Accuracy 0.5178\n",
      "Epoch 5 Batch 500 Loss 2.2084 Accuracy 0.5174\n",
      "Epoch 5 Batch 550 Loss 2.2091 Accuracy 0.5174\n",
      "Epoch 5 Batch 600 Loss 2.2097 Accuracy 0.5174\n",
      "Epoch 5 Batch 650 Loss 2.2094 Accuracy 0.5174\n",
      "Epoch 5 Batch 700 Loss 2.2109 Accuracy 0.5173\n",
      "Epoch 5 Batch 750 Loss 2.2123 Accuracy 0.5171\n",
      "Epoch 5 Batch 800 Loss 2.2123 Accuracy 0.5172\n",
      "Epoch 5 Batch 850 Loss 2.2126 Accuracy 0.5170\n",
      "Epoch 5 Batch 900 Loss 2.2128 Accuracy 0.5171\n",
      "Epoch 5 Batch 950 Loss 2.2130 Accuracy 0.5171\n",
      "Epoch 5 Batch 1000 Loss 2.2143 Accuracy 0.5169\n",
      "Epoch 5 Batch 1050 Loss 2.2151 Accuracy 0.5168\n",
      "Epoch 5 Batch 1100 Loss 2.2149 Accuracy 0.5168\n",
      "Epoch 5 Batch 1150 Loss 2.2145 Accuracy 0.5169\n",
      "Epoch 5 Batch 1200 Loss 2.2150 Accuracy 0.5168\n",
      "Epoch 5 Batch 1250 Loss 2.2151 Accuracy 0.5168\n",
      "Epoch 5 Batch 1300 Loss 2.2152 Accuracy 0.5167\n",
      "Epoch 5 Batch 1350 Loss 2.2156 Accuracy 0.5166\n",
      "Epoch 5 Batch 1400 Loss 2.2154 Accuracy 0.5167\n",
      "Epoch 5 Batch 1450 Loss 2.2159 Accuracy 0.5166\n",
      "Epoch 5 Batch 1500 Loss 2.2161 Accuracy 0.5166\n",
      "Epoch 5 Batch 1550 Loss 2.2161 Accuracy 0.5165\n",
      "Epoch 5 Batch 1600 Loss 2.2165 Accuracy 0.5166\n",
      "Epoch 5 Batch 1650 Loss 2.2169 Accuracy 0.5165\n",
      "Epoch 5 Batch 1700 Loss 2.2177 Accuracy 0.5164\n",
      "Epoch 5 Batch 1750 Loss 2.2174 Accuracy 0.5164\n",
      "Epoch 5 Batch 1800 Loss 2.2176 Accuracy 0.5165\n",
      "Epoch 5 Batch 1850 Loss 2.2175 Accuracy 0.5164\n",
      "Epoch 5 Batch 1900 Loss 2.2178 Accuracy 0.5164\n",
      "Epoch 5 Batch 1950 Loss 2.2175 Accuracy 0.5164\n",
      "Epoch 5 Batch 2000 Loss 2.2182 Accuracy 0.5163\n",
      "Epoch 5 Batch 2050 Loss 2.2187 Accuracy 0.5162\n",
      "Epoch 5 Batch 2100 Loss 2.2184 Accuracy 0.5162\n",
      "Epoch 5 Batch 2150 Loss 2.2185 Accuracy 0.5162\n",
      "Epoch 5 Batch 2200 Loss 2.2190 Accuracy 0.5161\n",
      "Epoch 5 Batch 2250 Loss 2.2193 Accuracy 0.5161\n",
      "Epoch 5 Batch 2300 Loss 2.2195 Accuracy 0.5161\n",
      "Epoch 5 Batch 2350 Loss 2.2195 Accuracy 0.5161\n",
      "Epoch 5 Batch 2400 Loss 2.2198 Accuracy 0.5161\n",
      "Epoch 5 Batch 2450 Loss 2.2203 Accuracy 0.5160\n",
      "Epoch 5 Batch 2500 Loss 2.2211 Accuracy 0.5159\n",
      "Epoch 5 Batch 2550 Loss 2.2216 Accuracy 0.5158\n",
      "Epoch 5 Batch 2600 Loss 2.2218 Accuracy 0.5158\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train_full/ckpt-19\n",
      "Epoch 5 Loss 2.2220 Accuracy 0.5158\n",
      "Time taken for 1 epoch: 454.330748796463 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.1660 Accuracy 0.5191\n",
      "Epoch 6 Batch 50 Loss 2.2193 Accuracy 0.5129\n",
      "Epoch 6 Batch 100 Loss 2.2098 Accuracy 0.5162\n",
      "Epoch 6 Batch 150 Loss 2.2029 Accuracy 0.5173\n",
      "Epoch 6 Batch 200 Loss 2.2047 Accuracy 0.5172\n",
      "Epoch 6 Batch 250 Loss 2.2033 Accuracy 0.5176\n",
      "Epoch 6 Batch 300 Loss 2.2024 Accuracy 0.5179\n",
      "Epoch 6 Batch 350 Loss 2.2022 Accuracy 0.5180\n",
      "Epoch 6 Batch 400 Loss 2.2034 Accuracy 0.5178\n",
      "Epoch 6 Batch 450 Loss 2.2035 Accuracy 0.5177\n",
      "Epoch 6 Batch 500 Loss 2.2044 Accuracy 0.5176\n",
      "Epoch 6 Batch 550 Loss 2.2041 Accuracy 0.5177\n",
      "Epoch 6 Batch 600 Loss 2.2048 Accuracy 0.5175\n",
      "Epoch 6 Batch 650 Loss 2.2048 Accuracy 0.5175\n",
      "Epoch 6 Batch 700 Loss 2.2063 Accuracy 0.5174\n",
      "Epoch 6 Batch 750 Loss 2.2073 Accuracy 0.5174\n",
      "Epoch 6 Batch 800 Loss 2.2076 Accuracy 0.5174\n",
      "Epoch 6 Batch 850 Loss 2.2075 Accuracy 0.5173\n",
      "Epoch 6 Batch 900 Loss 2.2078 Accuracy 0.5173\n",
      "Epoch 6 Batch 950 Loss 2.2082 Accuracy 0.5172\n",
      "Epoch 6 Batch 1000 Loss 2.2086 Accuracy 0.5172\n",
      "Epoch 6 Batch 1050 Loss 2.2094 Accuracy 0.5172\n",
      "Epoch 6 Batch 1100 Loss 2.2091 Accuracy 0.5173\n",
      "Epoch 6 Batch 1150 Loss 2.2092 Accuracy 0.5172\n",
      "Epoch 6 Batch 1200 Loss 2.2096 Accuracy 0.5172\n",
      "Epoch 6 Batch 1250 Loss 2.2100 Accuracy 0.5172\n",
      "Epoch 6 Batch 1300 Loss 2.2106 Accuracy 0.5170\n",
      "Epoch 6 Batch 1350 Loss 2.2110 Accuracy 0.5170\n",
      "Epoch 6 Batch 1400 Loss 2.2116 Accuracy 0.5168\n",
      "Epoch 6 Batch 1450 Loss 2.2114 Accuracy 0.5169\n",
      "Epoch 6 Batch 1500 Loss 2.2115 Accuracy 0.5169\n",
      "Epoch 6 Batch 1550 Loss 2.2120 Accuracy 0.5168\n",
      "Epoch 6 Batch 1600 Loss 2.2121 Accuracy 0.5169\n",
      "Epoch 6 Batch 1650 Loss 2.2125 Accuracy 0.5168\n",
      "Epoch 6 Batch 1700 Loss 2.2124 Accuracy 0.5169\n",
      "Epoch 6 Batch 1750 Loss 2.2131 Accuracy 0.5167\n",
      "Epoch 6 Batch 1800 Loss 2.2132 Accuracy 0.5167\n",
      "Epoch 6 Batch 1850 Loss 2.2133 Accuracy 0.5167\n",
      "Epoch 6 Batch 1900 Loss 2.2135 Accuracy 0.5167\n",
      "Epoch 6 Batch 1950 Loss 2.2139 Accuracy 0.5166\n",
      "Epoch 6 Batch 2000 Loss 2.2141 Accuracy 0.5166\n",
      "Epoch 6 Batch 2050 Loss 2.2141 Accuracy 0.5167\n",
      "Epoch 6 Batch 2100 Loss 2.2142 Accuracy 0.5167\n",
      "Epoch 6 Batch 2150 Loss 2.2144 Accuracy 0.5166\n",
      "Epoch 6 Batch 2200 Loss 2.2149 Accuracy 0.5166\n",
      "Epoch 6 Batch 2250 Loss 2.2152 Accuracy 0.5166\n",
      "Epoch 6 Batch 2300 Loss 2.2154 Accuracy 0.5165\n",
      "Epoch 6 Batch 2350 Loss 2.2155 Accuracy 0.5166\n",
      "Epoch 6 Batch 2400 Loss 2.2159 Accuracy 0.5165\n",
      "Epoch 6 Batch 2450 Loss 2.2162 Accuracy 0.5164\n",
      "Epoch 6 Batch 2500 Loss 2.2166 Accuracy 0.5164\n",
      "Epoch 6 Batch 2550 Loss 2.2173 Accuracy 0.5163\n",
      "Epoch 6 Batch 2600 Loss 2.2177 Accuracy 0.5162\n",
      "Epoch 6 Loss 2.2182 Accuracy 0.5161\n",
      "Time taken for 1 epoch: 456.6750330924988 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.2353 Accuracy 0.5033\n",
      "Epoch 7 Batch 50 Loss 2.1864 Accuracy 0.5203\n",
      "Epoch 7 Batch 100 Loss 2.1790 Accuracy 0.5219\n",
      "Epoch 7 Batch 150 Loss 2.1847 Accuracy 0.5206\n",
      "Epoch 7 Batch 200 Loss 2.1867 Accuracy 0.5203\n",
      "Epoch 7 Batch 250 Loss 2.1881 Accuracy 0.5203\n",
      "Epoch 7 Batch 300 Loss 2.1919 Accuracy 0.5201\n",
      "Epoch 7 Batch 350 Loss 2.1935 Accuracy 0.5197\n",
      "Epoch 7 Batch 400 Loss 2.1959 Accuracy 0.5192\n",
      "Epoch 7 Batch 450 Loss 2.1951 Accuracy 0.5192\n",
      "Epoch 7 Batch 500 Loss 2.1952 Accuracy 0.5192\n",
      "Epoch 7 Batch 550 Loss 2.1976 Accuracy 0.5190\n",
      "Epoch 7 Batch 600 Loss 2.2002 Accuracy 0.5185\n",
      "Epoch 7 Batch 650 Loss 2.2006 Accuracy 0.5186\n",
      "Epoch 7 Batch 700 Loss 2.2018 Accuracy 0.5185\n",
      "Epoch 7 Batch 750 Loss 2.2032 Accuracy 0.5183\n",
      "Epoch 7 Batch 800 Loss 2.2030 Accuracy 0.5184\n",
      "Epoch 7 Batch 850 Loss 2.2029 Accuracy 0.5185\n",
      "Epoch 7 Batch 900 Loss 2.2026 Accuracy 0.5185\n",
      "Epoch 7 Batch 950 Loss 2.2039 Accuracy 0.5183\n",
      "Epoch 7 Batch 1000 Loss 2.2037 Accuracy 0.5183\n",
      "Epoch 7 Batch 1050 Loss 2.2044 Accuracy 0.5181\n",
      "Epoch 7 Batch 1100 Loss 2.2047 Accuracy 0.5181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 1150 Loss 2.2050 Accuracy 0.5181\n",
      "Epoch 7 Batch 1200 Loss 2.2053 Accuracy 0.5180\n",
      "Epoch 7 Batch 1250 Loss 2.2053 Accuracy 0.5180\n",
      "Epoch 7 Batch 1300 Loss 2.2051 Accuracy 0.5180\n",
      "Epoch 7 Batch 1350 Loss 2.2046 Accuracy 0.5181\n",
      "Epoch 7 Batch 1400 Loss 2.2052 Accuracy 0.5181\n",
      "Epoch 7 Batch 1450 Loss 2.2061 Accuracy 0.5179\n",
      "Epoch 7 Batch 1500 Loss 2.2072 Accuracy 0.5177\n",
      "Epoch 7 Batch 1550 Loss 2.2070 Accuracy 0.5178\n",
      "Epoch 7 Batch 1600 Loss 2.2071 Accuracy 0.5178\n",
      "Epoch 7 Batch 1650 Loss 2.2071 Accuracy 0.5178\n",
      "Epoch 7 Batch 1700 Loss 2.2074 Accuracy 0.5178\n",
      "Epoch 7 Batch 1750 Loss 2.2078 Accuracy 0.5177\n",
      "Epoch 7 Batch 1800 Loss 2.2079 Accuracy 0.5178\n",
      "Epoch 7 Batch 1850 Loss 2.2081 Accuracy 0.5178\n",
      "Epoch 7 Batch 1900 Loss 2.2086 Accuracy 0.5176\n",
      "Epoch 7 Batch 1950 Loss 2.2088 Accuracy 0.5176\n",
      "Epoch 7 Batch 2000 Loss 2.2091 Accuracy 0.5176\n",
      "Epoch 7 Batch 2050 Loss 2.2094 Accuracy 0.5175\n",
      "Epoch 7 Batch 2100 Loss 2.2096 Accuracy 0.5175\n",
      "Epoch 7 Batch 2150 Loss 2.2101 Accuracy 0.5174\n",
      "Epoch 7 Batch 2200 Loss 2.2102 Accuracy 0.5174\n",
      "Epoch 7 Batch 2250 Loss 2.2107 Accuracy 0.5173\n",
      "Epoch 7 Batch 2300 Loss 2.2107 Accuracy 0.5173\n",
      "Epoch 7 Batch 2350 Loss 2.2110 Accuracy 0.5173\n",
      "Epoch 7 Batch 2400 Loss 2.2117 Accuracy 0.5172\n",
      "Epoch 7 Batch 2450 Loss 2.2121 Accuracy 0.5171\n",
      "Epoch 7 Batch 2500 Loss 2.2127 Accuracy 0.5170\n",
      "Epoch 7 Batch 2550 Loss 2.2129 Accuracy 0.5169\n",
      "Epoch 7 Batch 2600 Loss 2.2135 Accuracy 0.5168\n",
      "Epoch 7 Loss 2.2135 Accuracy 0.5169\n",
      "Time taken for 1 epoch: 454.48598885536194 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.1689 Accuracy 0.5232\n",
      "Epoch 8 Batch 50 Loss 2.1865 Accuracy 0.5192\n",
      "Epoch 8 Batch 100 Loss 2.1843 Accuracy 0.5204\n",
      "Epoch 8 Batch 150 Loss 2.1861 Accuracy 0.5204\n",
      "Epoch 8 Batch 200 Loss 2.1899 Accuracy 0.5197\n",
      "Epoch 8 Batch 250 Loss 2.1914 Accuracy 0.5191\n",
      "Epoch 8 Batch 300 Loss 2.1937 Accuracy 0.5192\n",
      "Epoch 8 Batch 350 Loss 2.1975 Accuracy 0.5186\n",
      "Epoch 8 Batch 400 Loss 2.1989 Accuracy 0.5184\n",
      "Epoch 8 Batch 450 Loss 2.1991 Accuracy 0.5182\n",
      "Epoch 8 Batch 500 Loss 2.1990 Accuracy 0.5184\n",
      "Epoch 8 Batch 550 Loss 2.1992 Accuracy 0.5184\n",
      "Epoch 8 Batch 600 Loss 2.1990 Accuracy 0.5184\n",
      "Epoch 8 Batch 650 Loss 2.1985 Accuracy 0.5185\n",
      "Epoch 8 Batch 700 Loss 2.1985 Accuracy 0.5184\n",
      "Epoch 8 Batch 750 Loss 2.1998 Accuracy 0.5182\n",
      "Epoch 8 Batch 800 Loss 2.1985 Accuracy 0.5184\n",
      "Epoch 8 Batch 850 Loss 2.1995 Accuracy 0.5184\n",
      "Epoch 8 Batch 900 Loss 2.2000 Accuracy 0.5183\n",
      "Epoch 8 Batch 950 Loss 2.2006 Accuracy 0.5183\n",
      "Epoch 8 Batch 1000 Loss 2.2013 Accuracy 0.5182\n",
      "Epoch 8 Batch 1050 Loss 2.2005 Accuracy 0.5184\n",
      "Epoch 8 Batch 1100 Loss 2.2000 Accuracy 0.5186\n",
      "Epoch 8 Batch 1150 Loss 2.2011 Accuracy 0.5184\n",
      "Epoch 8 Batch 1200 Loss 2.2013 Accuracy 0.5184\n",
      "Epoch 8 Batch 1250 Loss 2.2012 Accuracy 0.5184\n",
      "Epoch 8 Batch 1300 Loss 2.2014 Accuracy 0.5183\n",
      "Epoch 8 Batch 1350 Loss 2.2017 Accuracy 0.5183\n",
      "Epoch 8 Batch 1400 Loss 2.2026 Accuracy 0.5182\n",
      "Epoch 8 Batch 1450 Loss 2.2025 Accuracy 0.5182\n",
      "Epoch 8 Batch 1500 Loss 2.2031 Accuracy 0.5181\n",
      "Epoch 8 Batch 1550 Loss 2.2031 Accuracy 0.5180\n",
      "Epoch 8 Batch 1600 Loss 2.2035 Accuracy 0.5180\n",
      "Epoch 8 Batch 1650 Loss 2.2041 Accuracy 0.5180\n",
      "Epoch 8 Batch 1700 Loss 2.2041 Accuracy 0.5180\n",
      "Epoch 8 Batch 1750 Loss 2.2044 Accuracy 0.5179\n",
      "Epoch 8 Batch 1800 Loss 2.2044 Accuracy 0.5179\n",
      "Epoch 8 Batch 1850 Loss 2.2045 Accuracy 0.5179\n",
      "Epoch 8 Batch 1900 Loss 2.2040 Accuracy 0.5180\n",
      "Epoch 8 Batch 1950 Loss 2.2050 Accuracy 0.5178\n",
      "Epoch 8 Batch 2000 Loss 2.2049 Accuracy 0.5179\n",
      "Epoch 8 Batch 2050 Loss 2.2055 Accuracy 0.5178\n",
      "Epoch 8 Batch 2100 Loss 2.2058 Accuracy 0.5177\n",
      "Epoch 8 Batch 2150 Loss 2.2062 Accuracy 0.5177\n",
      "Epoch 8 Batch 2200 Loss 2.2062 Accuracy 0.5178\n",
      "Epoch 8 Batch 2250 Loss 2.2069 Accuracy 0.5177\n",
      "Epoch 8 Batch 2300 Loss 2.2072 Accuracy 0.5176\n",
      "Epoch 8 Batch 2350 Loss 2.2074 Accuracy 0.5176\n",
      "Epoch 8 Batch 2400 Loss 2.2077 Accuracy 0.5175\n",
      "Epoch 8 Batch 2450 Loss 2.2080 Accuracy 0.5176\n",
      "Epoch 8 Batch 2500 Loss 2.2086 Accuracy 0.5175\n",
      "Epoch 8 Batch 2550 Loss 2.2091 Accuracy 0.5174\n",
      "Epoch 8 Batch 2600 Loss 2.2094 Accuracy 0.5173\n",
      "Epoch 8 Loss 2.2094 Accuracy 0.5174\n",
      "Time taken for 1 epoch: 453.77208852767944 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.1420 Accuracy 0.5208\n",
      "Epoch 9 Batch 50 Loss 2.1852 Accuracy 0.5203\n",
      "Epoch 9 Batch 100 Loss 2.1838 Accuracy 0.5204\n",
      "Epoch 9 Batch 150 Loss 2.1839 Accuracy 0.5198\n",
      "Epoch 9 Batch 200 Loss 2.1782 Accuracy 0.5211\n",
      "Epoch 9 Batch 250 Loss 2.1828 Accuracy 0.5202\n",
      "Epoch 9 Batch 300 Loss 2.1849 Accuracy 0.5201\n",
      "Epoch 9 Batch 350 Loss 2.1874 Accuracy 0.5199\n",
      "Epoch 9 Batch 400 Loss 2.1906 Accuracy 0.5196\n",
      "Epoch 9 Batch 450 Loss 2.1887 Accuracy 0.5198\n",
      "Epoch 9 Batch 500 Loss 2.1883 Accuracy 0.5199\n",
      "Epoch 9 Batch 550 Loss 2.1890 Accuracy 0.5198\n",
      "Epoch 9 Batch 600 Loss 2.1915 Accuracy 0.5193\n",
      "Epoch 9 Batch 650 Loss 2.1936 Accuracy 0.5191\n",
      "Epoch 9 Batch 700 Loss 2.1921 Accuracy 0.5193\n",
      "Epoch 9 Batch 750 Loss 2.1928 Accuracy 0.5192\n",
      "Epoch 9 Batch 800 Loss 2.1939 Accuracy 0.5190\n",
      "Epoch 9 Batch 850 Loss 2.1935 Accuracy 0.5190\n",
      "Epoch 9 Batch 900 Loss 2.1938 Accuracy 0.5190\n",
      "Epoch 9 Batch 950 Loss 2.1945 Accuracy 0.5190\n",
      "Epoch 9 Batch 1000 Loss 2.1950 Accuracy 0.5189\n",
      "Epoch 9 Batch 1050 Loss 2.1950 Accuracy 0.5189\n",
      "Epoch 9 Batch 1100 Loss 2.1956 Accuracy 0.5188\n",
      "Epoch 9 Batch 1150 Loss 2.1962 Accuracy 0.5188\n",
      "Epoch 9 Batch 1200 Loss 2.1965 Accuracy 0.5187\n",
      "Epoch 9 Batch 1250 Loss 2.1962 Accuracy 0.5188\n",
      "Epoch 9 Batch 1300 Loss 2.1955 Accuracy 0.5188\n",
      "Epoch 9 Batch 1350 Loss 2.1964 Accuracy 0.5187\n",
      "Epoch 9 Batch 1400 Loss 2.1967 Accuracy 0.5187\n",
      "Epoch 9 Batch 1450 Loss 2.1972 Accuracy 0.5187\n",
      "Epoch 9 Batch 1500 Loss 2.1980 Accuracy 0.5186\n",
      "Epoch 9 Batch 1550 Loss 2.1978 Accuracy 0.5186\n",
      "Epoch 9 Batch 1600 Loss 2.1980 Accuracy 0.5186\n",
      "Epoch 9 Batch 1650 Loss 2.1983 Accuracy 0.5186\n",
      "Epoch 9 Batch 1700 Loss 2.1988 Accuracy 0.5186\n",
      "Epoch 9 Batch 1750 Loss 2.1993 Accuracy 0.5185\n",
      "Epoch 9 Batch 1800 Loss 2.2000 Accuracy 0.5184\n",
      "Epoch 9 Batch 1850 Loss 2.2004 Accuracy 0.5184\n",
      "Epoch 9 Batch 1900 Loss 2.2003 Accuracy 0.5184\n",
      "Epoch 9 Batch 1950 Loss 2.2004 Accuracy 0.5184\n",
      "Epoch 9 Batch 2000 Loss 2.2009 Accuracy 0.5184\n",
      "Epoch 9 Batch 2050 Loss 2.2014 Accuracy 0.5183\n",
      "Epoch 9 Batch 2100 Loss 2.2017 Accuracy 0.5183\n",
      "Epoch 9 Batch 2150 Loss 2.2017 Accuracy 0.5183\n",
      "Epoch 9 Batch 2200 Loss 2.2022 Accuracy 0.5182\n",
      "Epoch 9 Batch 2250 Loss 2.2026 Accuracy 0.5182\n",
      "Epoch 9 Batch 2300 Loss 2.2029 Accuracy 0.5181\n",
      "Epoch 9 Batch 2350 Loss 2.2028 Accuracy 0.5181\n",
      "Epoch 9 Batch 2400 Loss 2.2025 Accuracy 0.5182\n",
      "Epoch 9 Batch 2450 Loss 2.2026 Accuracy 0.5182\n",
      "Epoch 9 Batch 2500 Loss 2.2027 Accuracy 0.5182\n",
      "Epoch 9 Batch 2550 Loss 2.2031 Accuracy 0.5182\n",
      "Epoch 9 Batch 2600 Loss 2.2035 Accuracy 0.5181\n",
      "Epoch 9 Loss 2.2039 Accuracy 0.5181\n",
      "Time taken for 1 epoch: 453.77437114715576 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 2.1536 Accuracy 0.5372\n",
      "Epoch 10 Batch 50 Loss 2.1875 Accuracy 0.5215\n",
      "Epoch 10 Batch 100 Loss 2.1919 Accuracy 0.5199\n",
      "Epoch 10 Batch 150 Loss 2.1883 Accuracy 0.5197\n",
      "Epoch 10 Batch 200 Loss 2.1865 Accuracy 0.5195\n",
      "Epoch 10 Batch 250 Loss 2.1853 Accuracy 0.5197\n",
      "Epoch 10 Batch 300 Loss 2.1892 Accuracy 0.5195\n",
      "Epoch 10 Batch 350 Loss 2.1868 Accuracy 0.5201\n",
      "Epoch 10 Batch 400 Loss 2.1870 Accuracy 0.5201\n",
      "Epoch 10 Batch 450 Loss 2.1874 Accuracy 0.5201\n",
      "Epoch 10 Batch 500 Loss 2.1876 Accuracy 0.5203\n",
      "Epoch 10 Batch 550 Loss 2.1899 Accuracy 0.5202\n",
      "Epoch 10 Batch 600 Loss 2.1896 Accuracy 0.5202\n",
      "Epoch 10 Batch 650 Loss 2.1901 Accuracy 0.5202\n",
      "Epoch 10 Batch 700 Loss 2.1908 Accuracy 0.5200\n",
      "Epoch 10 Batch 750 Loss 2.1912 Accuracy 0.5197\n",
      "Epoch 10 Batch 800 Loss 2.1914 Accuracy 0.5198\n",
      "Epoch 10 Batch 850 Loss 2.1916 Accuracy 0.5198\n",
      "Epoch 10 Batch 900 Loss 2.1924 Accuracy 0.5197\n",
      "Epoch 10 Batch 950 Loss 2.1927 Accuracy 0.5197\n",
      "Epoch 10 Batch 1000 Loss 2.1933 Accuracy 0.5196\n",
      "Epoch 10 Batch 1050 Loss 2.1940 Accuracy 0.5195\n",
      "Epoch 10 Batch 1100 Loss 2.1943 Accuracy 0.5194\n",
      "Epoch 10 Batch 1150 Loss 2.1940 Accuracy 0.5195\n",
      "Epoch 10 Batch 1200 Loss 2.1949 Accuracy 0.5195\n",
      "Epoch 10 Batch 1250 Loss 2.1954 Accuracy 0.5193\n",
      "Epoch 10 Batch 1300 Loss 2.1955 Accuracy 0.5192\n",
      "Epoch 10 Batch 1350 Loss 2.1951 Accuracy 0.5194\n",
      "Epoch 10 Batch 1400 Loss 2.1955 Accuracy 0.5193\n",
      "Epoch 10 Batch 1450 Loss 2.1960 Accuracy 0.5192\n",
      "Epoch 10 Batch 1500 Loss 2.1962 Accuracy 0.5192\n",
      "Epoch 10 Batch 1550 Loss 2.1968 Accuracy 0.5191\n",
      "Epoch 10 Batch 1600 Loss 2.1967 Accuracy 0.5192\n",
      "Epoch 10 Batch 1650 Loss 2.1964 Accuracy 0.5193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 1700 Loss 2.1962 Accuracy 0.5193\n",
      "Epoch 10 Batch 1750 Loss 2.1966 Accuracy 0.5193\n",
      "Epoch 10 Batch 1800 Loss 2.1969 Accuracy 0.5192\n",
      "Epoch 10 Batch 1850 Loss 2.1972 Accuracy 0.5192\n",
      "Epoch 10 Batch 1900 Loss 2.1976 Accuracy 0.5191\n",
      "Epoch 10 Batch 1950 Loss 2.1980 Accuracy 0.5191\n",
      "Epoch 10 Batch 2000 Loss 2.1980 Accuracy 0.5190\n",
      "Epoch 10 Batch 2050 Loss 2.1980 Accuracy 0.5190\n",
      "Epoch 10 Batch 2100 Loss 2.1986 Accuracy 0.5189\n",
      "Epoch 10 Batch 2150 Loss 2.1988 Accuracy 0.5189\n",
      "Epoch 10 Batch 2200 Loss 2.1986 Accuracy 0.5190\n",
      "Epoch 10 Batch 2250 Loss 2.1988 Accuracy 0.5189\n",
      "Epoch 10 Batch 2300 Loss 2.1987 Accuracy 0.5190\n",
      "Epoch 10 Batch 2350 Loss 2.1984 Accuracy 0.5191\n",
      "Epoch 10 Batch 2400 Loss 2.1989 Accuracy 0.5190\n",
      "Epoch 10 Batch 2450 Loss 2.1995 Accuracy 0.5188\n",
      "Epoch 10 Batch 2500 Loss 2.2000 Accuracy 0.5188\n",
      "Epoch 10 Batch 2550 Loss 2.2004 Accuracy 0.5187\n",
      "Epoch 10 Batch 2600 Loss 2.2006 Accuracy 0.5187\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train_full/ckpt-20\n",
      "Epoch 10 Loss 2.2010 Accuracy 0.5187\n",
      "Time taken for 1 epoch: 454.5662429332733 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "          print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "    history.append(train_loss.result())\n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так заманчиво падает но очень медленно на самом деле... Окей. 100 эпох думаю хватит пока )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инференс - тут как в генерации текста. Отличия в том, что мы вычисляем метки начала и конца для разных языков\n",
    "# И маски, padding и look-ahead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_ru.vocab_size]\n",
    "    end_token = [tokenizer_ru.vocab_size + 1]\n",
    "\n",
    "    # inp sentence is russian, hence adding the start and end token\n",
    "    inp_sentence = start_token + tokenizer_ru.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer_en.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Осенним вечером шёл дождь!\n",
      "Predicted translation: winter night .\n"
     ]
    }
   ],
   "source": [
    "translate('Осенним вечером шёл дождь!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Началось веселье! )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Я должен идти домой\n",
      "Predicted translation: you have to stay home .\n"
     ]
    }
   ],
   "source": [
    "translate('Я должен идти домой')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Мама мыла раму\n",
      "Predicted translation: it 's a natural free-living organism .\n"
     ]
    }
   ],
   "source": [
    "translate('Мама мыла раму')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Это невозможно\n",
      "Predicted translation: it does n't have to be any other way to evolve .\n"
     ]
    }
   ],
   "source": [
    "translate('Это невозможно')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: С причала рыбачил апостол Андрей\n",
      "Predicted translation: it was a freak of candles , and it was a death sentence .\n"
     ]
    }
   ],
   "source": [
    "translate('С причала рыбачил апостол Андрей')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если принять во внимание текст песни то кажется упоминание \"смертного приговора\" в переводе не так уж и абсурдно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Очень скоро здесь станет совсем темно\n",
      "Predicted translation: the reality is almost a little bit long in the dark .\n"
     ]
    }
   ],
   "source": [
    "translate('Очень скоро здесь станет совсем темно')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: с оружием в руках\n",
      "Predicted translation: it was a death penalty .\n"
     ]
    }
   ],
   "source": [
    "translate('с оружием в руках')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: перемены возможны только с оружием в руках\n",
      "Predicted translation: we can do that with a lot of other people .\n"
     ]
    }
   ],
   "source": [
    "translate('перемены возможны только с оружием в руках')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Даже примеры из собственного трейн датасета он перевести не может никак (\n",
    "# Seq2seq c вниманием был как-то более вменяем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
