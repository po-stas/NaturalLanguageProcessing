{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ДЗ №11. Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень популярная статеечка http://jalammar.github.io/illustrated-transformer/\n",
    "Просто по какой-то причине читать методичку (даже на русском языке) очень трудно. Даже код не всегда спасает. Такое чувство что это автоматический перевод (вот ведь ирония) какой-то оригинальной работы..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/ru_to_en', with_info=True,\n",
    "                               as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = iter(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = i.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "к : успех , перемены возможны только с оружием в руках .\n"
     ]
    }
   ],
   "source": [
    "tf.print(example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c : success , the change is only coming through the barrel of the gun .\n"
     ]
    }
   ],
   "source": [
    "tf.print(example[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "к : успех , перемены возможны только с оружием в руках .\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_static_value(example[0]).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сабворд токенизатор\n",
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for ru, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_ru = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (ru.numpy() for ru, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8073, 139, 13, 54, 7, 2298]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_string = tokenizer_en.encode('This is just a test')\n",
    "tokenized_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is just a test'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.decode(tokenized_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token=8073 ===> \"T\"\n",
      "token=139 ===> \"his \"\n",
      "token=13 ===> \"is \"\n",
      "token=54 ===> \"just \"\n",
      "token=7 ===> \"a \"\n",
      "token=2298 ===> \"test\"\n"
     ]
    }
   ],
   "source": [
    "for token in tokenized_string:\n",
    "    print(f'{token=} ===> \"{tokenizer_en.decode([token])}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция кодирования (с добавлением метки начала и конца)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(ru, en):\n",
    "    ru = [tokenizer_ru.vocab_size] + tokenizer_ru.encode(\n",
    "      ru.numpy()) + [tokenizer_ru.vocab_size+1]\n",
    "\n",
    "    en = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      en.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "\n",
    "    return ru, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(ru, en):\n",
    "    result_ru, result_en = tf.py_function(encode, [ru, en], [tf.int64, tf.int64])\n",
    "    result_ru.set_shape([None])\n",
    "    result_en.set_shape([None])\n",
    "\n",
    "    return result_ru, result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length, tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 38), dtype=int64, numpy=\n",
       " array([[8179,   57,   86, ...,    0,    0,    0],\n",
       "        [8179,    3,   38, ...,    0,    0,    0],\n",
       "        [8179,   57,  135, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8179,    3,    7, ...,    0,    0,    0],\n",
       "        [8179,  138,  250, ...,    0,    0,    0],\n",
       "        [8179,   19,    7, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8245,   90,  101, ...,    0,    0,    0],\n",
       "        [8245,   70,   25, ...,    0,    0,    0],\n",
       "        [8245,   90,  153, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8245,    4,   18, ...,    0,    0,    0],\n",
       "        [8245,   19,   59, ...,    0,    0,    0],\n",
       "        [8245,   24,   18, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_batch, en_batch = next(iter(val_dataset))\n",
    "ru_batch, en_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(38,), dtype=int64, numpy=\n",
       "array([8179,   57,   86,   54,  578,   84, 4550, 1003, 5905, 6326, 1197,\n",
       "          2, 8180,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoder - готовим матрицу векторов (для всех позиций в предложении не более длины max_len)\n",
    "# Потом будем их (соответствующий позиции вектор) добавлять к эмбеддингу каждого слова, \n",
    "# как кодированную позицию слова в предложении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $\n",
    "\n",
    "$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 2, 4, 4, 6, 6, 8, 8]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.arange(10)[np.newaxis, :]//2) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 0.96466162, 0.96466162, 0.93057204,\n",
       "        0.93057204, 0.89768713, 0.89768713, 0.86596432, 0.86596432,\n",
       "        0.83536255, 0.83536255, 0.80584219, 0.80584219, 0.77736503,\n",
       "        0.77736503, 0.74989421, 0.74989421, 0.72339416, 0.72339416,\n",
       "        0.69783058, 0.69783058, 0.67317038, 0.67317038, 0.64938163,\n",
       "        0.64938163, 0.62643354, 0.62643354, 0.60429639, 0.60429639,\n",
       "        0.58294153, 0.58294153, 0.56234133, 0.56234133, 0.54246909,\n",
       "        0.54246909, 0.52329911, 0.52329911, 0.50480657, 0.50480657,\n",
       "        0.48696753, 0.48696753, 0.46975888, 0.46975888, 0.45315836,\n",
       "        0.45315836, 0.43714448, 0.43714448, 0.4216965 , 0.4216965 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / np.power(10000, (2 * (np.arange(50)[np.newaxis, :]//2)) / np.float32(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [1.        , 1.        , 0.96466162, 0.96466162, 0.93057204],\n",
       "       [2.        , 2.        , 1.92932324, 1.92932324, 1.86114408],\n",
       "       [3.        , 3.        , 2.89398486, 2.89398486, 2.79171612],\n",
       "       [4.        , 4.        , 3.85864648, 3.85864648, 3.72228816]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# То, от чего берется синус (для четных элементов) и косинус для нечетных\n",
    "rates = (1 / np.power(10000, (2 * (np.arange(5)[np.newaxis, :]//2))\\\n",
    "                      / np.float32(512))) * np.arange(5)[:, np.newaxis]\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 2, 4, 4, 6, 6, 8, 8]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (np.arange(10)[np.newaxis, :]//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 512), dtype=float32, numpy=\n",
       "array([[[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
       "          1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
       "        [ 8.4147096e-01,  5.4030228e-01,  8.2185620e-01, ...,\n",
       "          1.0000000e+00,  1.0366329e-04,  1.0000000e+00],\n",
       "        [ 9.0929741e-01, -4.1614684e-01,  9.3641472e-01, ...,\n",
       "          1.0000000e+00,  2.0732658e-04,  1.0000000e+00],\n",
       "        [ 1.4112000e-01, -9.8999250e-01,  2.4508542e-01, ...,\n",
       "          9.9999994e-01,  3.1098988e-04,  9.9999994e-01],\n",
       "        [-7.5680250e-01, -6.5364361e-01, -6.5716684e-01, ...,\n",
       "          9.9999988e-01,  4.1465316e-04,  9.9999994e-01]]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoding(5, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция маскирующая паддинговые нули... не супер понятно для чего там эти два доп. измерения\n",
    "# Написано для логитов внимания - вопрос - почему два....\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Маска закрывающая еще не предсказанную часть фразы. Чтобы самовнимание ее не использовало\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled dot product attention\n",
    "# Функция вычисления весов самовнимания. В нее подаются по три вектора Q, K, V производные из эмбеддингов слов.\n",
    "# Для каждого слова берется Q и скалярно перемножается с K векторами всех слов - полученные веса скейлятся \n",
    "# на корень из длины вектора, от них берется софтмакс и на них множатся V вектора. \n",
    "# Все соскейленные V вектора суммируются в итоговый вектор self attention (output)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  # Непонятно, почему так? Зачем отнимать огромное значение, почему не умножить на что нибудь маленькое\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = tf.constant([[0.0, 1.0, 2.0, 3.0]])\n",
    "k = tf.constant([[1.0, 0.0, 1.0, 0.0]])\n",
    "v = tf.constant([[5.0, 5.0, 5.0, 5.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(q, k, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[5., 5., 5., 5.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.constant([[1]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[5., 5., 5., 5.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       " array([[ 4.244919,  4.244919, 16.32622 ,  4.244919],\n",
       "        [ 4.      ,  4.      , 20.      ,  4.      ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[0.62245935, 0.37754068],\n",
       "        [0.5       , 0.5       ]], dtype=float32)>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = tf.constant([[0.0, 1.0, 2.0, 3.0], [1.0, 2.0, 3.0, 5.0]])\n",
    "k = tf.constant([[1.0, 0.0, 1.0, 0.0], [2.0, 1.0, 0.0, 0.0]])\n",
    "v = tf.constant([[5.0, 5.0, 5.0, 5.0], [3.0, 3.0, 35.0, 3.0]])\n",
    "scaled_dot_product_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Многоголовость внимания\n",
    "# Слоев внимания несколько. Для каждого слоя внимания мы прогоняем эмбеддинги через три dense слоя wq, wk, wv\n",
    "# Таким образом получая Q K V вектора, которые разбираем на части с помощью reshape (сколько голов внимания - столько частей)\n",
    "# Для всех вычисляется самовнимание и мы собираем матрицы векторов V из каждого \"внимания\" и конкатенейтим их (также через reshape). \n",
    "# И прогоняем через выходной FC слой. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Полносвязная сеточка (будет использоваться в конце енкодера)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Энкодер в трансформере состоит из нескольких повторяющихся слоёв. Вот один слой энкодера\n",
    "# В нем Multi-Head-Attention блок, выходная полносвязная сеть, и блоки нормализации и дропаут..\n",
    "# Сначала данные попадают в MHA, затем дропаут, затем нормалайз, затем в полносвязную сеть и снова дропаут и нормалайз\n",
    "# Кроме того есть два скип-соединения, один обходит MHA с дропаутом, \n",
    "# второй полносвязную сеть с дропаутом, чтобы при большой глубине не затухали градиенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собственно сам энкодер\n",
    "# Берет исходные фразы, делает для них эмбеддинги, скейлит их на корень из размерности эмбеддинга, добавляет позишн\n",
    "# И прогоняет их через какое-то количество энкодер-слоев (рассмотренных выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "    \n",
    "    \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Декодер. Так-же как и энкодер состоит из повторяющихся слоёв. \n",
    "# Каждый слой берет входные данные (эмбеддинги фразы или то, что пришло из предыдущего слоя декодера)\n",
    "# Вычисляет много-слойное само-внимание (с ограничением по маске). Маска в данном случае не дает вниманию\n",
    "# Вычислять скор с токенами которые находятся правее предсказываемого в данный момент. (поскольку декодер \n",
    "# генерирует результат токен за токеном - маска сначала закрывает все токены, потом постепенно сдвигаясь вправо\n",
    "# открывает ту часть которая уже была обработана)\n",
    "# Далее следует дропаут и слой нормализации со скип-соединением снизу.\n",
    "# После этого вычисляется скор еще одного самовнимания, на этот раз в нем используются Q и K вектора из Энкодера.\n",
    "# А V вектор используется от первого внимания (после дропаута и нормализации)\n",
    "# Дальше снова дропаут и нормалайз со скипом. \n",
    "# Далее полносвязная сеть и еще один проход дропаут и нормалайз. \n",
    "# И результат (V вектора, и веса первого и второго внимания) выдаются наружу для передачи в следующий слой декодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собственно сам Декодер\n",
    "# Все уже было расписано выше в основном.. Берем входные данные, эмбеддим их, скейлим корнем из глубины,\n",
    "# Добавляем позишн энкодинг, дальше дропаут и прогоняем через пачку слоев декодера (зачем-то сохраняем веса внимания)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Весь Трансформер.. Энкодер, декодер и финальный полносвязный слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        \n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры... Урезаны для того чтобы это не исполнялось вечность... \n",
    "# В оригинальном трансформере эмбеддинг 512, слоев энкодере и декодере по 6, размерность полносвязных слоев 2048\n",
    "# Здесь - 128, 4, 512.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_ru.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимайзер с variable learning rate. Формула из пэйпера [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лосс функция. И метрика. Поскольку секвенции имеют паддинг - нужно исключать его из вычислений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем по-тренировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Процесс тренировки - в энкодер подаются исходные фразы, \n",
    "# В декодер подаются таргетный фразы. Таргет подается в двух видах, снизу в исходном, сверху в сдвинутом на -1, \n",
    "# Поскольку предсказание энкодера (и процесс тренировки тоже) происходит в пошаговом режиме (предсказывается\n",
    "# по одному токену за шаг, а следующее предсказание строится на результатах предыдущего)\n",
    "# Сдвиг таргета делается чтобы таргетом для текущего слова было следующее - как при тренировке генерации текста...\n",
    "# Используется forced supervised learning, то-есть независимо от того, что предсказал на предыдущем шаге декодер\n",
    "# Следующий таргет все равно выдается по плану. (не отступает от эталонной таргетной фразы)\n",
    "# Чтобы самовнимание не учитывало часть как-бы \"еще не предсказанной\" фразу - используется look-ahead-mask\n",
    "# Это по-сути очевидно только на этапе тренировки, поскольку на инференсе - эта часть фразы и так неизвестна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 9.0751 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 9.0059 Accuracy 0.0088\n",
      "Epoch 1 Batch 100 Loss 8.9039 Accuracy 0.0312\n",
      "Epoch 1 Batch 150 Loss 8.7996 Accuracy 0.0387\n",
      "Epoch 1 Batch 200 Loss 8.6737 Accuracy 0.0424\n",
      "Epoch 1 Batch 250 Loss 8.5207 Accuracy 0.0448\n",
      "Epoch 1 Batch 300 Loss 8.3428 Accuracy 0.0482\n",
      "Epoch 1 Batch 350 Loss 8.1555 Accuracy 0.0552\n",
      "Epoch 1 Batch 400 Loss 7.9712 Accuracy 0.0608\n",
      "Epoch 1 Batch 450 Loss 7.8041 Accuracy 0.0660\n",
      "Epoch 1 Batch 500 Loss 7.6553 Accuracy 0.0727\n",
      "Epoch 1 Batch 550 Loss 7.5204 Accuracy 0.0808\n",
      "Epoch 1 Batch 600 Loss 7.3923 Accuracy 0.0889\n",
      "Epoch 1 Batch 650 Loss 7.2713 Accuracy 0.0966\n",
      "Epoch 1 Batch 700 Loss 7.1575 Accuracy 0.1042\n",
      "Epoch 1 Batch 750 Loss 7.0480 Accuracy 0.1115\n",
      "Epoch 1 Batch 800 Loss 6.9449 Accuracy 0.1186\n",
      "Epoch 1 Batch 850 Loss 6.8471 Accuracy 0.1255\n",
      "Epoch 1 Batch 900 Loss 6.7555 Accuracy 0.1319\n",
      "Epoch 1 Batch 950 Loss 6.6697 Accuracy 0.1379\n",
      "Epoch 1 Batch 1000 Loss 6.5891 Accuracy 0.1436\n",
      "Epoch 1 Batch 1050 Loss 6.5139 Accuracy 0.1489\n",
      "Epoch 1 Batch 1100 Loss 6.4439 Accuracy 0.1539\n",
      "Epoch 1 Batch 1150 Loss 6.3783 Accuracy 0.1588\n",
      "Epoch 1 Batch 1200 Loss 6.3166 Accuracy 0.1633\n",
      "Epoch 1 Batch 1250 Loss 6.2577 Accuracy 0.1678\n",
      "Epoch 1 Batch 1300 Loss 6.2008 Accuracy 0.1721\n",
      "Epoch 1 Batch 1350 Loss 6.1490 Accuracy 0.1761\n",
      "Epoch 1 Batch 1400 Loss 6.1001 Accuracy 0.1798\n",
      "Epoch 1 Batch 1450 Loss 6.0526 Accuracy 0.1835\n",
      "Epoch 1 Batch 1500 Loss 6.0071 Accuracy 0.1871\n",
      "Epoch 1 Batch 1550 Loss 5.9642 Accuracy 0.1904\n",
      "Epoch 1 Batch 1600 Loss 5.9239 Accuracy 0.1935\n",
      "Epoch 1 Batch 1650 Loss 5.8838 Accuracy 0.1966\n",
      "Epoch 1 Batch 1700 Loss 5.8464 Accuracy 0.1995\n",
      "Epoch 1 Batch 1750 Loss 5.8108 Accuracy 0.2022\n",
      "Epoch 1 Batch 1800 Loss 5.7761 Accuracy 0.2049\n",
      "Epoch 1 Batch 1850 Loss 5.7422 Accuracy 0.2076\n",
      "Epoch 1 Batch 1900 Loss 5.7103 Accuracy 0.2100\n",
      "Epoch 1 Batch 1950 Loss 5.6791 Accuracy 0.2125\n",
      "Epoch 1 Batch 2000 Loss 5.6497 Accuracy 0.2148\n",
      "Epoch 1 Batch 2050 Loss 5.6203 Accuracy 0.2171\n",
      "Epoch 1 Batch 2100 Loss 5.5925 Accuracy 0.2192\n",
      "Epoch 1 Batch 2150 Loss 5.5659 Accuracy 0.2213\n",
      "Epoch 1 Batch 2200 Loss 5.5393 Accuracy 0.2234\n",
      "Epoch 1 Batch 2250 Loss 5.5140 Accuracy 0.2254\n",
      "Epoch 1 Batch 2300 Loss 5.4903 Accuracy 0.2272\n",
      "Epoch 1 Batch 2350 Loss 5.4663 Accuracy 0.2291\n",
      "Epoch 1 Batch 2400 Loss 5.4434 Accuracy 0.2309\n",
      "Epoch 1 Batch 2450 Loss 5.4214 Accuracy 0.2325\n",
      "Epoch 1 Batch 2500 Loss 5.3997 Accuracy 0.2342\n",
      "Epoch 1 Batch 2550 Loss 5.3784 Accuracy 0.2359\n",
      "Epoch 1 Batch 2600 Loss 5.3576 Accuracy 0.2375\n",
      "Epoch 1 Loss 5.3477 Accuracy 0.2383\n",
      "Time taken for 1 epoch: 186.29582452774048 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 4.3560 Accuracy 0.3102\n",
      "Epoch 2 Batch 50 Loss 4.2714 Accuracy 0.3218\n",
      "Epoch 2 Batch 100 Loss 4.2651 Accuracy 0.3220\n",
      "Epoch 2 Batch 150 Loss 4.2513 Accuracy 0.3248\n",
      "Epoch 2 Batch 200 Loss 4.2377 Accuracy 0.3266\n",
      "Epoch 2 Batch 250 Loss 4.2287 Accuracy 0.3272\n",
      "Epoch 2 Batch 300 Loss 4.2212 Accuracy 0.3280\n",
      "Epoch 2 Batch 350 Loss 4.2152 Accuracy 0.3288\n",
      "Epoch 2 Batch 400 Loss 4.2069 Accuracy 0.3295\n",
      "Epoch 2 Batch 450 Loss 4.1973 Accuracy 0.3306\n",
      "Epoch 2 Batch 500 Loss 4.1893 Accuracy 0.3313\n",
      "Epoch 2 Batch 550 Loss 4.1818 Accuracy 0.3319\n",
      "Epoch 2 Batch 600 Loss 4.1743 Accuracy 0.3324\n",
      "Epoch 2 Batch 650 Loss 4.1666 Accuracy 0.3330\n",
      "Epoch 2 Batch 700 Loss 4.1610 Accuracy 0.3336\n",
      "Epoch 2 Batch 750 Loss 4.1525 Accuracy 0.3346\n",
      "Epoch 2 Batch 800 Loss 4.1458 Accuracy 0.3352\n",
      "Epoch 2 Batch 850 Loss 4.1363 Accuracy 0.3361\n",
      "Epoch 2 Batch 900 Loss 4.1284 Accuracy 0.3368\n",
      "Epoch 2 Batch 950 Loss 4.1199 Accuracy 0.3376\n",
      "Epoch 2 Batch 1000 Loss 4.1118 Accuracy 0.3385\n",
      "Epoch 2 Batch 1050 Loss 4.1042 Accuracy 0.3392\n",
      "Epoch 2 Batch 1100 Loss 4.0972 Accuracy 0.3399\n",
      "Epoch 2 Batch 1150 Loss 4.0886 Accuracy 0.3408\n",
      "Epoch 2 Batch 1200 Loss 4.0804 Accuracy 0.3417\n",
      "Epoch 2 Batch 1250 Loss 4.0713 Accuracy 0.3427\n",
      "Epoch 2 Batch 1300 Loss 4.0631 Accuracy 0.3435\n",
      "Epoch 2 Batch 1350 Loss 4.0563 Accuracy 0.3441\n",
      "Epoch 2 Batch 1400 Loss 4.0484 Accuracy 0.3450\n",
      "Epoch 2 Batch 1450 Loss 4.0407 Accuracy 0.3458\n",
      "Epoch 2 Batch 1500 Loss 4.0331 Accuracy 0.3467\n",
      "Epoch 2 Batch 1550 Loss 4.0253 Accuracy 0.3474\n",
      "Epoch 2 Batch 1600 Loss 4.0170 Accuracy 0.3483\n",
      "Epoch 2 Batch 1650 Loss 4.0082 Accuracy 0.3494\n",
      "Epoch 2 Batch 1700 Loss 4.0013 Accuracy 0.3502\n",
      "Epoch 2 Batch 1750 Loss 3.9937 Accuracy 0.3510\n",
      "Epoch 2 Batch 1800 Loss 3.9860 Accuracy 0.3519\n",
      "Epoch 2 Batch 1850 Loss 3.9785 Accuracy 0.3527\n",
      "Epoch 2 Batch 1900 Loss 3.9711 Accuracy 0.3536\n",
      "Epoch 2 Batch 1950 Loss 3.9634 Accuracy 0.3544\n",
      "Epoch 2 Batch 2000 Loss 3.9560 Accuracy 0.3552\n",
      "Epoch 2 Batch 2050 Loss 3.9486 Accuracy 0.3560\n",
      "Epoch 2 Batch 2100 Loss 3.9412 Accuracy 0.3567\n",
      "Epoch 2 Batch 2150 Loss 3.9339 Accuracy 0.3575\n",
      "Epoch 2 Batch 2200 Loss 3.9261 Accuracy 0.3584\n",
      "Epoch 2 Batch 2250 Loss 3.9186 Accuracy 0.3592\n",
      "Epoch 2 Batch 2300 Loss 3.9115 Accuracy 0.3599\n",
      "Epoch 2 Batch 2350 Loss 3.9036 Accuracy 0.3608\n",
      "Epoch 2 Batch 2400 Loss 3.8958 Accuracy 0.3616\n",
      "Epoch 2 Batch 2450 Loss 3.8882 Accuracy 0.3624\n",
      "Epoch 2 Batch 2500 Loss 3.8804 Accuracy 0.3633\n",
      "Epoch 2 Batch 2550 Loss 3.8733 Accuracy 0.3640\n",
      "Epoch 2 Batch 2600 Loss 3.8655 Accuracy 0.3649\n",
      "Epoch 2 Loss 3.8625 Accuracy 0.3653\n",
      "Time taken for 1 epoch: 165.93331027030945 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.3823 Accuracy 0.4305\n",
      "Epoch 3 Batch 50 Loss 3.4292 Accuracy 0.4110\n",
      "Epoch 3 Batch 100 Loss 3.4311 Accuracy 0.4120\n",
      "Epoch 3 Batch 150 Loss 3.4343 Accuracy 0.4122\n",
      "Epoch 3 Batch 200 Loss 3.4272 Accuracy 0.4125\n",
      "Epoch 3 Batch 250 Loss 3.4298 Accuracy 0.4117\n",
      "Epoch 3 Batch 300 Loss 3.4252 Accuracy 0.4125\n",
      "Epoch 3 Batch 350 Loss 3.4201 Accuracy 0.4127\n",
      "Epoch 3 Batch 400 Loss 3.4133 Accuracy 0.4137\n",
      "Epoch 3 Batch 450 Loss 3.4098 Accuracy 0.4141\n",
      "Epoch 3 Batch 500 Loss 3.4054 Accuracy 0.4146\n",
      "Epoch 3 Batch 550 Loss 3.3996 Accuracy 0.4154\n",
      "Epoch 3 Batch 600 Loss 3.3944 Accuracy 0.4160\n",
      "Epoch 3 Batch 650 Loss 3.3879 Accuracy 0.4168\n",
      "Epoch 3 Batch 700 Loss 3.3840 Accuracy 0.4172\n",
      "Epoch 3 Batch 750 Loss 3.3786 Accuracy 0.4179\n",
      "Epoch 3 Batch 800 Loss 3.3734 Accuracy 0.4184\n",
      "Epoch 3 Batch 850 Loss 3.3672 Accuracy 0.4191\n",
      "Epoch 3 Batch 900 Loss 3.3635 Accuracy 0.4195\n",
      "Epoch 3 Batch 950 Loss 3.3585 Accuracy 0.4201\n",
      "Epoch 3 Batch 1000 Loss 3.3527 Accuracy 0.4207\n",
      "Epoch 3 Batch 1050 Loss 3.3461 Accuracy 0.4214\n",
      "Epoch 3 Batch 1100 Loss 3.3419 Accuracy 0.4218\n",
      "Epoch 3 Batch 1150 Loss 3.3363 Accuracy 0.4225\n",
      "Epoch 3 Batch 1200 Loss 3.3313 Accuracy 0.4231\n",
      "Epoch 3 Batch 1250 Loss 3.3267 Accuracy 0.4237\n",
      "Epoch 3 Batch 1300 Loss 3.3220 Accuracy 0.4242\n",
      "Epoch 3 Batch 1350 Loss 3.3158 Accuracy 0.4248\n",
      "Epoch 3 Batch 1400 Loss 3.3103 Accuracy 0.4254\n",
      "Epoch 3 Batch 1450 Loss 3.3057 Accuracy 0.4259\n",
      "Epoch 3 Batch 1500 Loss 3.3005 Accuracy 0.4265\n",
      "Epoch 3 Batch 1550 Loss 3.2956 Accuracy 0.4272\n",
      "Epoch 3 Batch 1600 Loss 3.2902 Accuracy 0.4278\n",
      "Epoch 3 Batch 1650 Loss 3.2861 Accuracy 0.4284\n",
      "Epoch 3 Batch 1700 Loss 3.2819 Accuracy 0.4289\n",
      "Epoch 3 Batch 1750 Loss 3.2766 Accuracy 0.4296\n",
      "Epoch 3 Batch 1800 Loss 3.2714 Accuracy 0.4302\n",
      "Epoch 3 Batch 1850 Loss 3.2672 Accuracy 0.4306\n",
      "Epoch 3 Batch 1900 Loss 3.2623 Accuracy 0.4312\n",
      "Epoch 3 Batch 1950 Loss 3.2580 Accuracy 0.4316\n",
      "Epoch 3 Batch 2000 Loss 3.2533 Accuracy 0.4321\n",
      "Epoch 3 Batch 2050 Loss 3.2487 Accuracy 0.4327\n",
      "Epoch 3 Batch 2100 Loss 3.2448 Accuracy 0.4332\n",
      "Epoch 3 Batch 2150 Loss 3.2409 Accuracy 0.4336\n",
      "Epoch 3 Batch 2200 Loss 3.2365 Accuracy 0.4340\n",
      "Epoch 3 Batch 2250 Loss 3.2332 Accuracy 0.4344\n",
      "Epoch 3 Batch 2300 Loss 3.2291 Accuracy 0.4348\n",
      "Epoch 3 Batch 2350 Loss 3.2250 Accuracy 0.4353\n",
      "Epoch 3 Batch 2400 Loss 3.2211 Accuracy 0.4357\n",
      "Epoch 3 Batch 2450 Loss 3.2165 Accuracy 0.4362\n",
      "Epoch 3 Batch 2500 Loss 3.2120 Accuracy 0.4368\n",
      "Epoch 3 Batch 2550 Loss 3.2078 Accuracy 0.4372\n",
      "Epoch 3 Batch 2600 Loss 3.2045 Accuracy 0.4376\n",
      "Epoch 3 Loss 3.2024 Accuracy 0.4378\n",
      "Time taken for 1 epoch: 165.94499802589417 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 3.1466 Accuracy 0.4292\n",
      "Epoch 4 Batch 50 Loss 2.9998 Accuracy 0.4586\n",
      "Epoch 4 Batch 100 Loss 2.9826 Accuracy 0.4615\n",
      "Epoch 4 Batch 150 Loss 2.9720 Accuracy 0.4629\n",
      "Epoch 4 Batch 200 Loss 2.9676 Accuracy 0.4640\n",
      "Epoch 4 Batch 250 Loss 2.9569 Accuracy 0.4651\n",
      "Epoch 4 Batch 300 Loss 2.9572 Accuracy 0.4649\n",
      "Epoch 4 Batch 350 Loss 2.9492 Accuracy 0.4662\n",
      "Epoch 4 Batch 400 Loss 2.9485 Accuracy 0.4659\n",
      "Epoch 4 Batch 450 Loss 2.9444 Accuracy 0.4664\n",
      "Epoch 4 Batch 500 Loss 2.9407 Accuracy 0.4669\n",
      "Epoch 4 Batch 550 Loss 2.9377 Accuracy 0.4675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 600 Loss 2.9369 Accuracy 0.4675\n",
      "Epoch 4 Batch 650 Loss 2.9351 Accuracy 0.4677\n",
      "Epoch 4 Batch 700 Loss 2.9299 Accuracy 0.4683\n",
      "Epoch 4 Batch 750 Loss 2.9286 Accuracy 0.4684\n",
      "Epoch 4 Batch 800 Loss 2.9239 Accuracy 0.4691\n",
      "Epoch 4 Batch 850 Loss 2.9215 Accuracy 0.4692\n",
      "Epoch 4 Batch 900 Loss 2.9183 Accuracy 0.4697\n",
      "Epoch 4 Batch 950 Loss 2.9170 Accuracy 0.4699\n",
      "Epoch 4 Batch 1000 Loss 2.9136 Accuracy 0.4703\n",
      "Epoch 4 Batch 1050 Loss 2.9099 Accuracy 0.4707\n",
      "Epoch 4 Batch 1100 Loss 2.9066 Accuracy 0.4710\n",
      "Epoch 4 Batch 1150 Loss 2.9039 Accuracy 0.4713\n",
      "Epoch 4 Batch 1200 Loss 2.9025 Accuracy 0.4715\n",
      "Epoch 4 Batch 1250 Loss 2.8987 Accuracy 0.4720\n",
      "Epoch 4 Batch 1300 Loss 2.8974 Accuracy 0.4721\n",
      "Epoch 4 Batch 1350 Loss 2.8958 Accuracy 0.4724\n",
      "Epoch 4 Batch 1400 Loss 2.8928 Accuracy 0.4729\n",
      "Epoch 4 Batch 1450 Loss 2.8901 Accuracy 0.4732\n",
      "Epoch 4 Batch 1500 Loss 2.8877 Accuracy 0.4735\n",
      "Epoch 4 Batch 1550 Loss 2.8855 Accuracy 0.4738\n",
      "Epoch 4 Batch 1600 Loss 2.8835 Accuracy 0.4741\n",
      "Epoch 4 Batch 1650 Loss 2.8805 Accuracy 0.4745\n",
      "Epoch 4 Batch 1700 Loss 2.8785 Accuracy 0.4747\n",
      "Epoch 4 Batch 1750 Loss 2.8763 Accuracy 0.4750\n",
      "Epoch 4 Batch 1800 Loss 2.8736 Accuracy 0.4752\n",
      "Epoch 4 Batch 1850 Loss 2.8713 Accuracy 0.4755\n",
      "Epoch 4 Batch 1900 Loss 2.8691 Accuracy 0.4758\n",
      "Epoch 4 Batch 1950 Loss 2.8668 Accuracy 0.4760\n",
      "Epoch 4 Batch 2000 Loss 2.8644 Accuracy 0.4764\n",
      "Epoch 4 Batch 2050 Loss 2.8618 Accuracy 0.4767\n",
      "Epoch 4 Batch 2100 Loss 2.8596 Accuracy 0.4770\n",
      "Epoch 4 Batch 2150 Loss 2.8575 Accuracy 0.4771\n",
      "Epoch 4 Batch 2200 Loss 2.8563 Accuracy 0.4773\n",
      "Epoch 4 Batch 2250 Loss 2.8546 Accuracy 0.4775\n",
      "Epoch 4 Batch 2300 Loss 2.8521 Accuracy 0.4778\n",
      "Epoch 4 Batch 2350 Loss 2.8500 Accuracy 0.4780\n",
      "Epoch 4 Batch 2400 Loss 2.8482 Accuracy 0.4783\n",
      "Epoch 4 Batch 2450 Loss 2.8458 Accuracy 0.4786\n",
      "Epoch 4 Batch 2500 Loss 2.8439 Accuracy 0.4788\n",
      "Epoch 4 Batch 2550 Loss 2.8419 Accuracy 0.4790\n",
      "Epoch 4 Batch 2600 Loss 2.8398 Accuracy 0.4792\n",
      "Epoch 4 Loss 2.8390 Accuracy 0.4793\n",
      "Time taken for 1 epoch: 164.0742495059967 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.7735 Accuracy 0.4991\n",
      "Epoch 5 Batch 50 Loss 2.7116 Accuracy 0.4959\n",
      "Epoch 5 Batch 100 Loss 2.6989 Accuracy 0.4967\n",
      "Epoch 5 Batch 150 Loss 2.6984 Accuracy 0.4962\n",
      "Epoch 5 Batch 200 Loss 2.7065 Accuracy 0.4949\n",
      "Epoch 5 Batch 250 Loss 2.7048 Accuracy 0.4950\n",
      "Epoch 5 Batch 300 Loss 2.7036 Accuracy 0.4953\n",
      "Epoch 5 Batch 350 Loss 2.6989 Accuracy 0.4953\n",
      "Epoch 5 Batch 400 Loss 2.6990 Accuracy 0.4951\n",
      "Epoch 5 Batch 450 Loss 2.6980 Accuracy 0.4953\n",
      "Epoch 5 Batch 500 Loss 2.6949 Accuracy 0.4957\n",
      "Epoch 5 Batch 550 Loss 2.6940 Accuracy 0.4958\n",
      "Epoch 5 Batch 600 Loss 2.6911 Accuracy 0.4964\n",
      "Epoch 5 Batch 650 Loss 2.6902 Accuracy 0.4966\n",
      "Epoch 5 Batch 700 Loss 2.6881 Accuracy 0.4969\n",
      "Epoch 5 Batch 750 Loss 2.6867 Accuracy 0.4971\n",
      "Epoch 5 Batch 800 Loss 2.6868 Accuracy 0.4972\n",
      "Epoch 5 Batch 850 Loss 2.6846 Accuracy 0.4974\n",
      "Epoch 5 Batch 900 Loss 2.6855 Accuracy 0.4975\n",
      "Epoch 5 Batch 950 Loss 2.6830 Accuracy 0.4979\n",
      "Epoch 5 Batch 1000 Loss 2.6817 Accuracy 0.4981\n",
      "Epoch 5 Batch 1050 Loss 2.6794 Accuracy 0.4984\n",
      "Epoch 5 Batch 1100 Loss 2.6763 Accuracy 0.4988\n",
      "Epoch 5 Batch 1150 Loss 2.6760 Accuracy 0.4988\n",
      "Epoch 5 Batch 1200 Loss 2.6738 Accuracy 0.4992\n",
      "Epoch 5 Batch 1250 Loss 2.6720 Accuracy 0.4995\n",
      "Epoch 5 Batch 1300 Loss 2.6702 Accuracy 0.4997\n",
      "Epoch 5 Batch 1350 Loss 2.6690 Accuracy 0.5000\n",
      "Epoch 5 Batch 1400 Loss 2.6678 Accuracy 0.5002\n",
      "Epoch 5 Batch 1450 Loss 2.6653 Accuracy 0.5005\n",
      "Epoch 5 Batch 1500 Loss 2.6632 Accuracy 0.5008\n",
      "Epoch 5 Batch 1550 Loss 2.6615 Accuracy 0.5010\n",
      "Epoch 5 Batch 1600 Loss 2.6605 Accuracy 0.5011\n",
      "Epoch 5 Batch 1650 Loss 2.6602 Accuracy 0.5011\n",
      "Epoch 5 Batch 1700 Loss 2.6592 Accuracy 0.5012\n",
      "Epoch 5 Batch 1750 Loss 2.6582 Accuracy 0.5013\n",
      "Epoch 5 Batch 1800 Loss 2.6562 Accuracy 0.5015\n",
      "Epoch 5 Batch 1850 Loss 2.6560 Accuracy 0.5015\n",
      "Epoch 5 Batch 1900 Loss 2.6550 Accuracy 0.5016\n",
      "Epoch 5 Batch 1950 Loss 2.6537 Accuracy 0.5018\n",
      "Epoch 5 Batch 2000 Loss 2.6523 Accuracy 0.5020\n",
      "Epoch 5 Batch 2050 Loss 2.6515 Accuracy 0.5021\n",
      "Epoch 5 Batch 2100 Loss 2.6502 Accuracy 0.5022\n",
      "Epoch 5 Batch 2150 Loss 2.6488 Accuracy 0.5024\n",
      "Epoch 5 Batch 2200 Loss 2.6472 Accuracy 0.5026\n",
      "Epoch 5 Batch 2250 Loss 2.6457 Accuracy 0.5028\n",
      "Epoch 5 Batch 2300 Loss 2.6451 Accuracy 0.5029\n",
      "Epoch 5 Batch 2350 Loss 2.6439 Accuracy 0.5030\n",
      "Epoch 5 Batch 2400 Loss 2.6430 Accuracy 0.5031\n",
      "Epoch 5 Batch 2450 Loss 2.6417 Accuracy 0.5032\n",
      "Epoch 5 Batch 2500 Loss 2.6401 Accuracy 0.5034\n",
      "Epoch 5 Batch 2550 Loss 2.6390 Accuracy 0.5036\n",
      "Epoch 5 Batch 2600 Loss 2.6385 Accuracy 0.5036\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
      "Epoch 5 Loss 2.6384 Accuracy 0.5036\n",
      "Time taken for 1 epoch: 170.48337197303772 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "          print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инференс - тут как в генерации текста. Отличия в том, что мы вычисляем метки начала и конца для разных языков\n",
    "# И маски, padding и look-ahead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_ru.vocab_size]\n",
    "    end_token = [tokenizer_ru.vocab_size + 1]\n",
    "\n",
    "    # inp sentence is portuguese, hence adding the start and end token\n",
    "    inp_sentence = start_token + tokenizer_ru.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer_en.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Осенним вечером шёл дождь!\n",
      "Predicted translation: the audience was going to go rain .\n"
     ]
    }
   ],
   "source": [
    "translate('Осенним вечером шёл дождь!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Началось веселье! )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Я должен идти домой\n",
      "Predicted translation: the result is going to go home .\n"
     ]
    }
   ],
   "source": [
    "translate('Я должен идти домой')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Мама мыла раму\n",
      "Predicted translation: the case of the amazon was n't shown by us .\n"
     ]
    }
   ],
   "source": [
    "translate('Мама мыла раму')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Это невозможно\n",
      "Predicted translation: the result is impossible .\n"
     ]
    }
   ],
   "source": [
    "translate('Это невозможно')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: С причала рыбачил апостол Андрей\n",
      "Predicted translation: the point was the fishery was struck by the bottom of the leather .\n"
     ]
    }
   ],
   "source": [
    "translate('С причала рыбачил апостол Андрей')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Очень скоро здесь станет совсем темно\n",
      "Predicted translation: the infinity is going to be going to be very dark .\n"
     ]
    }
   ],
   "source": [
    "translate('Очень скоро здесь станет совсем темно')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
