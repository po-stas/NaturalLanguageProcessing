{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/postas/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pyconll\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "from gensim.models import Word2Vec\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.python.eager import context\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(16)\n",
    "_ = tf.Variable([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-12 17:38:33--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 81043533 (77M) [text/plain]\n",
      "Saving to: ‘./datasets/ru_syntagrus-ud-train.conllu’\n",
      "\n",
      "./datasets/ru_synta 100%[===================>]  77,29M  4,70MB/s    in 18s     \n",
      "\n",
      "2021-04-12 17:38:54 (4,31 MB/s) - ‘./datasets/ru_syntagrus-ud-train.conllu’ saved [81043533/81043533]\n",
      "\n",
      "--2021-04-12 17:38:54--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10903424 (10M) [text/plain]\n",
      "Saving to: ‘./datasets/ru_syntagrus-ud-dev.conllu’\n",
      "\n",
      "./datasets/ru_synta 100%[===================>]  10,40M  4,25MB/s    in 2,4s    \n",
      "\n",
      "2021-04-12 17:38:58 (4,25 MB/s) - ‘./datasets/ru_syntagrus-ud-dev.conllu’ saved [10903424/10903424]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir datasets\n",
    "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
    "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = [[(token.form, token.upos) for token in sent] for sent in full_train]\n",
    "fdata_test = [[(token.form, token.upos) for token in sent] for sent in full_test]\n",
    "fdata_sent_test = [[token.form for token in sent] for sent in full_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Gram тэггер\n",
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "unigram_tagger = nltk.UnigramTagger(fdata_train, backoff=default_tagger)\n",
    "bigram_tagger = nltk.BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "ngram_tagger = nltk.TrigramTagger(fdata_train, backoff=bigram_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('В', 'ADP'),\n",
       " ('старой', 'ADJ'),\n",
       " ('трактовке', 'NOUN'),\n",
       " ('вместо', 'ADP'),\n",
       " ('слова', 'NOUN'),\n",
       " ('\"', 'PUNCT'),\n",
       " ('порядок', 'NOUN'),\n",
       " ('\"', 'PUNCT'),\n",
       " ('использовалось', 'VERB'),\n",
       " ('слово', 'NOUN'),\n",
       " ('\"', 'PUNCT'),\n",
       " ('последовательность', 'NOUN'),\n",
       " ('\"', 'PUNCT'),\n",
       " (',', 'PUNCT'),\n",
       " ('но', 'CCONJ'),\n",
       " ('по', 'ADP'),\n",
       " ('мере', 'NOUN'),\n",
       " ('развития', 'NOUN'),\n",
       " ('параллельности', 'NOUN'),\n",
       " ('в', 'ADP'),\n",
       " ('работе', 'NOUN'),\n",
       " ('компьютеров', 'NOUN'),\n",
       " ('слово', 'NOUN'),\n",
       " ('\"', 'PUNCT'),\n",
       " ('последовательность', 'NOUN'),\n",
       " ('\"', 'PUNCT'),\n",
       " ('стали', 'VERB'),\n",
       " ('заменять', 'VERB'),\n",
       " ('более', 'ADV'),\n",
       " ('общим', 'ADJ'),\n",
       " ('словом', 'NOUN'),\n",
       " ('\"', 'PUNCT'),\n",
       " ('порядок', 'NOUN'),\n",
       " ('\"', 'PUNCT'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9120159741178849"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ngram_tagger.tag(fdata_sent_test[1]), ngram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23568564014423887"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Эмм... мы уверены, что хотим еще что-то с этим сделать? Улучшить бейзлайн, который и так выше 91%? Really?\n",
    "# Ну окей, посмотрим хотя-бы какие методы сколько дали скора тут:\n",
    "default_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9063037104438378"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9118980217706333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9120159741178849"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9835025002122714"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Специально прочекал - я не проверяю скор на трейне... на трейне он естественно еще выше\n",
    "ngram_tagger.evaluate(fdata_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Мне кажется тут можно спокойно завернуть это в прод и идти спать ))\n",
    "# В жизни конечно могут встретиться другие тексты, с другими формами этих слов - и вот тут скор упадет.\n",
    "# Но у нас то сейчас их нет (размеченных) - не на чем провериться ну и ДЗ есть ДЗ.. \n",
    "# Приступим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "all_test_texts = [' '.join(token.form for token in sent) for sent in full_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_labels = [' '.join(token.upos if token.upos else 'UNK' for token in sent) for sent in full_train]\n",
    "all_test_labels = [' '.join(token.upos if token.upos else 'UNK' for token in sent) for sent in full_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN PUNCT\n",
      "NOUN ADJ NOUN NOUN PROPN PROPN AUX NOUN ADJ PUNCT VERB ADP NOUN ADV ADV PUNCT VERB ADP NOUN ADP NOUN CCONJ ADV PART VERB ADP NOUN NOUN ADP NOUN PUNCT NOUN PUNCT PUNCT\n",
      "ADP NOUN PRON ADP NOUN VERB NOUN PUNCT PUNCT PRON ADP ADJ NOUN PUNCT CCONJ PRON PART ADP DET PUNCT PRON ADV ADV AUX VERB ADP ADJ NOUN PUNCT PART VERB PROPN PROPN PUNCT\n",
      "ADV NOUN NOUN PROPN PROPN VERB ADP PRON PUNCT SCONJ VERB DET VERB CCONJ ADV VERB ADP NOUN PUNCT\n",
      "NOUN AUX VERB ADV PUNCT CCONJ ADV PUNCT\n",
      "ADP NOUN VERB NOUN NOUN PUNCT ADP NOUN PUNCT VERB NOUN ADP ADJ NOUN PUNCT\n",
      "ADP NOUN VERB NOUN CCONJ VERB NOUN ADP NOUN VERB CCONJ ADV ADP PRON PUNCT SCONJ VERB NOUN NOUN PUNCT VERB ADP NOUN PUNCT ADV SCONJ PUNCT ADV PUNCT ADP NOUN VERB VERB PART ADJ NOUN PUNCT\n",
      "NOUN VERB NOUN PUNCT ADJ PROPN PROPN PUNCT\n",
      "ADP NOUN VERB ADJ ADJ NOUN ADP ADJ NOUN CCONJ ADP PRON NUM ADJ NOUN PUNCT\n",
      "ADV VERB NOUN ADP NOUN PUNCT ADJ PUNCT VERB ADJ NOUN CCONJ ADP NUM NOUN ADV VERB NOUN PUNCT\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(all_train_labels[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так, ну нам то нужны списки\n",
    "train_tokens = [pair[0] for pair in [item for sublist in fdata_train for item in sublist]]\n",
    "train_labels = [pair[1] if pair[1] else 'UNK' for pair in [item for sublist in fdata_train for item in sublist]]\n",
    "test_tokens = [pair[0] for pair in [item for sublist in fdata_test for item in sublist]]\n",
    "test_labels = [pair[1] if pair[1] else 'UNK' for pair in [item for sublist in fdata_test for item in sublist]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ну, для начала бейзлайн в виде LogReg..\n",
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_labels)\n",
    "test_enc_labels = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvectorizer = HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hvectorizer.fit_transform(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = hvectorizer.transform(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871526, 10000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/postas/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9133218751053146"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ну - хотя... беру свои слова назад. Если по-агрессивнее себя вести (hash словарь 10К+ и сотни итераций регрессии)\n",
    "# Вполне можно побить результат nltk-шных n-gram-ных алгоритмов\n",
    "# Но - будем пробовать умные методы - не все же брут-форсом решать..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvectorizer = TfidfVectorizer(ngram_range=(1, 4), analyzer='char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871526, 60615)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf = tfvectorizer.fit_transform(train_tokens)\n",
    "X_test_tf = tfvectorizer.transform(test_tokens)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 41s, sys: 1min 53s, total: 6min 34s\n",
      "Wall time: 1min 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/postas/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "lr.fit(X_train_tf, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tf = lr.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9371903750884643"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_enc_labels, pred_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Налицо!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(n_estimators=100, learning_rate=0.087, reg_lambda=0.15, \n",
    "                      device=\"gpu\", gpu_use_dp=False, max_bin=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 1.58974\n",
      "[2]\tvalid_0's multi_logloss: 1.36151\n",
      "[3]\tvalid_0's multi_logloss: 1.20941\n",
      "[4]\tvalid_0's multi_logloss: 1.09007\n",
      "[5]\tvalid_0's multi_logloss: 0.996052\n",
      "[6]\tvalid_0's multi_logloss: 0.916222\n",
      "[7]\tvalid_0's multi_logloss: 0.846777\n",
      "[8]\tvalid_0's multi_logloss: 0.789336\n",
      "[9]\tvalid_0's multi_logloss: 0.739032\n",
      "[10]\tvalid_0's multi_logloss: 0.69438\n",
      "[11]\tvalid_0's multi_logloss: 0.655772\n",
      "[12]\tvalid_0's multi_logloss: 0.621252\n",
      "[13]\tvalid_0's multi_logloss: 0.591626\n",
      "[14]\tvalid_0's multi_logloss: 0.565632\n",
      "[15]\tvalid_0's multi_logloss: 0.541409\n",
      "[16]\tvalid_0's multi_logloss: 0.519939\n",
      "[17]\tvalid_0's multi_logloss: 0.500147\n",
      "[18]\tvalid_0's multi_logloss: 0.482608\n",
      "[19]\tvalid_0's multi_logloss: 0.466642\n",
      "[20]\tvalid_0's multi_logloss: 0.451675\n",
      "[21]\tvalid_0's multi_logloss: 0.438065\n",
      "[22]\tvalid_0's multi_logloss: 0.425893\n",
      "[23]\tvalid_0's multi_logloss: 0.414507\n",
      "[24]\tvalid_0's multi_logloss: 0.403963\n",
      "[25]\tvalid_0's multi_logloss: 0.394127\n",
      "[26]\tvalid_0's multi_logloss: 0.385128\n",
      "[27]\tvalid_0's multi_logloss: 0.377045\n",
      "[28]\tvalid_0's multi_logloss: 0.369201\n",
      "[29]\tvalid_0's multi_logloss: 0.362022\n",
      "[30]\tvalid_0's multi_logloss: 0.355406\n",
      "[31]\tvalid_0's multi_logloss: 0.349195\n",
      "[32]\tvalid_0's multi_logloss: 0.343339\n",
      "[33]\tvalid_0's multi_logloss: 0.337834\n",
      "[34]\tvalid_0's multi_logloss: 0.332813\n",
      "[35]\tvalid_0's multi_logloss: 0.327903\n",
      "[36]\tvalid_0's multi_logloss: 0.323356\n",
      "[37]\tvalid_0's multi_logloss: 0.319093\n",
      "[38]\tvalid_0's multi_logloss: 0.315185\n",
      "[39]\tvalid_0's multi_logloss: 0.311345\n",
      "[40]\tvalid_0's multi_logloss: 0.307687\n",
      "[41]\tvalid_0's multi_logloss: 0.304209\n",
      "[42]\tvalid_0's multi_logloss: 0.301049\n",
      "[43]\tvalid_0's multi_logloss: 0.2978\n",
      "[44]\tvalid_0's multi_logloss: 0.294832\n",
      "[45]\tvalid_0's multi_logloss: 0.292062\n",
      "[46]\tvalid_0's multi_logloss: 0.289374\n",
      "[47]\tvalid_0's multi_logloss: 0.287004\n",
      "[48]\tvalid_0's multi_logloss: 0.284448\n",
      "[49]\tvalid_0's multi_logloss: 0.281933\n",
      "[50]\tvalid_0's multi_logloss: 0.279636\n",
      "[51]\tvalid_0's multi_logloss: 0.277312\n",
      "[52]\tvalid_0's multi_logloss: 0.275305\n",
      "[53]\tvalid_0's multi_logloss: 0.273435\n",
      "[54]\tvalid_0's multi_logloss: 0.271375\n",
      "[55]\tvalid_0's multi_logloss: 0.269418\n",
      "[56]\tvalid_0's multi_logloss: 0.267501\n",
      "[57]\tvalid_0's multi_logloss: 0.265678\n",
      "[58]\tvalid_0's multi_logloss: 0.263949\n",
      "[59]\tvalid_0's multi_logloss: 0.262131\n",
      "[60]\tvalid_0's multi_logloss: 0.26065\n",
      "[61]\tvalid_0's multi_logloss: 0.259122\n",
      "[62]\tvalid_0's multi_logloss: 0.257528\n",
      "[63]\tvalid_0's multi_logloss: 0.256\n",
      "[64]\tvalid_0's multi_logloss: 0.254642\n",
      "[65]\tvalid_0's multi_logloss: 0.253128\n",
      "[66]\tvalid_0's multi_logloss: 0.251837\n",
      "[67]\tvalid_0's multi_logloss: 0.250541\n",
      "[68]\tvalid_0's multi_logloss: 0.248973\n",
      "[69]\tvalid_0's multi_logloss: 0.247601\n",
      "[70]\tvalid_0's multi_logloss: 0.246493\n",
      "[71]\tvalid_0's multi_logloss: 0.245366\n",
      "[72]\tvalid_0's multi_logloss: 0.244175\n",
      "[73]\tvalid_0's multi_logloss: 0.243109\n",
      "[74]\tvalid_0's multi_logloss: 0.242242\n",
      "[75]\tvalid_0's multi_logloss: 0.241151\n",
      "[76]\tvalid_0's multi_logloss: 0.240025\n",
      "[77]\tvalid_0's multi_logloss: 0.239015\n",
      "[78]\tvalid_0's multi_logloss: 0.237928\n",
      "[79]\tvalid_0's multi_logloss: 0.236814\n",
      "[80]\tvalid_0's multi_logloss: 0.235872\n",
      "[81]\tvalid_0's multi_logloss: 0.234957\n",
      "[82]\tvalid_0's multi_logloss: 0.233818\n",
      "[83]\tvalid_0's multi_logloss: 0.232995\n",
      "[84]\tvalid_0's multi_logloss: 0.232208\n",
      "[85]\tvalid_0's multi_logloss: 0.231261\n",
      "[86]\tvalid_0's multi_logloss: 0.230495\n",
      "[87]\tvalid_0's multi_logloss: 0.229741\n",
      "[88]\tvalid_0's multi_logloss: 0.228863\n",
      "[89]\tvalid_0's multi_logloss: 0.228069\n",
      "[90]\tvalid_0's multi_logloss: 0.22738\n",
      "[91]\tvalid_0's multi_logloss: 0.22649\n",
      "[92]\tvalid_0's multi_logloss: 0.225624\n",
      "[93]\tvalid_0's multi_logloss: 0.22503\n",
      "[94]\tvalid_0's multi_logloss: 0.224284\n",
      "[95]\tvalid_0's multi_logloss: 0.223515\n",
      "[96]\tvalid_0's multi_logloss: 0.22274\n",
      "[97]\tvalid_0's multi_logloss: 0.222084\n",
      "[98]\tvalid_0's multi_logloss: 0.221361\n",
      "[99]\tvalid_0's multi_logloss: 0.220749\n",
      "[100]\tvalid_0's multi_logloss: 0.220098\n",
      "CPU times: user 52min 5s, sys: 16.7 s, total: 52min 22s\n",
      "Wall time: 3min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(device='gpu', gpu_use_dp=False, learning_rate=0.087, max_bin=50,\n",
       "               reg_lambda=0.15)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lgbm.fit(X_train_tf, train_enc_labels, eval_set=[(X_test_tf, test_enc_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_pred = lgbm.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.929119064469383"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_enc_labels, lgbm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Более тяжелая бустинговая модель даже хуже справляется с этой задачей... либо ее вдумчиво тюнить надо еще"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_prep(token):\n",
    "    return lemmer.parse(token.lower())[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [[token_prep(token.form) for token in sent] for sent in full_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['анкета', '.'],\n",
       " ['начальник',\n",
       "  'областной',\n",
       "  'управление',\n",
       "  'связь',\n",
       "  'семён',\n",
       "  'еремеевич',\n",
       "  'быть',\n",
       "  'человек',\n",
       "  'простой',\n",
       "  ',',\n",
       "  'приходить',\n",
       "  'на',\n",
       "  'работа',\n",
       "  'всегда',\n",
       "  'вовремя',\n",
       "  ',',\n",
       "  'здороваться',\n",
       "  'с',\n",
       "  'секретарша',\n",
       "  'за',\n",
       "  'рука',\n",
       "  'и',\n",
       "  'иногда',\n",
       "  'даже',\n",
       "  'писать',\n",
       "  'в',\n",
       "  'стенгазета',\n",
       "  'заметка',\n",
       "  'под',\n",
       "  'псевдоним',\n",
       "  '\"',\n",
       "  'муха',\n",
       "  '\"',\n",
       "  '.']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Неясно что делать с пунктуацией. Поидее в процессе выучивания эмбеддингов она участвовать не должна\n",
    "# Но если ее убрать - на нее не будет эмбеддингов и классификатор не будет знать что с этими знаками делать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = Word2Vec(sentences=train_sentences, vector_size=500, window=5, min_count=1, workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21278732,  0.6149245 ,  0.19744252, -0.27500916,  0.03706319,\n",
       "       -0.08265077, -0.22061466,  0.5312531 , -0.03850966, -0.12585573,\n",
       "       -0.21457656,  0.17632453, -0.01801084, -0.5097521 , -0.33300465,\n",
       "        0.04141767, -0.04545579, -0.15542306,  0.10994256, -0.24727948,\n",
       "        0.00395595,  0.23952244,  0.37681627, -0.06451444, -0.18628356,\n",
       "       -0.27167365, -0.28647184, -0.38015452, -0.1577973 ,  0.21566355,\n",
       "        0.5575756 ,  0.04229384, -0.21567576, -0.20044881, -0.14605278,\n",
       "        0.41565326,  0.1468013 , -0.31206942, -0.00506167, -0.27831793,\n",
       "        0.307549  ,  0.1254016 ,  0.06283051, -0.3811765 , -0.20665228,\n",
       "       -0.07548293,  0.274045  ,  0.04755214, -0.3398731 ,  0.2926576 ,\n",
       "       -0.12400011, -0.1232708 , -0.20995376, -0.25031757,  0.22504409,\n",
       "       -0.18754041,  0.18352395,  0.1415958 , -0.02140302,  0.01020835,\n",
       "        0.06774039, -0.33364186,  0.09648377,  0.16614659,  0.18396562,\n",
       "       -0.12619042,  0.07967508,  0.05287063, -0.16100004,  0.23520797,\n",
       "       -0.1132908 , -0.163686  , -0.08189511,  0.06068981, -0.09424423,\n",
       "        0.04947789, -0.20715617,  0.00297744,  0.29419768,  0.4279538 ,\n",
       "        0.1854141 , -0.3516325 , -0.05512841, -0.1077275 , -0.59552205,\n",
       "        0.08924798, -0.1011195 ,  0.3758981 ,  0.15752609,  0.14861193,\n",
       "       -0.06515143, -0.09350096, -0.5268386 ,  0.2620236 ,  0.1908461 ,\n",
       "        0.09347572, -0.2907428 ,  0.05496849,  0.01615968, -0.12374555,\n",
       "       -0.13317859, -0.46269402, -0.04860259,  0.2543892 ,  0.47354847,\n",
       "        0.24881263,  0.30469856,  0.3942313 , -0.24700402,  0.34157586,\n",
       "       -0.06103801,  0.00808475, -0.22947475,  0.45638698, -0.32521644,\n",
       "        0.06791708,  0.14005314, -0.17978612,  0.32116196, -0.5013191 ,\n",
       "        0.06902475, -0.00170866,  0.5427543 ,  0.307847  ,  0.20533714,\n",
       "       -0.10413567, -0.6499347 ,  0.06233268, -0.31164047, -0.11238037,\n",
       "        0.32124957,  0.11491801, -0.02204029, -0.03223083, -0.16836514,\n",
       "        0.7002175 , -0.28627092,  0.2723497 , -0.07617185, -0.576001  ,\n",
       "        0.2893623 , -0.17367066,  0.20187369, -0.07767197, -0.17280814,\n",
       "        0.0486009 , -0.07767104, -0.14359026,  0.3069642 ,  0.28578883,\n",
       "        0.05955223, -0.11508754, -0.24267496,  0.10631368, -0.46841982,\n",
       "       -0.22283517,  0.04890459, -0.6395364 , -0.3992153 , -0.13926692,\n",
       "        0.05753442,  0.0023448 ,  0.00823826, -0.5222905 , -0.05734554,\n",
       "        0.45641357,  0.16416448,  0.3146981 ,  0.12530889,  0.18909825,\n",
       "        0.25853947,  0.47770387,  0.02129906,  0.18588752,  0.22839919,\n",
       "       -0.3684176 ,  0.4232772 ,  0.16897087, -0.03091376,  0.27732563,\n",
       "        0.08013445, -0.27492064,  0.29633626,  0.03406115, -0.2142514 ,\n",
       "       -0.34460345, -0.06443402,  0.23391172,  0.0196575 ,  0.19564804,\n",
       "        0.09426252,  0.04250623,  0.43331626, -0.09270283, -0.02731877,\n",
       "       -0.08530357,  0.08900384, -0.08219009, -0.2646038 ,  0.27780852,\n",
       "        0.3132982 , -0.083689  ,  0.3522145 ,  0.47495902, -0.16451983,\n",
       "        0.12824555,  0.29566154, -0.36626557, -0.22802557, -0.37137976,\n",
       "        0.24624604, -0.26194653,  0.09790264, -0.33452553, -0.39433816,\n",
       "       -0.08174571,  0.44223395, -0.25805053, -0.29557434, -0.29786733,\n",
       "        0.05431397,  0.27654764,  0.11172002, -0.01201173, -0.3103632 ,\n",
       "       -0.38641977, -0.34598523, -0.73086953,  0.06775788,  0.03360824,\n",
       "        0.0352147 ,  0.09684658, -0.37194085, -0.13856173, -0.35434437,\n",
       "        0.0349147 , -0.29391414,  0.01162029,  0.31269076, -0.18721305,\n",
       "       -0.02367445, -0.721811  , -0.5294788 ,  0.26509398,  0.09665948,\n",
       "       -0.35737723, -0.11315287, -0.23593453,  0.01667164,  0.06583505,\n",
       "       -0.17797868, -0.33622533,  0.3938598 , -0.01379527, -0.15330245,\n",
       "        0.03240686,  0.1445808 ,  0.4343151 , -0.18870334, -0.31398547,\n",
       "        0.07923542, -0.03415037, -0.0361362 , -0.32039663, -0.15882231,\n",
       "       -0.07269061, -0.32568717,  0.08445299, -0.18199877, -0.39394805,\n",
       "        0.252546  ,  0.27048996,  0.68443644, -0.16735445, -0.09381731,\n",
       "       -0.06582379,  0.2901804 , -0.57949406,  0.006926  , -0.10655095,\n",
       "        0.05158902,  0.08065468, -0.02029191, -0.22457612,  0.39092943,\n",
       "       -0.4490344 , -0.03417985,  0.04075149,  0.42107284, -0.22329924,\n",
       "       -0.06671369,  0.0758718 , -0.14101225,  0.10745849, -0.05749117,\n",
       "        0.04410778, -0.02053037, -0.22072539,  0.0380553 , -0.47137302,\n",
       "       -0.10309203,  0.01829508,  0.301223  ,  0.2703579 ,  0.29327512,\n",
       "       -0.43594643,  0.38592607, -0.25238186, -0.22872686, -0.06718307,\n",
       "        0.00245653, -0.687946  ,  0.17926674,  0.28507105, -0.06549989,\n",
       "        0.36474794,  0.38436195,  0.01501294, -0.04262927,  0.02753646,\n",
       "        0.15217927,  0.10471501,  0.05149004, -0.03945747,  0.21086179,\n",
       "        0.07117668, -0.2634681 , -0.14402822, -0.0492949 , -0.41833037,\n",
       "       -0.0752696 , -0.03501486,  0.38146558, -0.04505364,  0.05547708,\n",
       "        0.06146295,  0.15609662,  0.36076352, -0.12709385, -0.643737  ,\n",
       "       -0.3314478 , -0.27920103,  0.28720197, -0.3836726 ,  0.24320914,\n",
       "       -0.12237541, -0.32545733,  0.28784367,  0.02228453,  0.68664604,\n",
       "        0.07160877,  0.05393634, -0.25253275,  0.18779582, -0.07505293,\n",
       "        0.22466347, -0.28657782, -0.17024456,  0.1520334 , -0.01719414,\n",
       "       -0.18781012,  0.01028937,  0.02897764,  0.02737006, -0.5217167 ,\n",
       "       -0.09748946, -0.07672437,  0.07761144, -0.40098533,  0.20092016,\n",
       "        0.05306843, -0.54662347,  0.01121363,  0.06147273, -0.34679106,\n",
       "       -0.25179613, -0.2844076 , -0.29263487, -0.07326108, -0.02715462,\n",
       "        0.18739994, -0.28454292,  0.13579546,  0.34852883,  0.01714978,\n",
       "       -0.06180115,  0.20484939, -0.21964644, -0.11792399, -0.09358714,\n",
       "        0.3266859 , -0.37135723, -0.10097107,  0.22768654,  0.7475611 ,\n",
       "        0.5743918 ,  0.12812859,  0.14935927,  0.05592097,  0.12574755,\n",
       "       -0.3092248 ,  0.08119791, -0.48937625, -0.22374912, -0.06884319,\n",
       "       -0.24856453, -0.09727856, -0.15510327,  0.219696  , -0.1571851 ,\n",
       "       -0.05253631, -0.23151456, -0.03315229,  0.22700998,  0.44372186,\n",
       "        0.41713983,  0.8592422 , -0.54249966,  0.13612594, -0.7361524 ,\n",
       "        0.15527816,  0.2935132 , -0.25821513,  0.2761099 ,  0.46558836,\n",
       "        0.15144625,  0.09950025,  0.5360845 , -0.42592558,  0.06215207,\n",
       "       -0.03974584,  0.36955202, -0.47209147,  0.14209248, -0.32563004,\n",
       "        0.27885965, -0.15999596, -0.31263152, -0.4602143 ,  0.07118855,\n",
       "       -0.23440686, -0.14068344,  0.40040332, -0.2734008 ,  0.29325283,\n",
       "       -0.02837555,  0.06054192, -0.00701181, -0.19170883, -0.00129284,\n",
       "        0.0456561 ,  0.09310907, -0.36663812,  0.13740574, -0.01784082,\n",
       "       -0.16266234,  0.07444508, -0.42444232, -0.22393267,  0.14037868,\n",
       "        0.02253373,  0.00860687,  0.25774747, -0.17691457, -0.2079876 ,\n",
       "       -0.2599427 ,  0.03476413, -0.05196187, -0.7874727 ,  0.14714278,\n",
       "       -0.35460085,  0.32884   , -0.30399367, -0.88938093,  0.0018178 ,\n",
       "        0.1599817 ,  0.28076872,  0.2640523 , -0.00545812, -0.29948866,\n",
       "       -0.32740298, -0.49506608, -0.05784863,  0.4364918 ,  0.05707021,\n",
       "       -0.14214641, -0.4408513 ,  0.45005572,  0.34882864,  0.2284293 ,\n",
       "        0.03472962,  0.30233747, -0.21311681,  0.11408259,  0.3740487 ,\n",
       "        0.34238994, -0.04537789,  0.08995657,  0.14696702, -0.15365879],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.wv['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Окей, эмбеддинги есть. Даже на пунктуацию - попробуем классифицировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_emb = [wv.wv[token_prep(pair[0])] for pair in [item for sublist in fdata_train for item in sublist]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33min 13s, sys: 6min 17s, total: 39min 30s\n",
      "Wall time: 3min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/postas/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "lr.fit(train_tokens_emb, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'хорезми' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-ddb50543068e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_tokens_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfdata_test\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-101-ddb50543068e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_tokens_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfdata_test\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \"\"\"\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'хорезми' not present\""
     ]
    }
   ],
   "source": [
    "test_tokens_emb = [wv.wv[token_prep(pair[0])] for pair in [item for sublist in fdata_test for item in sublist]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Таак.. напоролись \n",
    "# Даже не знаю, как тут быть.. выкидывать слова из теста поидее нельзя - что, если у нас на инференсе попадется\n",
    "# Слово для которого нет эмбеддингов - мы же не сможем его не проклассифицировать. По аналогии с default_tagging\n",
    "# Надо бы его просто обозвать существительным наверное.. В этом случае его во входном потоке можно просто заменять\n",
    "# На эмбеддинг какого нибудь гарантированного существительного. (надеюсь я не слишком бредово мыслю в столь поздний час)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens_emb = [wv.wv[token_prep(pair[0])] if token_prep(pair[0]) in wv.wv \\\n",
    "                   else wv.wv['анкета'] for pair in [item for sublist in fdata_test for item in sublist]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_emb = lr.predict(test_tokens_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7940383513631921"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_enc_labels, pred_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нууу... такое.. открытым остается вопрос насчет пунктуации. Попробуем ее убрать из расчета эмбеддингов, но \n",
    "# Классифаеру ее отдадим в виде какого-нибудь специально сконструированного эмбеддинга типа все -1 например.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [[token_prep(token.form) for token in sent if token.form.isalpha()] for sent in full_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['анкета'],\n",
       " ['начальник',\n",
       "  'областной',\n",
       "  'управление',\n",
       "  'связь',\n",
       "  'семён',\n",
       "  'еремеевич',\n",
       "  'быть',\n",
       "  'человек',\n",
       "  'простой',\n",
       "  'приходить',\n",
       "  'на',\n",
       "  'работа',\n",
       "  'всегда',\n",
       "  'вовремя',\n",
       "  'здороваться',\n",
       "  'с',\n",
       "  'секретарша',\n",
       "  'за',\n",
       "  'рука',\n",
       "  'и',\n",
       "  'иногда',\n",
       "  'даже',\n",
       "  'писать',\n",
       "  'в',\n",
       "  'стенгазета',\n",
       "  'заметка',\n",
       "  'под',\n",
       "  'псевдоним',\n",
       "  'муха']]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = Word2Vec(sentences=train_sentences, vector_size=500, window=5, min_count=1, workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'.' in wv.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Отлично. Теперь эмбеддинг для пунктуации\n",
    "wv.wv['анкета'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = np.zeros(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заменяем\n",
    "train_tokens_emb = [wv.wv[token_prep(pair[0])] if pair[0].isalpha() else punct \\\n",
    "                    for pair in [item for sublist in fdata_train for item in sublist]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32min 8s, sys: 6min 9s, total: 38min 17s\n",
      "Wall time: 2min 56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/postas/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "lr.fit(train_tokens_emb, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так... Теперь еще сложнее процедура формирования теста ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens_emb = [wv.wv[token_prep(pair[0])] if token_prep(pair[0]) in wv.wv \\\n",
    "                   else wv.wv['анкета'] if pair[0].isalpha() else punct \\\n",
    "                   for pair in [item for sublist in fdata_test for item in sublist]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_emb = lr.predict(test_tokens_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7639942034846493"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_enc_labels, pred_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Еще хуже.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ну - для очистки совести еще нейросетевые варианты.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сетевой классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерировать эмбеддинги сетями мне не очень понятно как.. в тех примерах которые мы рассматривали на вебинарах\n",
    "# эмбеддинги были как-бы \"побочным\" продуктом процесса классификации текстов.\n",
    "# В этой задаче не понятно - на что мы можем классифицировать тексты - лейблов у нас нет для текстов\n",
    "# Есть только для слов (наши POS теги)\n",
    "# Была дурная мысль раздать текстам рандомные или одинаковые таргеты, но каково будет качество результата \n",
    "# Такой мусорной классификации - остается под вопросом. Думаю не гуд идея.\n",
    "\n",
    "# Поэтому будем брать уже векторизованные слова (наши tfidf или hashing) и тренировать сеть на классификацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сколько у нас лейблов\n",
    "len(list(set(train_enc_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(test_enc_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvectorizer = TfidfVectorizer(ngram_range=(1, 4), analyzer='char', max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = tfvectorizer.fit_transform(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = tfvectorizer.transform(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(1000, activation='relu'),\n",
    "    Dense(128),\n",
    "    Dense(18, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "13618/13618 [==============================] - 55s 4ms/step - loss: 0.2929 - accuracy: 0.9039 - val_loss: 0.1754 - val_accuracy: 0.9360\n",
      "Epoch 2/15\n",
      "13618/13618 [==============================] - 54s 4ms/step - loss: 0.1351 - accuracy: 0.9496 - val_loss: 0.1611 - val_accuracy: 0.9403\n",
      "Epoch 3/15\n",
      "13618/13618 [==============================] - 55s 4ms/step - loss: 0.1180 - accuracy: 0.9551 - val_loss: 0.1577 - val_accuracy: 0.9431\n",
      "Epoch 4/15\n",
      "13618/13618 [==============================] - 55s 4ms/step - loss: 0.1103 - accuracy: 0.9574 - val_loss: 0.1557 - val_accuracy: 0.9448\n",
      "Epoch 5/15\n",
      "13618/13618 [==============================] - 55s 4ms/step - loss: 0.1049 - accuracy: 0.9592 - val_loss: 0.1569 - val_accuracy: 0.9451\n",
      "Epoch 6/15\n",
      "13618/13618 [==============================] - 55s 4ms/step - loss: 0.1026 - accuracy: 0.9598 - val_loss: 0.1550 - val_accuracy: 0.9460\n",
      "Epoch 7/15\n",
      "13618/13618 [==============================] - 54s 4ms/step - loss: 0.1004 - accuracy: 0.9607 - val_loss: 0.1559 - val_accuracy: 0.9454\n",
      "Epoch 8/15\n",
      "13618/13618 [==============================] - 54s 4ms/step - loss: 0.0986 - accuracy: 0.9610 - val_loss: 0.1547 - val_accuracy: 0.9466\n",
      "Epoch 9/15\n",
      "13618/13618 [==============================] - 49s 4ms/step - loss: 0.0972 - accuracy: 0.9614 - val_loss: 0.1555 - val_accuracy: 0.9467\n",
      "Epoch 10/15\n",
      "13618/13618 [==============================] - 48s 4ms/step - loss: 0.0951 - accuracy: 0.9620 - val_loss: 0.1564 - val_accuracy: 0.9465\n",
      "Epoch 11/15\n",
      "13618/13618 [==============================] - 55s 4ms/step - loss: 0.0944 - accuracy: 0.9625 - val_loss: 0.1573 - val_accuracy: 0.9477\n",
      "Epoch 12/15\n",
      "13618/13618 [==============================] - 55s 4ms/step - loss: 0.0943 - accuracy: 0.9626 - val_loss: 0.1573 - val_accuracy: 0.9476\n",
      "Epoch 13/15\n",
      "13618/13618 [==============================] - 56s 4ms/step - loss: 0.0935 - accuracy: 0.9625 - val_loss: 0.1588 - val_accuracy: 0.9468\n",
      "Epoch 14/15\n",
      "13618/13618 [==============================] - 56s 4ms/step - loss: 0.0928 - accuracy: 0.9630 - val_loss: 0.1585 - val_accuracy: 0.9478\n",
      "Epoch 15/15\n",
      "13618/13618 [==============================] - 56s 4ms/step - loss: 0.0935 - accuracy: 0.9626 - val_loss: 0.1630 - val_accuracy: 0.9468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f684bf3d670>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x = pd.DataFrame.sparse.from_spmatrix(X_train_tf),\n",
    "    y = train_enc_labels,\n",
    "    batch_size=64,\n",
    "    validation_data=(pd.DataFrame.sparse.from_spmatrix(X_test_tf), test_enc_labels),\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c59d499ac150adc1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c59d499ac150adc1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вобщем - 0.947 - отыграли один процент у ЛогРегрессии )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvectorizer = TfidfVectorizer(ngram_range=(1, 4), analyzer='char', max_features=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = tfvectorizer.fit_transform(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = tfvectorizer.transform(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(2048, activation='relu'),\n",
    "    Dense(512),\n",
    "    Dense(18, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "6809/6809 [==============================] - 33s 5ms/step - loss: 0.2588 - accuracy: 0.9151 - val_loss: 0.1539 - val_accuracy: 0.9456\n",
      "Epoch 2/15\n",
      "6809/6809 [==============================] - 31s 5ms/step - loss: 0.1163 - accuracy: 0.9565 - val_loss: 0.1496 - val_accuracy: 0.9482\n",
      "Epoch 3/15\n",
      "6809/6809 [==============================] - 31s 5ms/step - loss: 0.1022 - accuracy: 0.9606 - val_loss: 0.1397 - val_accuracy: 0.9506\n",
      "Epoch 4/15\n",
      "6809/6809 [==============================] - 31s 5ms/step - loss: 0.0946 - accuracy: 0.9631 - val_loss: 0.1397 - val_accuracy: 0.9510\n",
      "Epoch 5/15\n",
      "6809/6809 [==============================] - 31s 5ms/step - loss: 0.0896 - accuracy: 0.9645 - val_loss: 0.1442 - val_accuracy: 0.9491\n",
      "Epoch 6/15\n",
      "6809/6809 [==============================] - 31s 5ms/step - loss: 0.0879 - accuracy: 0.9653 - val_loss: 0.1390 - val_accuracy: 0.9528\n",
      "Epoch 7/15\n",
      "6809/6809 [==============================] - 31s 5ms/step - loss: 0.0855 - accuracy: 0.9657 - val_loss: 0.1359 - val_accuracy: 0.9532\n",
      "Epoch 8/15\n",
      "6809/6809 [==============================] - 31s 5ms/step - loss: 0.0840 - accuracy: 0.9661 - val_loss: 0.1373 - val_accuracy: 0.9533\n",
      "Epoch 9/15\n",
      "6809/6809 [==============================] - 31s 5ms/step - loss: 0.0836 - accuracy: 0.9661 - val_loss: 0.1331 - val_accuracy: 0.9532\n",
      "Epoch 10/15\n",
      "6809/6809 [==============================] - 31s 5ms/step - loss: 0.0826 - accuracy: 0.9663 - val_loss: 0.1389 - val_accuracy: 0.9511\n",
      "Epoch 11/15\n",
      "6809/6809 [==============================] - 31s 5ms/step - loss: 0.0815 - accuracy: 0.9666 - val_loss: 0.1338 - val_accuracy: 0.9537\n",
      "Epoch 12/15\n",
      "6809/6809 [==============================] - 26s 4ms/step - loss: 0.0812 - accuracy: 0.9666 - val_loss: 0.1406 - val_accuracy: 0.9542\n",
      "Epoch 13/15\n",
      "6809/6809 [==============================] - 27s 4ms/step - loss: 0.0807 - accuracy: 0.9669 - val_loss: 0.1365 - val_accuracy: 0.9530\n",
      "Epoch 14/15\n",
      "6809/6809 [==============================] - 31s 5ms/step - loss: 0.0801 - accuracy: 0.9672 - val_loss: 0.1395 - val_accuracy: 0.9538\n",
      "Epoch 15/15\n",
      "6809/6809 [==============================] - 31s 5ms/step - loss: 0.0788 - accuracy: 0.9676 - val_loss: 0.1439 - val_accuracy: 0.9534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f68534c5cd0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x = pd.DataFrame.sparse.from_spmatrix(X_train_tf),\n",
    "    y = train_enc_labels,\n",
    "    batch_size=128,\n",
    "    validation_data=(pd.DataFrame.sparse.from_spmatrix(X_test_tf), test_enc_labels),\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-40e3ebf6a2b4bdd5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-40e3ebf6a2b4bdd5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Еще процент.. 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(pd.DataFrame.sparse.from_spmatrix(X_test_tf[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Алгоритм', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('от', 'ADP'),\n",
       " ('имени', 'NOUN'),\n",
       " ('учёного', 'NOUN'),\n",
       " ('аль', 'PROPN'),\n",
       " ('-', 'PUNCT'),\n",
       " ('Хорезми', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('-', 'PUNCT'),\n",
       " ('точный', 'ADJ'),\n",
       " ('набор', 'NOUN'),\n",
       " ('инструкций', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('описывающих', 'VERB'),\n",
       " ('порядок', 'NOUN'),\n",
       " ('действий', 'NOUN'),\n",
       " ('исполнителя', 'NOUN'),\n",
       " ('для', 'ADP'),\n",
       " ('достижения', 'NOUN'),\n",
       " ('результата', 'NOUN'),\n",
       " ('решения', 'NOUN'),\n",
       " ('задачи', 'NOUN'),\n",
       " ('за', 'ADP'),\n",
       " ('конечное', 'ADJ'),\n",
       " ('время', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('В', 'ADP'),\n",
       " ('старой', 'ADJ'),\n",
       " ('трактовке', 'NOUN')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, le.classes_[np.argmax(probs)]) for token, probs in zip(test_tokens[:30], preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Слегка ломает, что модель не понимает контекста... так \"достижение\" и \"решение\" здесь явно глаголы - \n",
    "# но скорее всего они существительное даже в разметке... так, что этот факап модели даже не предьявишь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В общем с этой задачей на данном этапе закончу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-13 15:17:17--  http://ai-center.botik.ru/Airec/ai-resources/Persons-1000.zip\n",
      "Resolving ai-center.botik.ru (ai-center.botik.ru)... failed: Temporary failure in name resolution.\n",
      "wget: unable to resolve host address ‘ai-center.botik.ru’\n"
     ]
    }
   ],
   "source": [
    "!wget http://ai-center.botik.ru/Airec/ai-resources/Persons-1000.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Интересно... что еще за новости ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-13 16:45:11--  http://ai-center.botik.ru/Airec/ai-resources/Persons-1000.zip\n",
      "Resolving ai-center.botik.ru (ai-center.botik.ru)... 95.129.138.2\n",
      "Connecting to ai-center.botik.ru (ai-center.botik.ru)|95.129.138.2|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3363777 (3,2M) [application/zip]\n",
      "Saving to: ‘Persons-1000.zip.1’\n",
      "\n",
      "Persons-1000.zip.1  100%[===================>]   3,21M  2,28MB/s    in 1,4s    \n",
      "\n",
      "2021-04-13 16:45:13 (2,28 MB/s) - ‘Persons-1000.zip.1’ saved [3363777/3363777]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget ai-center.botik.ru/Airec/ai-resources/Persons-1000.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Persons-1000.zip'\n",
    "records = corus.persons.load_persons(path)\n",
    "rec = next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PersonsMarkup(\n",
       "    text='Россия рассчитывает на конструктивное воздействие США на Грузию\\r\\n\\r\\n04/08/2008 12:08\\r\\n\\r\\nМОСКВА, 4 авг - РИА Новости. Россия рассчитывает, что США воздействуют на Тбилиси в связи с обострением ситуации в зоне грузино-осетинского конфликта. Об этом статс-секретарь - заместитель министра иностранных дел России Григорий Карасин заявил в телефонном разговоре с заместителем госсекретаря США Дэниэлом Фридом.\\r\\n\\r\\n\"С российской стороны выражена глубокая озабоченность в связи с новым витком напряженности вокруг Южной Осетии, противозаконными действиями грузинской стороны по наращиванию своих вооруженных сил в регионе, бесконтрольным строительством фортификационных сооружений\", - говорится в сообщении.\\r\\n\\r\\n\"Россия уже призвала Тбилиси к ответственной линии и рассчитывает также на конструктивное воздействие со стороны Вашингтона\", - сообщил МИД России. ',\n",
       "    spans=[PersonsSpan(\n",
       "         id=1,\n",
       "         start=308,\n",
       "         stop=324,\n",
       "         value='ГРИГОРИЙ КАРАСИН'\n",
       "     ), PersonsSpan(\n",
       "         id=2,\n",
       "         start=387,\n",
       "         stop=402,\n",
       "         value='ДЭНИЭЛ ФРИД'\n",
       "     )]\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ну вобщем так... нужно сделать нормальный датасет с нормальной разметкой собственно. Для классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PersonsSpan(\n",
       "    id=2,\n",
       "    start=387,\n",
       "    stop=402,\n",
       "    value='ДЭНИЭЛ ФРИД'\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.spans[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кстати вот интересно, почему для этого текста не помечены также \"Россия\", \"Северная Осетия\", \"США\", \"Тбилиси\" и тп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spans(text, spans):\n",
    "    result = []\n",
    "    for span in spans:\n",
    "        result.extend(text[span.start:span.stop].split())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_prep(token):\n",
    "    return lemmer.parse(token.lower())[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Григорий', 'Карасин', 'Дэниэлом', 'Фридом']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_spans(rec.text, rec.spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь лабелинг текста\n",
    "labelled_text = [(token_prep(token), 1) if token in get_spans(rec.text, rec.spans) else (token_prep(token), 0) \\\n",
    " for token in re.findall(r\"\\w+|[^\\w\\s]\", rec.text, re.UNICODE) if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('россия', 0),\n",
       " ('рассчитывать', 0),\n",
       " ('на', 0),\n",
       " ('конструктивный', 0),\n",
       " ('воздействие', 0),\n",
       " ('сша', 0),\n",
       " ('на', 0),\n",
       " ('грузия', 0),\n",
       " ('москва', 0),\n",
       " ('авг', 0),\n",
       " ('риа', 0),\n",
       " ('новость', 0),\n",
       " ('россия', 0),\n",
       " ('рассчитывать', 0),\n",
       " ('что', 0),\n",
       " ('сша', 0),\n",
       " ('воздействовать', 0),\n",
       " ('на', 0),\n",
       " ('тбилиси', 0),\n",
       " ('в', 0),\n",
       " ('связь', 0),\n",
       " ('с', 0),\n",
       " ('обострение', 0),\n",
       " ('ситуация', 0),\n",
       " ('в', 0),\n",
       " ('зона', 0),\n",
       " ('грузино', 0),\n",
       " ('осетинский', 0),\n",
       " ('конфликт', 0),\n",
       " ('о', 0),\n",
       " ('это', 0),\n",
       " ('статс', 0),\n",
       " ('секретарь', 0),\n",
       " ('заместитель', 0),\n",
       " ('министр', 0),\n",
       " ('иностранный', 0),\n",
       " ('дело', 0),\n",
       " ('россия', 0),\n",
       " ('григорий', 1),\n",
       " ('карасиный', 1),\n",
       " ('заявить', 0),\n",
       " ('в', 0),\n",
       " ('телефонный', 0),\n",
       " ('разговор', 0),\n",
       " ('с', 0),\n",
       " ('заместитель', 0),\n",
       " ('госсекретарь', 0),\n",
       " ('сша', 0),\n",
       " ('дэниэлом', 1),\n",
       " ('фрид', 1),\n",
       " ('с', 0),\n",
       " ('российский', 0),\n",
       " ('сторона', 0),\n",
       " ('выразить', 0),\n",
       " ('глубокий', 0),\n",
       " ('озабоченность', 0),\n",
       " ('в', 0),\n",
       " ('связь', 0),\n",
       " ('с', 0),\n",
       " ('новый', 0),\n",
       " ('виток', 0),\n",
       " ('напряжённость', 0),\n",
       " ('вокруг', 0),\n",
       " ('южный', 0),\n",
       " ('осетия', 0),\n",
       " ('противозаконный', 0),\n",
       " ('действие', 0),\n",
       " ('грузинский', 0),\n",
       " ('сторона', 0),\n",
       " ('по', 0),\n",
       " ('наращивание', 0),\n",
       " ('свой', 0),\n",
       " ('вооружённый', 0),\n",
       " ('сила', 0),\n",
       " ('в', 0),\n",
       " ('регион', 0),\n",
       " ('бесконтрольный', 0),\n",
       " ('строительство', 0),\n",
       " ('фортификационный', 0),\n",
       " ('сооружение', 0),\n",
       " ('говориться', 0),\n",
       " ('в', 0),\n",
       " ('сообщение', 0),\n",
       " ('россия', 0),\n",
       " ('уже', 0),\n",
       " ('призвать', 0),\n",
       " ('тбилиси', 0),\n",
       " ('к', 0),\n",
       " ('ответственный', 0),\n",
       " ('линия', 0),\n",
       " ('и', 0),\n",
       " ('рассчитывать', 0),\n",
       " ('также', 0),\n",
       " ('на', 0),\n",
       " ('конструктивный', 0),\n",
       " ('воздействие', 0),\n",
       " ('с', 0),\n",
       " ('сторона', 0),\n",
       " ('вашингтон', 0),\n",
       " ('сообщить', 0),\n",
       " ('мид', 0),\n",
       " ('россия', 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('григорий', 1), ('карасиный', 1), ('дэниэлом', 1), ('фрид', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in labelled_text if item[1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ммм... что то с лемматизацией фигня получается. Может просто стемминг на имена собственные?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'карасин'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(word='карасин')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'дэниэлом'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(word='дэниэлом')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тоже не работает как хочется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/postas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'карасин'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_lemmatizer.lemmatize('карасин')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'дэниэлом'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_lemmatizer.lemmatize('дэниэлом')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'дэниэлом'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmer.parse('Дэниэлом')[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ничего путного не получается... возможно будем использовать форму которая в таргете у нас дана\n",
    "# И надеяться на буквенные ngram-ы при векторизации (может зря мы его лемматизируем тогда...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spans(text, spans):\n",
    "    raw_list = []\n",
    "    for span in spans:\n",
    "        raw_list.extend([{real_form: ref_form} for real_form, ref_form in \\\n",
    "                         zip(text[span.start:span.stop].split(), span.value.split())])\n",
    "    result = {}\n",
    "    for item in raw_list:\n",
    "        result.update(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Григорий': 'ГРИГОРИЙ',\n",
       " 'Карасин': 'КАРАСИН',\n",
       " 'Дэниэлом': 'ДЭНИЭЛ',\n",
       " 'Фридом': 'ФРИД'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_spans(rec.text, rec.spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_text = [(get_spans(rec.text, rec.spans).get(token).lower(), 1) \\\n",
    "                 if token in get_spans(rec.text, rec.spans) else (token_prep(token), 0) \\\n",
    "                 for token in re.findall(r\"\\w+|[^\\w\\s]\", rec.text, re.UNICODE) if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('россия', 0),\n",
       " ('рассчитывать', 0),\n",
       " ('на', 0),\n",
       " ('конструктивный', 0),\n",
       " ('воздействие', 0),\n",
       " ('сша', 0),\n",
       " ('на', 0),\n",
       " ('грузия', 0),\n",
       " ('москва', 0),\n",
       " ('авг', 0),\n",
       " ('риа', 0),\n",
       " ('новость', 0),\n",
       " ('россия', 0),\n",
       " ('рассчитывать', 0),\n",
       " ('что', 0),\n",
       " ('сша', 0),\n",
       " ('воздействовать', 0),\n",
       " ('на', 0),\n",
       " ('тбилиси', 0),\n",
       " ('в', 0),\n",
       " ('связь', 0),\n",
       " ('с', 0),\n",
       " ('обострение', 0),\n",
       " ('ситуация', 0),\n",
       " ('в', 0),\n",
       " ('зона', 0),\n",
       " ('грузино', 0),\n",
       " ('осетинский', 0),\n",
       " ('конфликт', 0),\n",
       " ('о', 0),\n",
       " ('это', 0),\n",
       " ('статс', 0),\n",
       " ('секретарь', 0),\n",
       " ('заместитель', 0),\n",
       " ('министр', 0),\n",
       " ('иностранный', 0),\n",
       " ('дело', 0),\n",
       " ('россия', 0),\n",
       " ('григорий', 1),\n",
       " ('карасин', 1),\n",
       " ('заявить', 0),\n",
       " ('в', 0),\n",
       " ('телефонный', 0),\n",
       " ('разговор', 0),\n",
       " ('с', 0),\n",
       " ('заместитель', 0),\n",
       " ('госсекретарь', 0),\n",
       " ('сша', 0),\n",
       " ('дэниэл', 1),\n",
       " ('фрид', 1),\n",
       " ('с', 0),\n",
       " ('российский', 0),\n",
       " ('сторона', 0),\n",
       " ('выразить', 0),\n",
       " ('глубокий', 0),\n",
       " ('озабоченность', 0),\n",
       " ('в', 0),\n",
       " ('связь', 0),\n",
       " ('с', 0),\n",
       " ('новый', 0),\n",
       " ('виток', 0),\n",
       " ('напряжённость', 0),\n",
       " ('вокруг', 0),\n",
       " ('южный', 0),\n",
       " ('осетия', 0),\n",
       " ('противозаконный', 0),\n",
       " ('действие', 0),\n",
       " ('грузинский', 0),\n",
       " ('сторона', 0),\n",
       " ('по', 0),\n",
       " ('наращивание', 0),\n",
       " ('свой', 0),\n",
       " ('вооружённый', 0),\n",
       " ('сила', 0),\n",
       " ('в', 0),\n",
       " ('регион', 0),\n",
       " ('бесконтрольный', 0),\n",
       " ('строительство', 0),\n",
       " ('фортификационный', 0),\n",
       " ('сооружение', 0),\n",
       " ('говориться', 0),\n",
       " ('в', 0),\n",
       " ('сообщение', 0),\n",
       " ('россия', 0),\n",
       " ('уже', 0),\n",
       " ('призвать', 0),\n",
       " ('тбилиси', 0),\n",
       " ('к', 0),\n",
       " ('ответственный', 0),\n",
       " ('линия', 0),\n",
       " ('и', 0),\n",
       " ('рассчитывать', 0),\n",
       " ('также', 0),\n",
       " ('на', 0),\n",
       " ('конструктивный', 0),\n",
       " ('воздействие', 0),\n",
       " ('с', 0),\n",
       " ('сторона', 0),\n",
       " ('вашингтон', 0),\n",
       " ('сообщить', 0),\n",
       " ('мид', 0),\n",
       " ('россия', 0)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('григорий', 1), ('карасин', 1), ('дэниэл', 1), ('фрид', 1)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in labelled_text if item[1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так будет норм (если на векторизации мы сможем удачно порезать исходный текст на нграмы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группируем все слова корпуса с признаками\n",
    "all_tokens_labelled = []\n",
    "for rec in records:\n",
    "    all_tokens_labelled.append([(get_spans(rec.text, rec.spans).get(token).lower(), 1) \\\n",
    "                 if token in get_spans(rec.text, rec.spans) else (token_prep(token), 0) \\\n",
    "                 for token in re.findall(r\"\\w+|[^\\w\\s]\", rec.text, re.UNICODE) if token.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('октябрь', 0),\n",
       " ('назначить', 0),\n",
       " ('очередной', 0),\n",
       " ('выборы', 0),\n",
       " ('верховный', 0),\n",
       " ('совет', 0),\n",
       " ('аджарский', 0),\n",
       " ('ар', 0),\n",
       " ('по', 0),\n",
       " ('распоряжение', 0),\n",
       " ('президент', 0),\n",
       " ('грузия', 0),\n",
       " ('михаил', 1),\n",
       " ('саакашвили', 1),\n",
       " ('октябрь', 0),\n",
       " ('год', 0),\n",
       " ('назначить', 0),\n",
       " ('очередной', 0),\n",
       " ('выборы', 0),\n",
       " ('верховный', 0),\n",
       " ('совет', 0),\n",
       " ('аджарский', 0),\n",
       " ('ар', 0),\n",
       " ('о', 0),\n",
       " ('это', 0),\n",
       " ('новость', 0),\n",
       " ('грузия', 0),\n",
       " ('сообщить', 0),\n",
       " ('в', 0),\n",
       " ('пресс', 0),\n",
       " ('служба', 0),\n",
       " ('администрация', 0),\n",
       " ('президент', 0),\n",
       " ('в', 0),\n",
       " ('вторник', 0),\n",
       " ('выборы', 0),\n",
       " ('верховный', 0),\n",
       " ('совет', 0),\n",
       " ('аджарский', 0),\n",
       " ('автономный', 0),\n",
       " ('республика', 0),\n",
       " ('назначить', 0),\n",
       " ('в', 0),\n",
       " ('соответствие', 0),\n",
       " ('с', 0),\n",
       " ('ой', 0),\n",
       " ('статья', 0),\n",
       " ('и', 0),\n",
       " ('м', 0),\n",
       " ('пункт', 0),\n",
       " ('й', 0),\n",
       " ('статья', 0),\n",
       " ('конституционный', 0),\n",
       " ('закон', 0),\n",
       " ('грузия', 0),\n",
       " ('о', 0),\n",
       " ('статус', 0),\n",
       " ('аджарский', 0),\n",
       " ('автономный', 0),\n",
       " ('республика', 0)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens_labelled[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('михаил', 1), ('саакашвили', 1)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in all_tokens_labelled[1] if item[1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Все это бы объединить в один большой список (сейчас он резделен на документы)\n",
    "joint_tokens_labbeled = [item for sublist in all_tokens_labelled for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218858"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(joint_tokens_labbeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делим на трейн и тест\n",
    "train_tokens_labbeled = joint_tokens_labbeled[:150000]\n",
    "test_tokens_labbeled = joint_tokens_labbeled[150000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разбираем  на токены и таргеты\n",
    "train_X = [pair[0] for pair in train_tokens_labbeled]\n",
    "test_X = [pair[0] for pair in test_tokens_labbeled]\n",
    "train_y = [pair[1] for pair in train_tokens_labbeled]\n",
    "test_y = [pair[1] for pair in test_tokens_labbeled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пока без эмбеддингов ))\n",
    "tfvectorizer = TfidfVectorizer(ngram_range=(2, 5), analyzer='char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 54102)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_tf = tfvectorizer.fit_transform(train_X)\n",
    "test_X_tf = tfvectorizer.transform(test_X)\n",
    "train_X_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Штош.. попробуем простую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.1 s, sys: 18.4 s, total: 35.5 s\n",
      "Wall time: 2.46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr = LogisticRegression(random_state=0, max_iter=1000)\n",
    "lr.fit(train_X_tf, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(test_X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9743820616340876"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не знаю... может я что-то совершенно не понимаю - почему такой скор опять?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real life test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = corus.persons.load_persons(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PersonsSpan(\n",
       "     id=1,\n",
       "     start=320,\n",
       "     stop=334,\n",
       "     value='ЕГОР БОРИСОВ'\n",
       " ), PersonsSpan(\n",
       "     id=2,\n",
       "     start=989,\n",
       "     stop=1003,\n",
       "     value='ЕГОР БОРИСОВ'\n",
       " ), PersonsSpan(\n",
       "     id=3,\n",
       "     start=1318,\n",
       "     stop=1332,\n",
       "     value='ЕГОР БОРИСОВ'\n",
       " ), PersonsSpan(\n",
       "     id=4,\n",
       "     start=1624,\n",
       "     stop=1643,\n",
       "     value='АНАТОЛИЙ ПОДЛАСЕНКО'\n",
       " )]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Следственное управление при прокуратуре требует наказать премьера Якутии\\r\\n\\r\\nСледственное управление Следственного комитета при прокуратуре Российской Федерации по Якутии обжаловало решение прокуратуры республики. Ранее прокуратура отказала в возбуждении уголовного дела в отношении председателя правительства республики Егора Борисова, подозреваемого в хищении 30 млн руб.\\r\\n\\r\\nКак сообщили в четверг корреспонденту Агентства национальных новостей в следственном управлении, еще 16 мая 2007 г. прокуратурой Якутии было возбуждено уголовное дело № 66144 по признакам преступления, предусмотренного ч. 4 ст. 159 УК РФ по факту причинения имущественного ущерба в размере 30 млн руб. государственному унитарному предприятию <Дирекция по строительству железной дороги <Беркакит-Томмот-Якутск>.\\r\\n\\r\\n<В ходе расследования данного уголовного дела собраны достаточные данные, свидетельствующие о причастности к причинению ущерба данному предприятию председателя правительства Республики Саха (Якутия) Егора Борисова>, - говорится в официальном заявлении следственного управления.\\r\\n\\r\\nДля установления процессуальным путем всех обстоятельств, касающихся причинения ущерба, 4 августа 2008 года Следственное управление Следственного комитета при прокуратуре Российской Федерации по Якутии возбудило уголовное дело № 49234 в отношении Егора Борисова по признакам составов преступлений, предусмотренных ч. 2 ст. 286, ч. 5 ст. 33, ч. 4 ст. 160 и ч. 2 ст. 286 УК РФ.\\r\\n\\r\\n12 августа прокуратура Якутии постановление о возбуждении уголовного дела № 49234 отменила. В связи с этим следственное управление повторно обратилось к прокурору республики Анатолию Подласенко с требованием о возбуждении уголовного дела.\\r\\n'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Следственное управление при прокуратуре требует наказать премьера Якутии Следственное управление Следственного комитета при прокуратуре Российской Федерации по Якутии обжаловало решение прокуратуры республики. Ранее прокуратура отказала в возбуждении уголовного дела в отношении председателя правительства республики \u001b[31mЕгора\u001b[0m \u001b[31mБорисова,\u001b[0m подозреваемого в хищении 30 млн руб. Как сообщили в четверг корреспонденту Агентства национальных новостей в следственном управлении, еще 16 мая 2007 г. прокуратурой Якутии было возбуждено уголовное дело № 66144 по признакам преступления, предусмотренного ч. 4 ст. 159 УК РФ по факту причинения имущественного ущерба в размере 30 млн руб. государственному унитарному предприятию <Дирекция по строительству железной дороги <Беркакит-Томмот-Якутск>. <В ходе расследования данного уголовного дела собраны достаточные данные, свидетельствующие о причастности к причинению ущерба данному предприятию председателя правительства Республики Саха (Якутия) \u001b[31mЕгора\u001b[0m \u001b[31mБорисова>,\u001b[0m - говорится в официальном заявлении следственного управления. Для установления процессуальным путем всех обстоятельств, касающихся причинения ущерба, 4 августа 2008 года Следственное управление Следственного комитета при прокуратуре Российской Федерации по Якутии возбудило уголовное дело № 49234 в отношении \u001b[31mЕгора\u001b[0m \u001b[31mБорисова\u001b[0m по признакам составов преступлений, предусмотренных ч. 2 ст. 286, ч. 5 ст. 33, ч. 4 ст. 160 и ч. 2 ст. 286 УК РФ. 12 августа прокуратура Якутии постановление о возбуждении уголовного дела № 49234 отменила. В связи с этим следственное управление повторно обратилось к прокурору республики \u001b[31mАнатолию\u001b[0m \u001b[31mПодласенко\u001b[0m с требованием о возбуждении уголовного дела.\n"
     ]
    }
   ],
   "source": [
    "text_prepared = [token_prep(token) for token in rec.text.split()]\n",
    "text_vectorized = tfvectorizer.transform(text_prepared)\n",
    "preds = lr.predict(text_vectorized)\n",
    "result = []\n",
    "for word, label in zip(rec.text.split(), preds):\n",
    "    result.append(word if label == 0 else f'\\x1b[31m{word}\\x1b[0m')\n",
    "print(' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не знаю, отобразиться ли цветная печать там где этот ноутбук будут просматривать - у меня в локальном юпитере\n",
    "# Все 4 NER-а нашлись корректно и подсвечены красным )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попозже еще поэкспериментирую с deeppavlov и natasha - сейчас пока-что залью в таком виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
